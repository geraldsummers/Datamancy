# Auto-generated by build-datamancy.main.kts
# Image versions are HARDCODED at build time
# Only secrets and deployment paths use runtime ${VARS}

services:
  vllm-7b:
    image: datamancy/vllm-qwen-7b:latest
    build:
      context: ./docker/vllm-qwen-7b
      dockerfile: Dockerfile
    container_name: vllm-7b
    restart: unless-stopped
    networks:
      - ai
    environment:
      HUGGINGFACEHUB_API_TOKEN: ${HUGGINGFACEHUB_API_TOKEN}
      HF_HUB_ENABLE_HF_TRANSFER: 0
      HF_HUB_OFFLINE: 1
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      HF_HOME: /models
      TRANSFORMERS_CACHE: /models
      HF_HUB_CACHE: /models
    command:
      - Qwen/Qwen2.5-7B-Instruct-AWQ
      - --quantization
      - awq
      - --served-model-name
      - qwen2.5-7b-instruct
      - --max-model-len
      - "32768"
      - --max-num-seqs
      - "16"
      - --dtype
      - auto
      - --enable-auto-tool-choice
      - --tool-call-parser
      - hermes
      - --gpu-memory-utilization
      - "0.9"
      - --tensor-parallel-size
      - "1"
      - --trust-remote-code
    deploy:
      resources:
        limits:
          memory: 12G
          cpus: '8.0'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 300s

  embedding-service:
    image: datamancy/embedding-bge:latest
    build:
      context: ./docker/embedding-bge
      dockerfile: Dockerfile
    container_name: embedding-service
    restart: unless-stopped
    networks:
      - ai
      - ai-gateway
    environment:
      HF_HUB_ENABLE_HF_TRANSFER: 0
      MODEL_ID: BAAI/bge-base-en-v1.5
    command: --port 8080
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 120s

  litellm:
    image: ghcr.io/berriai/litellm:v1.80.8-stable.1
    container_name: litellm
    restart: unless-stopped
    networks:
      ai:
        aliases:
          - litellm.${DOMAIN}
          - api.litellm.${DOMAIN}
      litellm:
        aliases:
          - litellm.${DOMAIN}
          - api.litellm.${DOMAIN}
      caddy:
        aliases:
          - litellm.${DOMAIN}
          - api.litellm.${DOMAIN}
    depends_on:
      vllm-7b:
        condition: service_started
      embedding-service:
        condition: service_started
    environment:
      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY}
      UI_USERNAME: ${STACK_ADMIN_USER}
      UI_PASSWORD: ${STACK_ADMIN_PASSWORD}
    volumes:
      - litellm_config:/app/config
      - ./configs/infrastructure/litellm/config.yaml:/app/config.yaml:ro
    command: --config /app/config.yaml --port 4000 --num_workers 4
    healthcheck:
      test: ["CMD-SHELL", "python3 -c 'import socket; s=socket.socket(); s.settimeout(5); s.connect((\"127.0.0.1\", 4000)); s.close()' || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s

