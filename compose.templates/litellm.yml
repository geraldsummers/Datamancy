services:
  litellm:
    image: ghcr.io/berriai/litellm:v1.80.8-stable.1
    container_name: litellm
    restart: unless-stopped
    networks:
      ai:
        aliases:
          - litellm.${DOMAIN}
          - api.litellm.${DOMAIN}
      litellm:
        aliases:
          - litellm.${DOMAIN}
          - api.litellm.${DOMAIN}
      caddy:
        aliases:
          - litellm.${DOMAIN}
          - api.litellm.${DOMAIN}
    depends_on:
      vllm-7b:
        condition: service_started
      embedding-service:
        condition: service_started
    environment:
      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY}
      UI_USERNAME: ${STACK_ADMIN_USER}
      UI_PASSWORD: ${STACK_ADMIN_PASSWORD}
    volumes:
      - litellm_config:/app/config
      - ./configs/litellm/config.yaml:/app/config.yaml:ro
    command: --config /app/config.yaml --port 4000 --num_workers 4
    healthcheck:
      test:
        - CMD-SHELL
        - |
          python3 -c "import urllib.request; import sys; import json; \
          urllib.request.urlopen('http://127.0.0.1:4000/health', timeout=3); \
          resp = urllib.request.urlopen('http://127.0.0.1:4000/models', timeout=3); \
          data = json.load(resp); \
          sys.exit(0 if 'data' in data else 1)"
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
