services:
  vllm-7b:
    image: datamancy/vllm-qwen-7b:latest
    build:
      context: ./containers.src/vllm-qwen-7b
      dockerfile: Dockerfile
    container_name: vllm-7b
    restart: unless-stopped
    networks:
      - ai
    environment:
      HUGGINGFACEHUB_API_TOKEN: ${HUGGINGFACEHUB_API_TOKEN}
      HF_HUB_ENABLE_HF_TRANSFER: 0
      HF_HUB_OFFLINE: 1
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      HF_HOME: /models
      TRANSFORMERS_CACHE: /models
      HF_HUB_CACHE: /models
    command:
      - Qwen/Qwen2.5-7B-Instruct-AWQ
      - --quantization
      - awq
      - --served-model-name
      - qwen2.5-7b-instruct
      - --max-model-len
      - "32768"
      - --max-num-seqs
      - "16"
      - --dtype
      - auto
      - --enable-auto-tool-choice
      - --tool-call-parser
      - hermes
      - --gpu-memory-utilization
      - "0.9"
      - --tensor-parallel-size
      - "1"
      - --trust-remote-code
    deploy:
      resources:
        limits:
          memory: 12G
          cpus: '8.0'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test:
        - CMD-SHELL
        - |
          curl -sf -m 5 http://localhost:8000/health >/dev/null && \
          curl -sf -m 5 http://localhost:8000/v1/models | grep -q qwen
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 120s
