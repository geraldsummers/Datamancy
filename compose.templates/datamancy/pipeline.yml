# Auto-generated by build-datamancy.main.kts
# Image versions are HARDCODED at build time
# Only secrets and deployment paths use runtime ${VARS}

services:
  pipeline:
    image: datamancy/pipeline:local-build
    build:
      context: ./src/pipeline
      dockerfile: Dockerfile
    container_name: pipeline
    restart: unless-stopped
    networks:
      - qdrant
      - clickhouse
      - ai-gateway
    depends_on:
      qdrant:
        condition: service_started
      clickhouse:
        condition: service_started
      embedding-service:
        condition: service_started
    environment:
      # RSS Feed - REAL DATA
      RSS_ENABLED: "true"
      RSS_SCHEDULE_MINUTES: "60"  # Hourly
      RSS_FEED_URLS: "https://hnrss.org/frontpage,https://arxiv.org/rss/cs.AI,https://www.reddit.com/r/programming/.rss,https://feeds.arstechnica.com/arstechnica/index"

      # CVE - REAL NVD API (no key = 5 req/30s, with key = 50 req/30s)
      CVE_ENABLED: "true"
      CVE_API_KEY: "${CVE_API_KEY:-}"  # Optional - works without key but slower
      CVE_SCHEDULE_MINUTES: "1440"  # Daily
      CVE_MAX_RESULTS: "2147483647"  # Pull ALL CVEs - FULL DATASET

      # Torrents - REAL torrents.csv dataset
      # Dataset: https://gitlab.com/dessalines/torrents.csv/-/raw/main/torrents.csv.gz
      TORRENTS_ENABLED: "true"
      TORRENTS_DATA_PATH: "https://gitlab.com/dessalines/torrents.csv/-/raw/main/torrents.csv.gz"
      TORRENTS_SCHEDULE_MINUTES: "10080"  # Weekly
      TORRENTS_MAX_RESULTS: "2147483647"  # Pull ALL torrents - let's go FULL POWER

      # Wikipedia - REAL sample XML dump
      # Using Simple English Wikipedia for testing (smaller than main EN dump)
      # Main EN: https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2 (20GB!)
      # Simple: https://dumps.wikimedia.org/simplewiki/latest/simplewiki-latest-pages-articles.xml.bz2 (200MB)
      WIKIPEDIA_ENABLED: "true"
      WIKIPEDIA_DUMP_PATH: "https://dumps.wikimedia.org/simplewiki/latest/simplewiki-latest-pages-articles.xml.bz2"
      WIKIPEDIA_SCHEDULE_MINUTES: "43200"  # Twice daily
      WIKIPEDIA_MAX_ARTICLES: "2147483647"  # ALL ARTICLES - COMPLETE WIKIPEDIA

      # Australian Laws - REAL but using sample data (full API complex)
      AUSTRALIAN_LAWS_ENABLED: "true"
      AUSTRALIAN_LAWS_JURISDICTION: "commonwealth"
      AUSTRALIAN_LAWS_SCHEDULE_MINUTES: "1440"  # Daily
      AUSTRALIAN_LAWS_MAX: "2147483647"  # ALL LAWS

      # Linux Docs - REAL system man pages
      LINUX_DOCS_ENABLED: "true"
      LINUX_DOCS_SOURCES: "MAN_PAGES"  # Reads from /usr/share/man inside container
      LINUX_DOCS_SCHEDULE_MINUTES: "10080"  # Weekly
      LINUX_DOCS_MAX: "2147483647"  # ALL MAN PAGES

      # Services
      EMBEDDING_SERVICE_URL: http://embedding-service:8080
      QDRANT_URL: http://qdrant:6333
      CLICKHOUSE_URL: http://clickhouse:8123

      # BookStack integration (optional - for wiki presentation)
      BOOKSTACK_ENABLED: "${BOOKSTACK_ENABLED:-false}"
      BOOKSTACK_URL: "http://bookstack:80"
      BOOKSTACK_TOKEN_ID: "${BOOKSTACK_API_TOKEN_ID:-}"
      BOOKSTACK_TOKEN_SECRET: "${BOOKSTACK_API_TOKEN_SECRET:-}"

      # Collection names
      QDRANT_RSS_COLLECTION: rss_feeds
      QDRANT_CVE_COLLECTION: cve
      QDRANT_TORRENTS_COLLECTION: torrents
      QDRANT_MARKET_COLLECTION: market_data
      QDRANT_WIKIPEDIA_COLLECTION: wikipedia
      QDRANT_AUSTRALIAN_LAWS_COLLECTION: australian_laws
      QDRANT_LINUX_DOCS_COLLECTION: linux_docs
    healthcheck:
      test: ["CMD-SHELL", "timeout 5 wget --quiet --tries=1 -O /dev/null http://127.0.0.1:8090/health || exit 1"]
      interval: 60s
      timeout: 20s
      retries: 3
      start_period: 60s
