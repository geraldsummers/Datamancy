# Diagnostic Artifacts Reference

## Overview

The autonomous diagnostic system persists all evidence to the `/proofs` volume for analysis, auditing, and model training. Artifacts include screenshots, DOM dumps, logs, resource metrics, and AI-generated reports.

---

## Volume Configuration

### Mount Point
```yaml
# docker-compose.yml
volumes:
  proofs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/proofs

services:
  probe-orchestrator:
    volumes:
      - proofs:/proofs
```

**Local path:** `./volumes/proofs/`
**Container path:** `/proofs/`

---

## Artifact Types

### 1. Screenshots (Visual Evidence)

**Location:** `/proofs/screenshots/<service>_<timestamp>.png`

**Purpose:** Visual proof of service state at time of diagnosis

**Format:** PNG image (base64-encoded during transmission)

**Example filename:** `http___open-webui_8080_1732478404997.png`

**Sample metadata:**
```json
{
  "service": "http://open-webui:8080",
  "captured_at": 1732478404997,
  "size_bytes": 245830,
  "resolution": "1920x1080",
  "format": "png"
}
```

**Use cases:**
- Verify UI is rendering correctly
- Detect error pages, loading spinners
- OCR text extraction for analysis
- Training data for vision models

### 2. DOM Excerpts (Structure Evidence)

**Location:** Embedded in probe results, not saved separately

**Purpose:** HTML structure analysis for programmatic inspection

**Format:** Raw HTML string (truncated to 3000 chars)

**Example:**
```html
<!DOCTYPE html>
<html lang="en">
<head><title>Open WebUI</title></head>
<body>
  <div id="root">
    <div class="error-boundary">
      <h1>Error: Failed to load configuration</h1>
      <p>Cannot connect to API server</p>
    </div>
  </div>
</body>
</html>
```

**Extracted info:**
- Page title
- Error messages
- Loading states
- API endpoints referenced
- JavaScript errors

### 3. Container Logs (Behavioral Evidence)

**Location:** Embedded in diagnostic reports

**Source:** `docker logs <container> --tail=100`

**Format:** Plain text, last 100-500 lines

**Example:**
```
2025-11-30T04:30:15.123Z [ERROR] Connection refused: http://litellm:4000
2025-11-30T04:30:16.456Z [WARN] Retrying in 5s (attempt 3/5)
2025-11-30T04:30:21.789Z [ERROR] Connection refused: http://litellm:4000
2025-11-30T04:30:22.012Z [FATAL] Max retries exceeded, shutting down
```

**Analysis targets:**
- Exception stack traces
- Connection errors
- Timeout patterns
- Restart loops
- Resource warnings

### 4. Resource Metrics (Performance Evidence)

**Location:** Embedded in diagnostic reports

**Source:** `docker stats <container> --no-stream`

**Format:** JSON object

**Example:**
```json
{
  "container": "vllm-router",
  "cpu_percent": "95.3%",
  "mem_usage": "15.2GiB / 16GiB",
  "mem_percent": "95.0%",
  "net_io": "1.2GB / 450MB",
  "block_io": "2.3GB / 1.1GB",
  "pids": "42"
}
```

**Red flags:**
- CPU > 90% sustained
- Memory > 85%
- High restart count
- Zero network I/O (dead service)

### 5. Basic Diagnostic Reports

**Location:** `/proofs/stack_diagnostics_<timestamp>.json`

**Purpose:** Service-level health summary

**Generated by:** `/start-stack-probe` endpoint

**Example structure:**
```json
{
  "generated_at": 1732478400000,
  "total_services": 14,
  "healthy": 11,
  "degraded": 2,
  "failed": 1,
  "services": [
    {
      "name": "vllm-router",
      "results": [
        {
          "service": "http://vllm-router:8010/health",
          "status": "failed",
          "reason": "HTTP 503 Service Unavailable",
          "screenshot_path": "/proofs/screenshots/vllm-router_1732478401234.png",
          "dom_excerpt": "<html><body>503 Backend Not Ready</body></html>",
          "ocr_text": "503 Service Unavailable\nBackend not responding",
          "wellness_report": null,
          "steps": [
            {"step": 1, "tool": "http_get", "args": {"url": "..."}, "result": {"status": 503}},
            {"step": 2, "tool": "browser_screenshot", "args": {"url": "..."}}
          ]
        }
      ],
      "overall_status": "failed",
      "best_reason": "Backend service not responding",
      "best_screenshot": "/proofs/screenshots/vllm-router_1732478401234.png",
      "container_info": {
        "State": {"Status": "running", "Health": {"Status": "unhealthy"}},
        "RestartCount": 5
      }
    }
  ]
}
```

### 6. Enhanced Diagnostic Reports (AI Analysis)

**Location:** `/proofs/enhanced_diagnostics_<timestamp>.json`

**Purpose:** Root cause analysis + fix proposals

**Generated by:** `/analyze-and-propose-fixes` endpoint

**Example structure:**
```json
{
  "generated_at": 1732478500000,
  "report_id": "enhanced-1732478500",
  "summary": {
    "total": 14,
    "healthy": 11,
    "degraded": 2,
    "failed": 1,
    "issues": 3,
    "safe_actions": 2,
    "needs_review": 1
  },
  "issues": [
    {
      "id": "issue-vllm-router-1732478500123",
      "service": "vllm-router",
      "severity": "critical",
      "status": "failed",
      "evidence": [
        "screenshot:/proofs/screenshots/vllm-router_1732478401234.png",
        "wellness_report"
      ],
      "root_cause_hypothesis": "vllm-router cannot connect to upstream vllm:8000. Logs show repeated connection timeouts. Resource usage is normal (CPU 2%, Memory 180MB), ruling out exhaustion. Likely causes: (1) vllm service not fully started, (2) network misconfiguration, (3) health check endpoint mismatch.",
      "log_excerpt": "[ERROR] Connection timeout to http://vllm:8000\n[WARN] Retrying...\n[ERROR] Connection timeout to http://vllm:8000\n",
      "resource_metrics": {
        "cpu": "2.1%",
        "memory": "180MiB / 16GiB",
        "mem_percent": "1.1%",
        "net_io": "120KB / 45KB",
        "pids": "12"
      },
      "proposed_fixes": [
        {
          "action": "check_dependencies",
          "confidence": "high",
          "reasoning": "Verify vllm service is healthy and accessible on port 8000. Check docker-compose network and service startup order.",
          "parameters": {
            "dependency": "vllm",
            "expected_port": "8000"
          }
        },
        {
          "action": "restart",
          "confidence": "medium",
          "reasoning": "Restart vllm-router to clear stale connection state. Success rate: 70% for timeout-related issues.",
          "parameters": {
            "service": "vllm-router",
            "grace_period": "10s"
          }
        },
        {
          "action": "check_config",
          "confidence": "medium",
          "reasoning": "Verify VLLM_BASE_URL environment variable matches actual endpoint.",
          "parameters": {
            "config_keys": ["VLLM_BASE_URL", "VLLM_TIMEOUT"]
          }
        }
      ]
    }
  ],
  "automated_actions_safe": [
    "vllm-router: check_dependencies",
    "open-webui: restart"
  ],
  "requires_human_review": [
    "postgres: check_config"
  ],
  "base_report_path": "/proofs/stack_diagnostics_1732478400.json"
}
```

### 7. Approved Actions Manifest

**Location:** `/proofs/approved_fixes_<timestamp>.json`

**Purpose:** Record of human-approved remediation actions

**Generated by:** `scripts/supervisor-session.sh review` CLI

**Example structure:**
```json
{
  "generated_at": "2025-11-30T05:15:00Z",
  "source_report": "/proofs/enhanced_diagnostics_1732478500.json",
  "reviewer": "human",
  "approved_actions": [
    {
      "issue_id": "issue-vllm-router-1732478500123",
      "service": "vllm-router",
      "action": "restart",
      "confidence": "medium",
      "reasoning": "Restart to clear stale connections",
      "parameters": {"service": "vllm-router", "grace_period": "10s"},
      "approved_at": "2025-11-30T05:15:23Z"
    }
  ]
}
```

---

## Artifact Lifecycle

### 1. Detection Phase
```
User/Alert → /start-stack-probe → Probe all services
```
**Generates:** Screenshots, DOM, basic diagnostics JSON

### 2. Analysis Phase
```
/analyze-and-propose-fixes → Call local LLM → Generate fix proposals
```
**Generates:** Enhanced diagnostics JSON with AI analysis

### 3. Review Phase
```
supervisor-session.sh review → Human reviews → Approves/rejects fixes
```
**Generates:** Approved actions manifest

### 4. Execution Phase (Future - Phase 2)
```
execute-fixes.sh → Apply approved fixes → Re-diagnose → Verify
```
**Generates:** Execution logs, before/after comparisons

---

## Storage & Retention

### Current Implementation
- **Location:** `./volumes/proofs/`
- **Retention:** Unlimited (manual cleanup)
- **Size:** Varies by service count and probe frequency

### Recommended Production Setup

```yaml
# docker-compose.yml
volumes:
  proofs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /mnt/diagnostics/proofs  # Dedicated volume
```

**Retention policy:**
```bash
# Keep last 30 days of diagnostics
find /mnt/diagnostics/proofs -name "*.json" -mtime +30 -delete

# Keep screenshots for 7 days
find /mnt/diagnostics/proofs/screenshots -name "*.png" -mtime +7 -delete

# Archive important reports to S3/ClickHouse
aws s3 cp /mnt/diagnostics/proofs/enhanced_*.json s3://diagnostics-archive/
```

---

## Artifact Analysis Examples

### Example 1: Service Restart Loop Detection

**Evidence:**
```json
{
  "container_info": {
    "RestartCount": 47,
    "State": {"Status": "restarting"}
  },
  "log_excerpt": "FATAL: Out of memory\nKilled\n...",
  "resource_metrics": {"mem_percent": "99.8%"}
}
```

**AI Analysis:**
```json
{
  "root_cause": "Container repeatedly OOM killed",
  "fixes": [
    {
      "action": "scale_up",
      "confidence": "high",
      "reasoning": "Memory limit too low. Increase from 2GB to 4GB",
      "parameters": {"service": "open-webui", "memory_limit": "4GB"}
    }
  ]
}
```

### Example 2: Dependency Chain Failure

**Evidence:**
```json
{
  "service": "open-webui",
  "status": "failed",
  "log_excerpt": "[ERROR] Cannot connect to http://litellm:4000\n",
  "upstream_check": {
    "litellm": {"status": "unhealthy", "reason": "vLLM backend timeout"}
  }
}
```

**AI Analysis:**
```json
{
  "root_cause": "Cascade failure: vLLM → LiteLLM → Open WebUI",
  "fixes": [
    {
      "action": "restart_dependencies",
      "confidence": "high",
      "reasoning": "Start with root cause (vLLM), then cascade up",
      "parameters": {"services": ["vllm", "litellm", "open-webui"], "sequential": true}
    }
  ]
}
```

---

## Integration with External Systems

### ClickHouse (Historical Analysis)

```sql
CREATE TABLE diagnostics (
  timestamp DateTime64(3),
  service String,
  status String,
  root_cause String,
  fix_action String,
  fix_success Boolean,
  resolution_time_ms UInt32
) ENGINE = MergeTree()
ORDER BY (timestamp, service);

-- Query: Most common failures
SELECT service, root_cause, count(*) as occurrences
FROM diagnostics
WHERE timestamp > now() - INTERVAL 30 DAY
GROUP BY service, root_cause
ORDER BY occurrences DESC
LIMIT 10;

-- Query: Fix success rate
SELECT fix_action,
       countIf(fix_success) as successes,
       count(*) as total,
       round(successes / total * 100, 1) as success_rate_pct
FROM diagnostics
WHERE fix_action IS NOT NULL
GROUP BY fix_action
ORDER BY success_rate_pct DESC;
```

### Grafana (Real-time Monitoring)

```yaml
# dashboard.json
{
  "panels": [
    {
      "title": "Service Health Over Time",
      "targets": [
        {
          "expr": "count(diagnostics{status='failed'}) by (service)",
          "legendFormat": "{{service}}"
        }
      ]
    }
  ]
}
```

---

## Security & Privacy

### Sensitive Data Handling

**DO NOT log:**
- API keys, tokens, passwords
- User credentials
- PII (emails, names, addresses)
- Private keys or certificates

**Safe to log:**
- Service names and endpoints
- HTTP status codes
- Resource metrics (CPU, memory)
- Anonymized error messages

### Artifact Permissions

```bash
# Secure proofs directory
chmod 750 volumes/proofs/
chown root:docker volumes/proofs/

# Screenshots readable by docker group only
find volumes/proofs/ -name "*.png" -exec chmod 640 {} \;
```

---

## Troubleshooting

### Issue: Screenshots are JSON error files

**Symptom:**
```bash
$ file volumes/proofs/*.png
volumes/proofs/service_123.png: JSON text data
```

**Cause:** Playwright browser navigation failed

**Solutions:**
1. Check if service is accessible from probe-orchestrator network
2. Verify service has no SSL/TLS errors
3. Increase navigation timeout in Application.kt
4. Check playwright service is healthy: `docker ps --filter name=playwright`

### Issue: No artifacts generated

**Symptom:** `/proofs` directory empty after probe

**Causes & Solutions:**
1. **Volume not mounted:** Check `docker-compose.yml` volumes section
2. **Permission denied:** `docker exec probe-orchestrator touch /proofs/test.txt`
3. **Probe never ran:** Check probe-orchestrator logs for errors
4. **Path misconfiguration:** Verify `PROOFS_DIR` environment variable

### Issue: Large artifact storage

**Symptom:** `/proofs` consuming excessive disk space

**Solutions:**
```bash
# Check size
du -sh volumes/proofs/

# Clean old artifacts
find volumes/proofs/ -mtime +7 -delete

# Compress old reports
find volumes/proofs/ -name "*.json" -mtime +7 -exec gzip {} \;
```

---

## Future Enhancements

### Phase 2: Enhanced Persistence
- [ ] Structured logging to ClickHouse
- [ ] Screenshot diffs (before/after fixes)
- [ ] Video recordings of failure reproduction
- [ ] Network traffic captures (tcpdump)

### Phase 3: Pattern Learning
- [ ] Build fix success rate database
- [ ] Anomaly detection from historical metrics
- [ ] Auto-categorize failure modes
- [ ] Suggest proactive fixes before failure

### Phase 4: Multi-Stack Support
- [ ] Compare diagnostics across dev/staging/prod
- [ ] Diff configurations between environments
- [ ] Share artifact repositories across teams

---

## References

- Main documentation: `docs/AUTONOMOUS_DIAGNOSTICS.md`
- Evaluation report: `DIAGNOSTIC_AGENT_EVALUATION.md`
- Configuration: `configs/probe-orchestrator/services_manifest.json`
- Review CLI: `scripts/supervisor-session.sh review`
