version: "3.9"

# Minimal bootstrap stack to run locally after a fresh Debian install.
# Goal: Provide Open WebUI + LiteLLM + LocalAI without any external auth or TLS.
# Usage:
#   - Copy/clone repo to host
#   - Run scripts/setup-host.sh (once) to install Docker
#   - Run scripts/bootstrap-stack.sh up-bootstrap
#   - Visit http://<host>:8080

networks:
  backend: {}

volumes:
  litellm_config: {}
  localai_models: {}
  open_webui_data: {}

services:
  localai:
    image: localai/localai:latest-aio-cpu
    container_name: localai
    restart: unless-stopped
    networks: [backend]
    # Expose for troubleshooting (optional)
    ports:
      - "18080:8080"
    volumes:
      - localai_models:/build/models
      - ./models:/models
    environment:
      HUGGINGFACEHUB_API_TOKEN: ${HUGGINGFACEHUB_API_TOKEN:-}
      # Adjust preload list to your needs; the image will download models on demand.
      PRELOAD_MODELS: '[{"id":"llama3"},{"id":"router"},{"id":"qwen-coder"},{"id":"embed"}]'

  litellm:
    image: ghcr.io/berriai/litellm:v1.74.9-stable.patch.1
    container_name: litellm
    restart: unless-stopped
    networks: [backend]
    depends_on:
      - localai
    ports:
      - "4000:4000"
    volumes:
      - litellm_config:/app/config
      - ./configs/litellm/config.bootstrap.yaml:/app/config.yaml:ro
    environment:
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}
      - DATABASE_URL=sqlite:////app/config/litellm.db
      - PROXY_BASE_URL=http://localhost:4000
    command: --config /app/config.yaml --port 4000 --num_workers 2

  open-webui:
    image: ghcr.io/open-webui/open-webui:0.3.32
    container_name: open-webui
    restart: unless-stopped
    networks: [backend]
    depends_on:
      - litellm
    ports:
      - "8080:8080"
    environment:
      - WEBUI_URL=http://localhost:8080
      - ENABLE_OAUTH_SIGNUP=false
      - OAUTH_MERGE_ACCOUNTS_BY_EMAIL=false
      - OAUTH_CLIENT_ID=
      - OAUTH_CLIENT_SECRET=
      - OPENID_PROVIDER_URL=
      - OAUTH_PROVIDER_NAME=
      - OAUTH_SCOPES=
      - OAUTH_GROUPS_CLAIM=
      - DEFAULT_USER_ROLE=admin
      - OPENAI_API_BASE_URL=http://litellm:4000
      - OPENAI_API_KEY=${LITELLM_MASTER_KEY}
    volumes:
      - open_webui_data:/app/backend/data

  kfuncdb:
    build: ./kfuncdb
    container_name: kfuncdb
    restart: unless-stopped
    networks: [backend]
    user: "1000:1000"
    environment:
      - KFUNCDB_PORT=8081
      - KFUNCDB_PLUGINS_DIR=/app/plugins
      - KFUNCDB_ALLOW_CAPS=host.shell.read,host.docker.inspect
    volumes:
      - ${DOCKER_SOCKET:-/run/user/1000/docker.sock}:/var/run/docker.sock:ro
    ports:
      - "18081:8081"
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8081/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
