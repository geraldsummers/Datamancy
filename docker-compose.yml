###############################################
#Important Notes:
#All changes need to be here or in the template files
#Within stack-controller.main.kts is a command called obliterate which will delete all containers, networks, volumes, and images approximating the change to a new computer
###############################################
networks:
  frontend:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/24
          gateway: 172.20.0.1
  backend:
    driver: bridge
    ipam:
      config:
        - subnet: 172.21.0.0/24
          gateway: 172.21.0.1
  database:
    driver: bridge
    ipam:
      config:
        - subnet: 172.22.0.0/24
          gateway: 172.22.0.1

volumes:
  caddy_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${HOME}/.caddy_data
  caddy_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/caddy_config
  grafana_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/grafana_data
  open_webui_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/open_webui_data
  vaultwarden_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/vaultwarden_data
  planka_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/planka_data
  bookstack_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/bookstack_data
  seafile_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/seafile_data
  onlyoffice_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/onlyoffice_data
  synapse_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/synapse_data
  element_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/element_data
  mastodon_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/mastodon_data
  jupyterhub_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/jupyterhub_data
  homeassistant_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/homeassistant_config
  litellm_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/litellm_config
  ldap_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/ldap_data
  ldap_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/ldap_config
  lam_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/lam_config
  mariadb_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/mariadb_data
  postgres_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/postgres_data
  couchdb_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/couchdb_data
  clickhouse_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/clickhouse_data
  qdrant_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/qdrant_data
  benthos_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/benthos_data
  kopia_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/kopia_data
  kopia_cache:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/kopia_cache
  kopia_repository:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/kopia_repository
  redis_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/redis_data
  mailu_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/mailu_data
  mailu_filter:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/mailu_filter
  mailu_cert:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/mailu_cert
  mailu_dkim:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/mailu_dkim
  sogo_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/sogo_data
  forgejo_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/forgejo_data
  qbittorrent_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/qbittorrent_config
  qbittorrent_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/qbittorrent_data
  proofs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT:-./volumes}/proofs

services:
  caddy:
    image: caddy:2.8.4
    container_name: caddy
    restart: unless-stopped
    networks:
      frontend: {}
      backend:
        aliases:
          - www.${DOMAIN}
          - ${DOMAIN}
          - grafana.${DOMAIN}
          - open-webui.${DOMAIN}
          - app.vaultwarden.${DOMAIN}
          - vaultwarden.${DOMAIN}
          - planka.${DOMAIN}
          - bookstack.${DOMAIN}
          - jupyterhub.${DOMAIN}
          - homepage.${DOMAIN}
          - seafile.${DOMAIN}
          - onlyoffice.${DOMAIN}
          - matrix.${DOMAIN}
          - element.${DOMAIN}
          - sogo.${DOMAIN}
          - homeassistant.${DOMAIN}
          - litellm.${DOMAIN}
          - api.litellm.${DOMAIN}
          - auth.${DOMAIN}
          - kopia.${DOMAIN}
          - portainer.${DOMAIN}
          - mail.${DOMAIN}
          - agent-tool-server.${DOMAIN}
          - lam.${DOMAIN}
          - forgejo.${DOMAIN}
          - qbittorrent.${DOMAIN}
    ports:
      - "80:80"
      - "443:443"
      - "443:443/udp"
    volumes:
      - caddy_data:/data
      - caddy_config:/config
      # Provide the static Caddyfile. Defaults to self-signed (internal CA).
      # To enable Let's Encrypt for a specific site, edit configs/infrastructure/caddy/Caddyfile
      # and add `tls you@example.com` inside that site's block.
      - ${HOME}/.datamancy/configs/infrastructure/caddy/Caddyfile:/etc/caddy/Caddyfile:ro
      # Custom landing page for Vaultwarden SSO auto-redirect
      - ${HOME}/.datamancy/configs/applications/vaultwarden/index.html:/srv/vaultwarden/index.html:ro
    environment:
      # Domain for vhosts in Caddyfile
      DOMAIN: "${DOMAIN}"
      # Default IP allowlist for api.litellm subdomain (space-separated CIDRs)
      API_LITELLM_ALLOWLIST: "${API_LITELLM_ALLOWLIST:-10.0.0.0/8 172.16.0.0/12 192.168.0.0/16 100.64.0.0/10 ::1 fc00::/7 fe80::/10}"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      # Use the caddy binary as a lightweight readiness signal
      test: ["CMD", "caddy", "version"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s


  ldap:
    image: osixia/openldap:1.5.0
    container_name: ldap
    restart: unless-stopped
    networks:
      - backend
    environment:
      - LDAP_ORGANISATION=Datamancy
      - LDAP_DOMAIN=stack.local
      - LDAP_BASE_DN=dc=stack,dc=local
      - LDAP_ADMIN_PASSWORD=${STACK_ADMIN_PASSWORD}
      - LDAP_CONFIG_PASSWORD=${STACK_ADMIN_PASSWORD}
      - LDAP_TLS=false
      # Configure memberOf overlay for Authelia group membership detection
      - LDAP_MEMBEROF_OVERLAY=true
      - LDAP_MEMBEROF_GROUP_OC=groupOfNames
      - LDAP_MEMBEROF_MEMBER_AD=member
    volumes:
      - ldap_data:/var/lib/ldap
      - ldap_config:/etc/ldap/slapd.d
      - ${HOME}/.datamancy/bootstrap_ldap.ldif:/container/service/slapd/assets/config/bootstrap/ldif/custom/bootstrap_ldap.ldif:ro
    command: --copy-service --loglevel info
    healthcheck:
      test: ["CMD", "ldapsearch", "-x", "-H", "ldap://localhost:389", "-b", "dc=stack,dc=local", "-D", "cn=admin,dc=stack,dc=local", "-w", "${STACK_ADMIN_PASSWORD}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  ldap-account-manager:
    image: ghcr.io/ldapaccountmanager/lam:9.3
    container_name: ldap-account-manager
    restart: unless-stopped
    networks:
      - backend
      - frontend
    depends_on:
      ldap:
        condition: service_healthy
      authelia:
        condition: service_healthy
    environment:
      - LDAP_DOMAIN=stack.local
      - LDAP_BASE_DN=dc=stack,dc=local
      - LDAP_USER=cn=admin,dc=stack,dc=local
      - LDAP_PASSWORD=${STACK_ADMIN_PASSWORD}
      - LAM_PASSWORD=${STACK_ADMIN_PASSWORD}
      - LAM_LANG=en_US
    # Note: No volume mount - LAM uses container-internal storage and connects to persistent LDAP
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:80/lam/"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  valkey:
    image: valkey/valkey:8.0.1
    container_name: valkey
    restart: unless-stopped
    networks:
      backend:
        aliases:
          - redis
          - redis-synapse
          - mailu-redis
    volumes:
      - redis_data:/data
    command: valkey-server --appendonly yes --dir /data --databases 16
    healthcheck:
      test: ["CMD", "valkey-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  authelia:
    image: authelia/authelia:4.39.13
    container_name: authelia
    restart: unless-stopped
    networks:
      - backend
      - frontend
      - database
    depends_on:
      ldap:
        condition: service_healthy
      valkey:
        condition: service_healthy
      postgres:
        condition: service_healthy
    volumes:
      - ${VOLUMES_ROOT}/authelia:/config
      - ${HOME}/.datamancy/configs/applications/authelia/configuration.yml:/config/configuration.yml:ro
      - ${HOME}/.datamancy/configs/applications/authelia/entrypoint.sh:/entrypoint.sh:ro
    entrypoint: ["/entrypoint.sh"]
    command: ["--config", "/config/configuration.yml"]
    environment:
      - TZ=UTC
      - AUTHELIA_JWT_SECRET=${AUTHELIA_JWT_SECRET}
      - AUTHELIA_SESSION_SECRET=${AUTHELIA_SESSION_SECRET}
      - AUTHELIA_STORAGE_ENCRYPTION_KEY=${AUTHELIA_STORAGE_ENCRYPTION_KEY}
      - AUTHELIA_AUTHENTICATION_BACKEND_LDAP_PASSWORD=${STACK_ADMIN_PASSWORD}
      - AUTHELIA_IDENTITY_PROVIDERS_OIDC_HMAC_SECRET=${AUTHELIA_OIDC_HMAC_SECRET}
      - AUTHELIA_IDENTITY_PROVIDERS_OIDC_ISSUER_PRIVATE_KEY=${AUTHELIA_IDENTITY_PROVIDERS_OIDC_ISSUER_PRIVATE_KEY}
      - AUTHELIA_STORAGE_POSTGRES_HOST=postgres
      - AUTHELIA_STORAGE_POSTGRES_PORT=5432
      - AUTHELIA_STORAGE_POSTGRES_DATABASE=authelia
      - AUTHELIA_STORAGE_POSTGRES_USERNAME=authelia
      - AUTHELIA_STORAGE_POSTGRES_PASSWORD=${AUTHELIA_DB_PASSWORD}
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9091/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  mariadb:
    image: mariadb:11.6.2
    container_name: mariadb
    restart: unless-stopped
    networks:
      database: {}
      backend:
        aliases:
          - mysql
    environment:
      - MYSQL_ROOT_PASSWORD=${STACK_ADMIN_PASSWORD}
      - MYSQL_DATABASE=datamancy
      - MYSQL_USER=${STACK_ADMIN_USER}
      - MYSQL_PASSWORD=${STACK_ADMIN_PASSWORD}
    volumes:
      - mariadb_data:/var/lib/mysql
      - ${HOME}/.datamancy/configs/databases/mariadb/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    command: --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci
    healthcheck:
      test: ["CMD", "mariadb-admin", "ping", "-h", "localhost", "-u", "root", "-p${STACK_ADMIN_PASSWORD}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  postgres:
    image: postgres:16.11
    container_name: postgres
    restart: unless-stopped
    networks:
      database:
        aliases:
          - db
      backend: {}
    environment:
      - POSTGRES_PASSWORD=${STACK_ADMIN_PASSWORD}
      - POSTGRES_USER=${STACK_ADMIN_USER}
      - POSTGRES_DB=postgres
      - STACK_ADMIN_PASSWORD=${STACK_ADMIN_PASSWORD}
      - PLANKA_DB_PASSWORD=${PLANKA_DB_PASSWORD}
      - SYNAPSE_DB_PASSWORD=${SYNAPSE_DB_PASSWORD}
      - MAILU_DB_PASSWORD=${MAILU_DB_PASSWORD}
      - AUTHELIA_DB_PASSWORD=${AUTHELIA_DB_PASSWORD}
      - GRAFANA_DB_PASSWORD=${GRAFANA_DB_PASSWORD}
      - VAULTWARDEN_DB_PASSWORD=${VAULTWARDEN_DB_PASSWORD}
      - OPENWEBUI_DB_PASSWORD=${OPENWEBUI_DB_PASSWORD}
      - MASTODON_DB_PASSWORD=${MASTODON_DB_PASSWORD}
      - FORGEJO_DB_PASSWORD=${FORGEJO_DB_PASSWORD}
      - HOMEASSISTANT_DB_PASSWORD=${HOMEASSISTANT_DB_PASSWORD:-}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ${HOME}/.datamancy/configs/databases/postgres/init-db.sh:/docker-entrypoint-initdb.d/init-db.sh:ro
      - ${HOME}/.datamancy/configs/databases/postgres/init-mailu-schema.sql:/docker-entrypoint-initdb.d/init-mailu-schema.sql:ro
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
    command: postgres -c 'max_connections=300' -c 'shared_buffers=1GB'
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${STACK_ADMIN_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  couchdb:
    image: couchdb:3.5.1
    container_name: couchdb
    restart: unless-stopped
    networks:
      - database
    environment:
      - COUCHDB_USER=${STACK_ADMIN_USER}
      - COUCHDB_PASSWORD=${STACK_ADMIN_PASSWORD}
    volumes:
      - couchdb_data:/opt/couchdb/data
      - ${HOME}/.datamancy/configs/databases/couchdb/init-wrapper.sh:/usr/local/bin/init-wrapper.sh:ro
    entrypoint: ["/bin/bash", "/usr/local/bin/init-wrapper.sh"]
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://127.0.0.1:5984/_up | grep -q '\"status\":\"ok\"' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s


  memcached:
    image: memcached:1.6.39
    container_name: seafile-memcached
    restart: unless-stopped
    networks:
      - backend
    healthcheck:
      test: ["CMD-SHELL", "timeout 1 bash -c 'cat < /dev/null > /dev/tcp/127.0.0.1/11211'"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  mailu-front:
    image: ghcr.io/mailu/nginx:2024.06.45
    container_name: mailu-front
    restart: unless-stopped
    env_file:
      - ${HOME}/.datamancy/configs/applications/mailu/mailu.env
    ports:
      - "25:25"
      - "465:465"
      - "587:587"
      - "110:110"
      - "995:995"
      - "143:143"
      - "993:993"
    volumes:
      - mailu_cert:/certs
      - mailu_dkim:/dkim
    networks:
      backend:
        aliases:
          - front
    depends_on:
      mailu-admin:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  mailu-antispam:
    image: ghcr.io/mailu/rspamd:2024.06.45
    container_name: mailu-antispam
    restart: unless-stopped
    env_file:
      - ${HOME}/.datamancy/configs/applications/mailu/mailu.env
    volumes:
      - mailu_filter:/var/lib/rspamd
    networks:
      - backend
    depends_on:
      mailu-admin:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "sh", "-c", "pgrep rspamd"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  mailu-antivirus:
    image: ghcr.io/mailu/clamav:2.0.43
    container_name: mailu-antivirus
    restart: unless-stopped
    env_file:
      - ${HOME}/.datamancy/configs/applications/mailu/mailu.env
    networks:
      - backend
    healthcheck:
      test: ["CMD", "clamdscan", "--ping", "1"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 5m

  mailu-imap:
    image: ghcr.io/mailu/dovecot:2024.06.45
    container_name: mailu-imap
    restart: unless-stopped
    env_file:
      - ${HOME}/.datamancy/configs/applications/mailu/mailu.env
    volumes:
      - mailu_data:/data
    networks:
      - backend
    depends_on:
      mailu-admin:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "doveadm", "log", "test"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  mailu-smtp:
    image: ghcr.io/mailu/postfix:2024.06.45
    container_name: mailu-smtp
    restart: unless-stopped
    env_file:
      - ${HOME}/.datamancy/configs/applications/mailu/mailu.env
    volumes:
      - mailu_data:/data
      - mailu_dkim:/dkim
    networks:
      - backend
    depends_on:
      mailu-admin:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "postfix", "status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  mailu-admin:
    image: ghcr.io/mailu/admin:2024.06.45
    container_name: mailu-admin
    restart: unless-stopped
    env_file:
      - ${HOME}/.datamancy/configs/applications/mailu/mailu.env
    volumes:
      - mailu_data:/data
    networks:
      backend:
        aliases:
          - admin
    depends_on:
      postgres:
        condition: service_healthy
      valkey:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "-L", "http://localhost:8080/admin/ui"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s


  sogo:
    image: pmietlicki/sogo:v5
    container_name: sogo
    restart: unless-stopped
    environment:
      - TZ=UTC
    volumes:
      - sogo_data:/var/lib/sogo
      - ${HOME}/.datamancy/configs/applications/sogo/sogo.conf:/etc/sogo/sogo.conf:ro
      - ${HOME}/.datamancy/configs/applications/sogo/init-apache.sh:/docker-entrypoint.d/init-apache.sh:ro
    depends_on:
      postgres:
        condition: service_healthy
      ldap:
        condition: service_healthy
      mailu-imap:
        condition: service_healthy
      mailu-smtp:
        condition: service_healthy
    networks:
      - backend
    healthcheck:
      test: ["CMD-SHELL", "pgrep sogod > /dev/null || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  sogo-init:
    image: docker:27-cli
    container_name: sogo-init
    restart: "no"
    depends_on:
      sogo:
        condition: service_healthy
    volumes:
      - ${DOCKER_SOCKET:-/var/run/docker.sock}:/var/run/docker.sock:ro
    command: docker exec sogo bash /docker-entrypoint.d/init-apache.sh


  kopia:
    image: kopia/kopia:0.18.2
    container_name: kopia
    restart: "no"  # Manual start after all services are up to avoid volume mount race conditions
    networks:
      - backend
    volumes:
      - kopia_data:/app/config
      - kopia_cache:/app/cache
      # Mount volumes to backup (read-only)
      - postgres_data:/backup/postgres:ro
      - mariadb_data:/backup/mariadb:ro
      - grafana_data:/backup/grafana:ro
      - open_webui_data:/backup/open_webui:ro
      - vaultwarden_data:/backup/vaultwarden:ro
      - kopia_repository:/repository
      - ${HOME}/.datamancy/configs/applications/kopia/init-kopia.sh:/init-kopia.sh:ro
    environment:
      - KOPIA_PASSWORD=${STACK_ADMIN_PASSWORD}
      - KOPIA_REPO_PATH=/repository
      - USER=kopia
    entrypoint: ["sh", "/init-kopia.sh"]
    healthcheck:
      test: ["CMD", "kopia", "repository", "status"]
      interval: 5m
      timeout: 30s
      retries: 3
      start_period: 1m

  docker-proxy:
    image: tecnativa/docker-socket-proxy:v0.4.1
    container_name: docker-proxy
    restart: unless-stopped
    networks:
      - backend
    environment:
      # Minimal read-only permissions for security
      # See: https://github.com/Tecnativa/docker-socket-proxy
      - CONTAINERS=1  # Read container status (needed for health checks, logs)
      - IMAGES=1      # Enabled for JupyterHub spawner to check image availability
      - NETWORKS=1    # Enabled for JupyterHub spawner to attach containers
      - VOLUMES=1     # Enabled for JupyterHub to mount volumes
      - POST=1        # Enabled for JupyterHub to spawn notebook containers
      - DELETE=1      # Enabled for JupyterHub to stop/remove containers
      - BUILD=0       # Disabled - no image builds
      - COMMIT=0
      - CONFIGS=0
      - DISTRIBUTION=0
      - EXEC=0        # Disabled - prevents arbitrary command execution
      - GRPC=0
      - INFO=1        # Enabled for JupyterHub to query Docker info
      - PLUGINS=0
      - SERVICES=0
      - SESSION=0
      - SWARM=0
      - SYSTEM=0
      - TASKS=0
      - VERSION=1     # Enabled - allows version checks for compatibility
    volumes:
      - ${DOCKER_SOCKET:-/var/run/docker.sock}:/var/run/docker.sock:ro
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:2375/version"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  grafana:
    image: grafana/grafana:11.6.7
    container_name: grafana
    restart: unless-stopped
    networks:
      - backend
      - database
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    depends_on:
      postgres:
        condition: service_healthy
      authelia:
        condition: service_healthy
    environment:
      - GF_DEFAULT_LOCALE=en_US
      - GF_SERVER_ROOT_URL=https://grafana.${DOMAIN}
      - GF_SERVER_DOMAIN=grafana.${DOMAIN}
      - GF_DATABASE_TYPE=postgres
      - GF_DATABASE_HOST=postgres:5432
      - GF_DATABASE_NAME=grafana
      - GF_DATABASE_USER=grafana
      - GF_DATABASE_PASSWORD=${GRAFANA_DB_PASSWORD}
      - GF_DATABASE_SSL_MODE=disable
      - GF_AUTH_GENERIC_OAUTH_ENABLED=true
      - GF_AUTH_GENERIC_OAUTH_NAME=Authelia
      - GF_AUTH_GENERIC_OAUTH_CLIENT_ID=grafana
      - GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET=${GRAFANA_OAUTH_SECRET}
      - GF_AUTH_GENERIC_OAUTH_SCOPES=openid profile email groups
      - GF_AUTH_GENERIC_OAUTH_AUTH_URL=https://auth.${DOMAIN}/api/oidc/authorization
      - GF_AUTH_GENERIC_OAUTH_TOKEN_URL=http://authelia:9091/api/oidc/token
      - GF_AUTH_GENERIC_OAUTH_API_URL=http://authelia:9091/api/oidc/userinfo
      - GF_AUTH_GENERIC_OAUTH_LOGIN_ATTRIBUTE_PATH=email
      - GF_AUTH_GENERIC_OAUTH_EMAIL_ATTRIBUTE_PATH=email
      - GF_AUTH_GENERIC_OAUTH_GROUPS_ATTRIBUTE_PATH=groups
      - GF_AUTH_GENERIC_OAUTH_NAME_ATTRIBUTE_PATH=name
      - GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_PATH=contains(groups[*], 'admins') && 'Admin' || contains(groups[*], 'users') && 'Editor' || 'Viewer'
      - GF_AUTH_GENERIC_OAUTH_USE_PKCE=true
      - GF_AUTH_GENERIC_OAUTH_AUTH_STYLE=2  # 2 = pass client credentials in POST body (client_secret_post)
      - GF_AUTH_SIGNOUT_REDIRECT_URL=https://auth.${DOMAIN}/logout
      - GF_AUTH_GENERIC_OAUTH_SKIP_ORG_ROLE_SYNC=false
      - GF_AUTH_GENERIC_OAUTH_AUTO_LOGIN=false
      - GF_AUTH_GENERIC_OAUTH_ALLOW_SIGN_UP=true
      - GF_USERS_ALLOW_SIGN_UP=true
      - GF_USERS_AUTO_ASSIGN_ORG=true
      - GF_USERS_AUTO_ASSIGN_ORG_ROLE=Viewer
      - GF_AUTH_DISABLE_LOGIN_FORM=false
      - GF_SECURITY_ADMIN_USER=${STACK_ADMIN_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${STACK_ADMIN_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana
      - ${HOME}/.datamancy/configs/applications/grafana/grafana.ini:/etc/grafana/grafana.ini:ro
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  open-webui:
     image: ghcr.io/open-webui/open-webui:0.3.32
     container_name: open-webui
     restart: unless-stopped
     networks:
       - backend
       - frontend
       - database
     dns:
       - 8.8.8.8
       - 1.1.1.1
     depends_on:
       postgres:
         condition: service_healthy
       litellm:
         condition: service_healthy
       authelia:
         condition: service_healthy
     environment:
       - WEBUI_URL=${OPENWEBUI_WEBUI_URL:-https://open-webui.${DOMAIN}}
       # PostgreSQL database - use URL-encoded password for special characters
       - DATABASE_URL=postgresql://openwebui:${OPENWEBUI_DB_PASSWORD_ENCODED}@postgres:5432/openwebui
       - ENABLE_OAUTH_SIGNUP=${OPENWEBUI_ENABLE_OAUTH_SIGNUP:-true}
       - DEFAULT_USER_ROLE=${OPENWEBUI_DEFAULT_USER_ROLE:-admin}
       - ENABLE_OPENAI_API=true
       - OPENAI_API_BASE_URL=${OPENWEBUI_OPENAI_API_BASE_URL:-http://litellm:4000/v1}
       - OPENAI_API_KEY=${LITELLM_MASTER_KEY}
       # OIDC SSO Configuration
       - OPENID_PROVIDER_URL=https://auth.${DOMAIN}
       - OAUTH_CLIENT_ID=open-webui
       - OAUTH_CLIENT_SECRET=${OPENWEBUI_OAUTH_SECRET}
       - OAUTH_PROVIDER_NAME=Authelia
       - OAUTH_SCOPES=openid profile email groups
       # Seamless SSO - skip login form, go straight to OAuth
       - ENABLE_LOGIN_FORM=false
       - WEBUI_AUTH=false
     volumes:
       - open_webui_data:/app/backend/data
     healthcheck:
       test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
       interval: 30s
       timeout: 10s
       retries: 3
       start_period: 30s

  vaultwarden:
    image: vaultwarden/server:testing
    container_name: vaultwarden
    restart: unless-stopped
    networks:
      - backend
      - database
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    depends_on:
      postgres:
        condition: service_healthy
      authelia:
        condition: service_healthy
      mailu-smtp:
        condition: service_healthy
    environment:
      - DOMAIN=https://app.vaultwarden.${DOMAIN}
      - DATABASE_URL=postgresql://vaultwarden:${VAULTWARDEN_DB_PASSWORD}@postgres:5432/vaultwarden
      - SIGNUPS_ALLOWED=false
      - SIGNUPS_VERIFY=true
      - SIGNUPS_DOMAINS_WHITELIST=${MAIL_DOMAIN}
      - INVITATIONS_ALLOWED=false
      - SHOW_PASSWORD_HINT=false
      # SSO/OIDC Configuration
      - SSO_ENABLED=true
      - SSO_ONLY=true
      - SSO_AUTHORITY=https://auth.${DOMAIN}
      - SSO_CLIENT_ID=vaultwarden
      - SSO_CLIENT_SECRET=${VAULTWARDEN_OAUTH_SECRET}
      - SSO_SCOPES=openid email profile
      - SSO_CALLBACK_PATH=/identity/connect/oidc-signin
      - SSO_SIGNUPS_MATCH_EMAIL=true
      # Admin and SMTP
      - ADMIN_TOKEN=${VAULTWARDEN_ADMIN_TOKEN}
      - SMTP_HOST=mailu-smtp
      - SMTP_FROM=vaultwarden@${MAIL_DOMAIN}
      - SMTP_PORT=587
      - SMTP_SECURITY=starttls
      - SMTP_USERNAME=vaultwarden@${MAIL_DOMAIN}
      - SMTP_PASSWORD=${VAULTWARDEN_SMTP_PASSWORD}
    volumes:
      - vaultwarden_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/alive"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  planka:
    image: ghcr.io/plankanban/planka:1.26.3
    container_name: planka
    restart: unless-stopped
    networks:
      - backend
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    depends_on:
      postgres:
        condition: service_healthy
      authelia:
        condition: service_healthy
    environment:
      - BASE_URL=https://planka.${DOMAIN}
      # Use URL-encoded password for PostgreSQL connection string (encoded version of PLANKA_DB_PASSWORD)
      - DATABASE_URL=postgresql://planka:${PLANKA_DB_PASSWORD_ENCODED:-${PLANKA_DB_PASSWORD}}@postgres:5432/planka
      - SECRET_KEY=${PLANKA_SECRET_KEY}
      - OIDC_ISSUER=https://auth.${DOMAIN}
      - OIDC_CLIENT_ID=planka
      - OIDC_CLIENT_SECRET=${PLANKA_OAUTH_SECRET}
      - OIDC_SCOPES=openid profile email groups
      - OIDC_ADMIN_ROLES=admins
      - OIDC_EMAIL_ATTRIBUTE=email
      - OIDC_NAME_ATTRIBUTE=name
      - OIDC_USERNAME_ATTRIBUTE=preferred_username
      - OIDC_REDIRECT_URI=https://planka.${DOMAIN}/oidc-callback
      - OIDC_AUTHORIZATION_ENDPOINT=https://auth.${DOMAIN}/api/oidc/authorization
      - OIDC_TOKEN_ENDPOINT=http://authelia:9091/api/oidc/token
      - OIDC_USERINFO_ENDPOINT=http://authelia:9091/api/oidc/userinfo
      - OIDC_TOKEN_ENDPOINT_AUTH_METHOD=client_secret_post
      - OIDC_ENFORCED=false
      - NODE_EXTRA_CA_CERTS=/usr/local/share/ca-certificates/caddy-ca.crt
    volumes:
      - planka_data:/app/public/user-avatars
      - ${HOME}/.datamancy/configs/applications/planka/caddy-ca.crt:/usr/local/share/ca-certificates/caddy-ca.crt:ro
    healthcheck:
      test: ["CMD", "wget", "-q", "-O", "/dev/null", "http://localhost:1337/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  bookstack:
    image: lscr.io/linuxserver/bookstack:24.12.1
    container_name: bookstack
    hostname: bookstack
    restart: unless-stopped
    networks:
      - backend
    links:
      - mariadb:mariadb
    depends_on:
      mariadb:
        condition: service_healthy
      authelia:
        condition: service_healthy
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=UTC
      - APP_URL=https://bookstack.${DOMAIN}
      - APP_KEY=${BOOKSTACK_APP_KEY}
      - DB_HOST=mariadb
      - DB_PORT=3306
      - DB_USER=bookstack
      - DB_PASS=${BOOKSTACK_DB_PASSWORD}
      - DB_DATABASE=bookstack
      - LDAP_ADMIN_PASSWORD=${LDAP_ADMIN_PASSWORD}
      - BOOKSTACK_OAUTH_SECRET=${BOOKSTACK_OAUTH_SECRET}
      # OIDC SSO configuration - init script will configure .env file
      # AUTH_METHOD, OIDC_* settings are configured by 50-fix-env.sh
    volumes:
      - bookstack_data:/config
      - ${VOLUMES_ROOT}/bookstack_init:/custom-cont-init.d:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 90s

  seafile:
    image: seafileltd/seafile-mc:13.0.12
    container_name: seafile
    restart: unless-stopped
    depends_on:
      memcached:
        condition: service_healthy
      mariadb:
        condition: service_healthy
      authelia:
        condition: service_healthy
    environment:
      - TZ=UTC
      - DB_HOST=mysql  # Use alias for mariadb-seafile
      - DB_ROOT_PASSWD=${STACK_ADMIN_PASSWORD}
      - SEAFILE_MYSQL_DB_HOST=mysql
      - SEAFILE_MYSQL_DB_PORT=3306
      - SEAFILE_MYSQL_DB_USER=seafile
      - SEAFILE_MYSQL_DB_PASSWORD=${MARIADB_SEAFILE_PASSWORD}
      - INIT_SEAFILE_MYSQL_ROOT_PASSWORD=${STACK_ADMIN_PASSWORD}
      - SEAFILE_SERVER_HOSTNAME=seafile.${DOMAIN}
      - SEAFILE_ADMIN_EMAIL=${STACK_ADMIN_EMAIL}
      - SEAFILE_ADMIN_PASSWORD=${STACK_ADMIN_PASSWORD}
      - JWT_PRIVATE_KEY=${SEAFILE_JWT_KEY}
      - TIME_ZONE=UTC
    volumes:
      - seafile_data:/shared
      # Mount database settings template (will be applied by init container)
      - ${HOME}/.datamancy/configs/applications/seafile/seahub_settings.py:/tmp/seahub_settings_template.py:ro
    networks:
      - backend
    healthcheck:
      test: ["CMD-SHELL", "curl -s -o /dev/null -w '%{http_code}' http://localhost:8000/ | grep -E '^(200|302)$'"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 180s

  seafile-db-init:
    image: seafileltd/seafile-mc:13.0.12
    container_name: seafile-db-init
    restart: "no"
    depends_on:
      seafile:
        condition: service_started
    volumes:
      - seafile_data:/shared
      - ${HOME}/.datamancy/configs/applications/seafile/seahub_settings.py:/tmp/seahub_settings_template.py:ro
      - ${HOME}/.datamancy/configs/applications/seafile/init/apply-db-config.sh:/apply-db-config.sh:ro
    command: ["bash", "/apply-db-config.sh"]
    networks:
      - backend

  onlyoffice:
    image: onlyoffice/documentserver:8.2.2
    container_name: onlyoffice
    restart: unless-stopped
    environment:
      - TZ=UTC
      - JWT_ENABLED=true
      - JWT_SECRET=${ONLYOFFICE_JWT_SECRET}
      - JWT_HEADER=Authorization
    volumes:
      - onlyoffice_data:/var/www/onlyoffice/Data
    networks:
      - backend
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/healthcheck"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 2m

  synapse-init:
    image: alpine:latest
    container_name: synapse-init
    volumes:
      - synapse_data:/data
    command: chown -R 991:991 /data

  synapse:
    build:
      dockerfile: ./src/synapse/Dockerfile
      context: .
    container_name: synapse
    restart: unless-stopped
    depends_on:
      synapse-init:
        condition: service_completed_successfully
      valkey:
        condition: service_healthy
      postgres:
        condition: service_healthy
      ldap:
        condition: service_healthy
    environment:
      - SYNAPSE_SERVER_NAME=matrix.${DOMAIN}
      - SYNAPSE_REPORT_STATS=yes
      - SYNAPSE_ENABLE_REGISTRATION=${MATRIX_ENABLE_REGISTRATION:-false}
      - SYNAPSE_TRUSTED_PROXIES=172.18.0.0/16,172.21.0.0/24
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=synapse
      - POSTGRES_USER=synapse
      - POSTGRES_PASSWORD=${SYNAPSE_DB_PASSWORD}
      - REDIS_HOST=redis-synapse
      - STACK_ADMIN_PASSWORD=${STACK_ADMIN_PASSWORD}
    volumes:
      - synapse_data:/data
      - ${HOME}/.datamancy/configs/applications/synapse/homeserver.yaml:/data/homeserver.yaml:ro
    networks:
      - backend
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8008/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  element:
    image: vectorim/element-web:v1.11.88
    container_name: element
    restart: unless-stopped
    networks:
      - backend
    depends_on:
      synapse:
        condition: service_healthy
    volumes:
      - element_data:/app/config
      - ${HOME}/.datamancy/configs/applications/element/config.json:/app/config.json:ro
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:80/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  mastodon-web:
    image: ghcr.io/mastodon/mastodon:v4.3.3
    container_name: mastodon-web
    restart: unless-stopped
    networks:
      - backend
    depends_on:
      postgres:
        condition: service_healthy
      valkey:
        condition: service_healthy
      authelia:
        condition: service_healthy
      mailu-smtp:
        condition: service_healthy
    env_file:
      - ${HOME}/.datamancy/configs/applications/mastodon/mastodon.env
    volumes:
      - mastodon_data:/mastodon/public/system
      - ${HOME}/.datamancy/configs/applications/mastodon/entrypoint.sh:/entrypoint.sh:ro
    entrypoint: ["/bin/bash", "/entrypoint.sh"]
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '2.0'
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider --proxy=off localhost:3000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 180s

  mastodon-streaming:
    image: ghcr.io/mastodon/mastodon-streaming:v4.3.3
    container_name: mastodon-streaming
    restart: unless-stopped
    networks:
      - backend
    depends_on:
      mastodon-web:
        condition: service_healthy
      valkey:
        condition: service_healthy
      postgres:
        condition: service_healthy
    env_file:
      - ${HOME}/.datamancy/configs/applications/mastodon/mastodon.env
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider --proxy=off localhost:4000/api/v1/streaming/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  mastodon-sidekiq:
    image: ghcr.io/mastodon/mastodon:v4.3.3
    container_name: mastodon-sidekiq
    restart: unless-stopped
    networks:
      - backend
    depends_on:
      mastodon-web:
        condition: service_healthy
      valkey:
        condition: service_healthy
      postgres:
        condition: service_healthy
    env_file:
      - ${HOME}/.datamancy/configs/applications/mastodon/mastodon.env
    command: bundle exec sidekiq
    volumes:
      - mastodon_data:/mastodon/public/system
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '2.0'
    healthcheck:
      test: ["CMD-SHELL", "bundle exec rails runner 'exit(Sidekiq::ProcessSet.new.size > 0 ? 0 : 1)' || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s

  homepage:
    image: ghcr.io/gethomepage/homepage:v1.7.0
    container_name: homepage
    restart: unless-stopped
    networks:
      - backend
    depends_on:
      docker-proxy:
        condition: service_healthy
    environment:
      - PUID=1000
      - PGID=1000
      - HOMEPAGE_ALLOWED_HOSTS=homepage.${DOMAIN}
      - DOCKER_HOST=tcp://docker-proxy:2375
    volumes:
      - ${HOME}/.datamancy/configs/applications/homepage/services.yaml:/app/config/services.yaml:ro
      - ${HOME}/.datamancy/configs/applications/homepage/settings.yaml:/app/config/settings.yaml:ro
      - ${HOME}/.datamancy/configs/applications/homepage/widgets.yaml:/app/config/widgets.yaml:ro
      - ${HOME}/.datamancy/configs/applications/homepage/bookmarks.yaml:/app/config/bookmarks.yaml:ro
      - ${HOME}/.datamancy/configs/applications/homepage/docker.yaml:/app/config/docker.yaml:ro
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://127.0.0.1:3000/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  jupyterhub:
    build:
      dockerfile: ./src/jupyterhub/Dockerfile
      context: .
    container_name: jupyterhub
    restart: unless-stopped
    networks:
      - backend
    depends_on:
      authelia:
        condition: service_healthy
      docker-proxy:
        condition: service_started
    environment:
      - DOMAIN=${DOMAIN}
      - JUPYTERHUB_OAUTH_SECRET=${JUPYTERHUB_OAUTH_SECRET}
      - DOCKER_HOST=tcp://docker-proxy:2375
    volumes:
      - jupyterhub_data:/srv/jupyterhub
      - ./src/jupyterhub/jupyterhub_config.py:/srv/jupyterhub/jupyterhub_config.py:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/hub/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  homeassistant:
    image: ghcr.io/home-assistant/home-assistant:stable
    container_name: homeassistant
    restart: unless-stopped
    networks:
      - backend
      - database
    depends_on:
      authelia:
        condition: service_healthy
      postgres:
        condition: service_healthy
      ldap:
        condition: service_healthy
    environment:
      - TZ=UTC
      - HOMEASSISTANT_DB_URL=postgresql://${STACK_ADMIN_USER}:${HOMEASSISTANT_DB_PASSWORD}@postgres:5432/homeassistant
      - STACK_ADMIN_USER=${STACK_ADMIN_USER}
      - STACK_ADMIN_PASSWORD=${STACK_ADMIN_PASSWORD}
      - STACK_ADMIN_EMAIL=${STACK_ADMIN_EMAIL}
      - LDAP_HOST=ldap
      - LDAP_PORT=389
      - LDAP_BASE_DN=dc=stack,dc=local
      - LDAP_BIND_DN=cn=admin,dc=stack,dc=local
      - LDAP_BIND_PASSWORD=${STACK_ADMIN_PASSWORD}
      - LDAP_USER_FILTER=(uid={username})
    volumes:
      - homeassistant_config:/config
      - /etc/localtime:/etc/localtime:ro
      - ${HOME}/.datamancy/configs/applications/homeassistant/configuration.yaml:/config/configuration.yaml:ro
      - ${HOME}/.datamancy/configs/applications/homeassistant/automations.yaml:/config/automations.yaml:rw
      - ${HOME}/.datamancy/configs/applications/homeassistant/scripts.yaml:/config/scripts.yaml:rw
      - ${HOME}/.datamancy/configs/applications/homeassistant/scenes.yaml:/config/scenes.yaml:rw
      - ${HOME}/.datamancy/configs/applications/homeassistant/init-homeassistant.sh:/init-homeassistant.sh:ro
      - ${HOME}/.datamancy/configs/applications/homeassistant/entrypoint.sh:/entrypoint-wrapper.sh:ro
      - ${HOME}/.datamancy/configs/applications/homeassistant/auth_ldap.py:/usr/src/homeassistant/homeassistant/auth/providers/auth_ldap.py:ro
    entrypoint: ["/entrypoint-wrapper.sh"]
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8123/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  forgejo:
    image: codeberg.org/forgejo/forgejo:9
    container_name: forgejo
    restart: unless-stopped
    networks:
      - backend
      - database
    depends_on:
      postgres:
        condition: service_healthy
      authelia:
        condition: service_healthy
    environment:
      - USER_UID=1000
      - USER_GID=1000
      - FORGEJO__database__DB_TYPE=postgres
      - FORGEJO__database__HOST=postgres:5432
      - FORGEJO__database__NAME=forgejo
      - FORGEJO__database__USER=forgejo
      - FORGEJO__database__PASSWD=${FORGEJO_DB_PASSWORD}
      - FORGEJO__server__DOMAIN=forgejo.${DOMAIN}
      - FORGEJO__server__SSH_DOMAIN=forgejo.${DOMAIN}
      - FORGEJO__server__ROOT_URL=https://forgejo.${DOMAIN}/
      - FORGEJO__security__INSTALL_LOCK=true
      - FORGEJO__service__DISABLE_REGISTRATION=true
      - FORGEJO__service__REQUIRE_SIGNIN_VIEW=true
      - FORGEJO__openid__ENABLE_OPENID_SIGNIN=true
      - FORGEJO__openid__ENABLE_OPENID_SIGNUP=true
      - FORGEJO_OAUTH_SECRET=${FORGEJO_OAUTH_SECRET}
      - DOMAIN=${DOMAIN}
    volumes:
      - forgejo_data:/data
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  forgejo-init:
    image: codeberg.org/forgejo/forgejo:9
    container_name: forgejo-init
    restart: "no"
    networks:
      - backend
    depends_on:
      forgejo:
        condition: service_healthy
    environment:
      - FORGEJO_OAUTH_SECRET=${FORGEJO_OAUTH_SECRET}
      - DOMAIN=${DOMAIN}
    volumes:
      - ${HOME}/.datamancy/configs/applications/forgejo/init-forgejo.sh:/init-forgejo.sh:ro
    command: ["bash", "/init-forgejo.sh"]

  qbittorrent:
    image: lscr.io/linuxserver/qbittorrent:latest
    container_name: qbittorrent
    restart: unless-stopped
    networks:
      - backend
    depends_on:
      authelia:
        condition: service_healthy
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=UTC
      - WEBUI_PORT=8080
    volumes:
      - qbittorrent_config:/config
      - qbittorrent_data:/downloads
      # Mount pre-configured qBittorrent.conf with subnet whitelist
      - ${VOLUMES_ROOT}/qbittorrent_init:/custom-cont-init.d:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm
    restart: unless-stopped
    networks:
      - backend
    depends_on:
      authelia:
        condition: service_healthy
    environment:
      - HUGGINGFACEHUB_API_TOKEN=${HUGGINGFACEHUB_API_TOKEN}
      - HF_HUB_ENABLE_HF_TRANSFER=0
    command: [
      "--model", "Qwen/Qwen2.5-0.5B-Instruct",
      "--served-model-name", "qwen2.5-0.5b",
      "--max-model-len", "2048",
      "--max-num-seqs", "64",
      "--dtype", "auto",
      "--enable-auto-tool-choice",
      "--tool-call-parser", "hermes",
      "--gpu-memory-utilization", "0.3"
    ]
    volumes:
      - ${VOLUMES_ROOT}/vllm/hf-cache:/root/.cache/huggingface
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4.0'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 60s
      timeout: 10s
      retries: 5
      start_period: 2m

  vllm-router:
    build:
      dockerfile: ./src/vllm-router/Dockerfile
      context: .
    container_name: vllm-router
    restart: unless-stopped
    networks:
      - backend
    depends_on:
      vllm:
        condition: service_healthy
      authelia:
        condition: service_healthy
    environment:
      - VLLM_BASE_URL=http://vllm:8000
      - PORT=8010
    healthcheck:
      test: ["CMD", "wget", "-q", "-O", "/dev/null", "http://localhost:8010/health"]
      interval: 30s
      timeout: 5s
      retries: 10
      start_period: 30s

  embedding-service:
    image: ghcr.io/huggingface/text-embeddings-inference:cpu-1.2
    container_name: embedding-service
    restart: unless-stopped
    networks:
      - backend
    command: --model-id sentence-transformers/all-MiniLM-L6-v2 --port 8080
    volumes:
      - ${VOLUMES_ROOT}/embeddings/models:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  litellm:
    image: ghcr.io/berriai/litellm:v1.74.9-stable.patch.1
    container_name: litellm
    restart: unless-stopped
    networks:
      - backend
      - frontend
    depends_on:
      vllm-router:
        condition: service_healthy
      authelia:
        condition: service_healthy
      embedding-service:
        condition: service_healthy
    volumes:
      - litellm_config:/app/config
      - ${HOME}/.datamancy/configs/infrastructure/litellm/config.yaml:/app/config.yaml:ro
    environment:
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}
      # Consolidated admin credentials for UI login
      - UI_USERNAME=${STACK_ADMIN_USER}
      - UI_PASSWORD=${STACK_ADMIN_PASSWORD}
    command: --config /app/config.yaml --port 4000 --num_workers 4
    healthcheck:
      test: ["CMD-SHELL", "python3 -c 'import socket; s=socket.socket(); s.settimeout(1); s.connect((\"127.0.0.1\", 4000)); s.close()' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 15s

  agent-tool-server:
    build:
      dockerfile: ./src/agent-tool-server/Dockerfile
      context: .
    container_name: agent-tool-server
    restart: unless-stopped
    networks:
      - backend
      - frontend
    user: "1000:1000"
    group_add:
      - "985"  # docker group on host for socket access
    depends_on:
      authelia:
        condition: service_healthy
      playwright:
        condition: service_healthy
      litellm:
        condition: service_healthy
    environment:
      - TOOLSERVER_PORT=8081
      - TOOLSERVER_PLUGINS_DIR=/app/plugins
      - TOOLSERVER_ALLOW_CAPS=host.shell.read,host.docker.write,host.docker.inspect,host.network.http,host.network.ssh
      # SSH to host via forced-command wrapper (see scripts)
      - TOOLSERVER_SSH_HOST=${TOOLSERVER_SSH_HOST:-host.docker.internal}
      - TOOLSERVER_SSH_USER=${TOOLSERVER_SSH_USER:-stackops}
      - TOOLSERVER_SSH_KEY_PATH=/app/keys/stackops_ed25519
      - TOOLSERVER_SSH_KNOWN_HOSTS=/app/known_hosts
      - TOOLSERVER_SSH_TIMEOUT_MS=20000
      # Browser service timeouts (login operations need more time)
      - TOOLSERVER_BROWSER_HTTP_TIMEOUT_MS=90000
      - TOOLSERVER_BROWSER_URL=http://playwright:3000
      - TOOLSERVER_SCREENSHOTS_DIR=/app/proofs/screenshots
      - TOOLSERVER_DEBUG=true
      # LiteLLM connection for LLM completions
      - LITELLM_BASE_URL=http://litellm:4000
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}
    volumes:
      - ${DOCKER_SOCKET:-/var/run/docker.sock}:/var/run/docker.sock:ro
      - ${VOLUMES_ROOT}/secrets/stackops_ed25519:/app/keys/stackops_ed25519:ro
      - ${VOLUMES_ROOT}/agent_tool_server/known_hosts:/app/known_hosts:ro
      - ${HOME}/.datamancy/configs/infrastructure/ssh/bootstrap_known_hosts.sh:/app/scripts/bootstrap_known_hosts.sh:ro
      - proofs:/app/proofs
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "wget", "-q", "-O", "/dev/null", "http://localhost:8081/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s


  # Kotlin speech gateway service (Whisper ASR + Piper TTS proxy)
  ktspeechgateway:
    build:
      dockerfile: ./src/speech-gateway/Dockerfile
      context: .
    container_name: ktspeechgateway
    restart: unless-stopped
    networks:
      - backend
    depends_on:
      whisper:
        condition: service_healthy
      piper:
        condition: service_healthy
    environment:
      - WHISPER_BASE_URL=${WHISPER_BASE_URL:-http://whisper:9000}
      - PIPER_BASE_URL=${PIPER_BASE_URL:-http://piper:8080}
      - PIPER_VOICE=${PIPER_VOICE:-en_US-lessac-medium}
    healthcheck:
      test: ["CMD", "wget", "-q", "-O", "/dev/null", "http://localhost:8091/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Whisper ASR service (OpenAI-compatible)
  whisper:
    image: onerahmet/openai-whisper-asr-webservice:v1.6.0
    container_name: whisper
    restart: unless-stopped
    networks:
      - backend
    environment:
      - ASR_MODEL=base
      - ASR_ENGINE=faster_whisper
    volumes:
      - ${VOLUMES_ROOT}/whisper/models:/root/.cache/whisper
    healthcheck:
      test: ["CMD", "wget", "-q", "-O", "/dev/null", "http://localhost:9000/"]
      interval: 60s
      timeout: 10s
      retries: 5
      start_period: 30s

  # Piper TTS HTTP service
  piper:
    image: rhasspy/wyoming-piper
    container_name: piper
    restart: unless-stopped
    networks:
      - backend
    environment:
      - PIPER_VOICE=${PIPER_VOICE:-en_US-lessac-medium}
    command: ["--voice", "en_US-lessac-medium"]
    volumes:
      - ${VOLUMES_ROOT}/piper/data:/data
    healthcheck:
      test: ["CMD-SHELL", "timeout 1 bash -c 'cat < /dev/null > /dev/tcp/127.0.0.1/10200'"]
      interval: 60s
      timeout: 10s
      retries: 5
      start_period: 30s


  ###############################################
  # Vector Stack (profile: bootstrap_vector_dbs)
  ###############################################
  qdrant:
    image: qdrant/qdrant:v1.12.5
    container_name: qdrant
    restart: unless-stopped
    networks:
      - database
      - backend
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
    # Do not expose ports publicly; access internally via service name
    volumes:
      - qdrant_data:/qdrant/storage
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '2.0'
    healthcheck:
      test: ["CMD-SHELL", "timeout 5 bash -c '</dev/tcp/127.0.0.1/6333' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  clickhouse:
    image: clickhouse/clickhouse-server:24.8
    container_name: clickhouse
    restart: unless-stopped
    networks:
      - database
    environment:
      - CLICKHOUSE_DB=default
      - CLICKHOUSE_USER=${STACK_ADMIN_USER}
      - CLICKHOUSE_PASSWORD=${STACK_ADMIN_PASSWORD}
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - ${HOME}/.datamancy/configs/databases/clickhouse/config.xml:/etc/clickhouse-server/config.d/config.xml:ro
      - ${HOME}/.datamancy/configs/databases/clickhouse/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    # Do not expose native TCP publicly; use internal networking
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8123/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  vector-bootstrap:
    build:
      dockerfile: ./src/debiankotlin/Dockerfile
      context: .
    container_name: vector-bootstrap
    restart: "no"
    networks:
      - backend
      - database
    depends_on:
      qdrant:
        condition: service_started
    environment:
      - QDRANT_URL=http://qdrant:6333
      - VECTOR_SIZE=${VECTOR_EMBED_SIZE:-384}
      - QDRANT_API_KEY=${QDRANT_API_KEY:-}
    volumes:
      - ${HOME}/.datamancy/configs/databases/vectors/collections.yaml:/configs/collections.yaml:ro
      - ${HOME}/.datamancy/configs/databases/vectors/bootstrap_vectors.main.kts:/scripts/bootstrap_vectors.main.kts:ro
    command: ["kotlin", "/scripts/bootstrap_vectors.main.kts", "/configs/collections.yaml"]
    healthcheck:
      test: ["CMD", "kotlin", "-version"]
      interval: 1m
      timeout: 10s
      retries: 1
      start_period: 5s

  rag-gateway:
    build:
      dockerfile: ./src/rag-gateway/Dockerfile
      context: .
    container_name: rag-gateway
    restart: unless-stopped
    networks:
      - backend
      - database
    environment:
      - PORT=8094
      - BENTHOS_URL=http://benthos:4195
      - QDRANT_URL=http://qdrant:6333
    depends_on:
      benthos:
        condition: service_healthy
      qdrant:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8094/health/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  benthos:
    image: jeffail/benthos:4.27.0
    container_name: benthos
    restart: unless-stopped
    networks:
      - backend
      - database
    volumes:
      - ${HOME}/.datamancy/configs/infrastructure/benthos/benthos.yaml:/benthos.yaml:ro
      - benthos_data:/data
    command: -c /benthos.yaml
    environment:
      - QDRANT_URL=${QDRANT_URL:-http://qdrant:6333}
      - QDRANT_API_KEY=${QDRANT_API_KEY:-}
      - LITELLM_URL=${LITELLM_URL:-http://litellm:4000/v1}
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}
      - CLICKHOUSE_DSN=${CLICKHOUSE_DSN:-clickhouse://${STACK_ADMIN_USER}:${STACK_ADMIN_PASSWORD}@clickhouse:9000/default?dial_timeout=5s&read_timeout=10s&compress=true}
      - EMBED_MODEL=${EMBED_MODEL:-embed-small}
      - RAG_GATEWAY_URL=${RAG_GATEWAY_URL:-http://rag-gateway:8094}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      qdrant:
        condition: service_started
      clickhouse:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:4195/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Headless browser for automation - Playwright with chromium
  playwright:
    build:
      dockerfile: ./src/playwright-controller/Dockerfile
      context: .
    container_name: playwright
    restart: unless-stopped
    networks:
      - backend
    shm_size: '2gb'
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:3000/healthz')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  # VM Provisioner - Manages VMs via libvirt/QEMU with SSH key management
  # SECURITY NOTE: Uses CAP_NET_ADMIN for libvirt networking instead of privileged mode
  # If this fails, see docs/PRE_DEPLOYMENT_REVIEW.md for troubleshooting
  vm-provisioner:
    build:
      dockerfile: ./src/vm-provisioner/Dockerfile
      context: .
    container_name: vm-provisioner
    restart: unless-stopped
    networks:
      - backend
    # Use specific capabilities instead of privileged mode
    cap_add:
      - NET_ADMIN       # Required for libvirt network management
      - SYS_ADMIN       # Required for mounting and cgroups access
      - DAC_OVERRIDE    # Required for accessing libvirt socket
    security_opt:
      - apparmor=unconfined  # Libvirt requires this for QEMU operations
    volumes:
      - /var/run/libvirt/libvirt-sock:/var/run/libvirt/libvirt-sock
      - ${VOLUMES_ROOT}/vm-provisioner/ssh_keys:/app/ssh_keys
      - ${VOLUMES_ROOT}/vm-provisioner/vms:/var/lib/libvirt/images
    environment:
      - VM_PROVISIONER_PORT=8092
      - LIBVIRT_URI=qemu:///system
      - SSH_KEY_DIR=/app/ssh_keys
      - LIBVIRT_STORAGE_POOL=default
      - LIBVIRT_STORAGE_PATH=/var/lib/libvirt/images
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8092/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
