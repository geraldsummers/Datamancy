###############################################
# Datamancy â€” docker-compose.yml
#
# Documentation:
# - Overview and Quickstart: README.md
# - Bootstrap guide: docs/BOOTSTRAP.md
# - Architecture overview: docs/ARCHITECTURE.md
# - App catalog (all services): docs/APP_CATALOG.md
# - Data ingestion & RAG: docs/DATA_AND_RAG.md
# - Operations & Security: docs/OPERATIONS.md, docs/SECURITY.md
#
# Tip: Services are grouped by profiles (bootstrap, bootstrap_vector_dbs, full).
#
# TODO PRE-PRODUCTION:
# - Add memory/CPU resource limits after real lab deployment profiling
# - Waiting for actual usage data to properly regulate resource constraints
# - Priority services needing limits: vllm, clickhouse, postgres, mastodon-*
###############################################
networks:
  frontend:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/24
          gateway: 172.20.0.1
  backend:
    driver: bridge
    ipam:
      config:
        - subnet: 172.21.0.0/24
          gateway: 172.21.0.1
  database:
    driver: bridge
    ipam:
      config:
        - subnet: 172.22.0.0/24
          gateway: 172.22.0.1

volumes:
  caddy_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/caddy_data
  caddy_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/caddy_config
  grafana_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/grafana_data
  open_webui_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/open_webui_data
  vaultwarden_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/vaultwarden_data
  planka_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/planka_data
  outline_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/outline_data
  bookstack_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/bookstack_data
  seafile_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/seafile_data
  onlyoffice_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/onlyoffice_data
  synapse_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/synapse_data
  mastodon_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/mastodon_data
  jupyterhub_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/jupyterhub_data
  homeassistant_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/homeassistant_config
  litellm_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/litellm_config
  ldap_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/ldap_data
  ldap_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/ldap_config
  lam_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/lam_config
  mariadb_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/mariadb_data
  postgres_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/postgres_data
  couchdb_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/couchdb_data
  clickhouse_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/clickhouse_data
  qdrant_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/qdrant_data
  benthos_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/benthos_data
  dockge_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/dockge_data
  kopia_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/kopia_data
  kopia_cache:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/kopia_cache
  kopia_repository:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/kopia_repository
  redis_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/redis_data
  seafile_mysql_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/seafile_mysql_data
  mailu_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/mailu_data
  mailu_filter:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/mailu_filter
  mailu_cert:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/mailu_cert
  mailu_dkim:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/mailu_dkim
  sogo_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/sogo_data
  portainer_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/portainer_data
  proofs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VOLUMES_ROOT}/proofs

services:
  caddy:
    image: caddy:2.8.4
    container_name: caddy
    restart: unless-stopped
    profiles:
      - bootstrap
      - infrastructure
    networks:
      frontend: {}
      backend:
        aliases:
          - grafana.${DOMAIN}
          - open-webui.${DOMAIN}
          - vaultwarden.${DOMAIN}
          - planka.${DOMAIN}
          - bookstack.${DOMAIN}
          - jupyterhub.${DOMAIN}
          - homepage.${DOMAIN}
          - seafile.${DOMAIN}
          - onlyoffice.${DOMAIN}
          - matrix.${DOMAIN}
          - sogo.${DOMAIN}
          - homeassistant.${DOMAIN}
          - litellm.${DOMAIN}
          - api.litellm.${DOMAIN}
          - auth.${DOMAIN}
          - kopia.${DOMAIN}
          - adminer.${DOMAIN}
          - pgadmin.${DOMAIN}
          - portainer.${DOMAIN}
          - mail.${DOMAIN}
          - browserless.${DOMAIN}
          - agent-tool-server.${DOMAIN}
          - lam.${DOMAIN}
    ports:
      - "80:80"
      - "443:443"
      - "443:443/udp"
    volumes:
      - caddy_data:/data
      - caddy_config:/config
      # Provide the static Caddyfile. Defaults to self-signed (internal CA).
      # To enable Let's Encrypt for a specific site, edit configs/infrastructure/caddy/Caddyfile
      # and add `tls you@example.com` inside that site's block.
      - ./configs/infrastructure/caddy/Caddyfile:/etc/caddy/Caddyfile:ro
    environment:
      # Domain for vhosts in Caddyfile
      DOMAIN: "${DOMAIN}"
      # Default IP allowlist for api.litellm subdomain (space-separated CIDRs)
      API_LITELLM_ALLOWLIST: "${API_LITELLM_ALLOWLIST:-10.0.0.0/8 172.16.0.0/12 192.168.0.0/16 100.64.0.0/10 ::1 fc00::/7 fe80::/10}"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      # Use the caddy binary as a lightweight readiness signal
      test: ["CMD", "caddy", "version"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s


  ldap:
    image: osixia/openldap:1.5.0
    container_name: ldap
    restart: unless-stopped
    networks:
      - backend
    profiles:
      - bootstrap
      - infrastructure
    environment:
      - LDAP_ORGANISATION=Datamancy
      - LDAP_DOMAIN=stack.local
      - LDAP_BASE_DN=dc=stack,dc=local
      # Consolidated: use stack-wide admin password for LDAP admin/config
      - LDAP_ADMIN_PASSWORD=${STACK_ADMIN_PASSWORD}
      - LDAP_CONFIG_PASSWORD=${STACK_ADMIN_PASSWORD}
      - LDAP_TLS=false
    volumes:
      - ldap_data:/var/lib/ldap
      - ldap_config:/etc/ldap/slapd.d
      - ./bootstrap_ldap.ldif:/container/service/slapd/assets/config/bootstrap/ldif/custom/bootstrap_ldap.ldif:ro
    command: --copy-service --loglevel info
    healthcheck:
      test: ["CMD", "ldapsearch", "-x", "-H", "ldap://localhost:389", "-b", "dc=stack,dc=local", "-D", "cn=admin,dc=stack,dc=local", "-w", "${STACK_ADMIN_PASSWORD}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  ldap-account-manager:
    image: ghcr.io/ldapaccountmanager/lam:9.3
    container_name: ldap-account-manager
    restart: unless-stopped
    networks:
      - backend
      - frontend
    profiles:
      - bootstrap
      - applications
    depends_on:
      ldap:
        condition: service_healthy
      authelia:
        condition: service_healthy
    environment:
      - LDAP_DOMAIN=stack.local
      - LDAP_BASE_DN=dc=stack,dc=local
      - LDAP_USER=cn=admin,dc=stack,dc=local
      # Consolidated: use stack-wide admin password for LDAP bind
      - LDAP_PASSWORD=${STACK_ADMIN_PASSWORD}
      # Consolidated admin credentials: use stack-wide admin password
      - LAM_PASSWORD=${STACK_ADMIN_PASSWORD}
      - LAM_LANG=en_US
    volumes:
      - lam_config:/var/lib/ldap-account-manager
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:80/lam/"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  redis:
    image: valkey/valkey:8.0.1
    container_name: redis
    restart: unless-stopped
    networks:
      - backend
    profiles:
      - bootstrap
      - databases
    volumes:
      - redis_data:/data
    command: valkey-server --appendonly yes
    healthcheck:
      test: ["CMD", "valkey-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  authelia:
    image: authelia/authelia:4.39.13
    container_name: authelia
    restart: unless-stopped
    networks:
      - backend
      - frontend
      - database
    profiles:
      - bootstrap
      - infrastructure
    depends_on:
      ldap:
        condition: service_healthy
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
    volumes:
      - ${VOLUMES_ROOT}/authelia:/config
      - ${AUTHELIA_CONFIG_FILE:-./configs/applications/authelia/configuration.yml}:/config/configuration.yml:ro
    command: ["--config", "/config/configuration.yml"]
    environment:
      - TZ=UTC
      - AUTHELIA_JWT_SECRET=${AUTHELIA_JWT_SECRET}
      - AUTHELIA_SESSION_SECRET=${AUTHELIA_SESSION_SECRET}
      - AUTHELIA_STORAGE_ENCRYPTION_KEY=${AUTHELIA_STORAGE_ENCRYPTION_KEY}
      # Consolidated: use stack-wide admin password for LDAP bind
      - AUTHELIA_AUTHENTICATION_BACKEND_LDAP_PASSWORD=${STACK_ADMIN_PASSWORD}
      - AUTHELIA_IDENTITY_PROVIDERS_OIDC_HMAC_SECRET=${AUTHELIA_OIDC_HMAC_SECRET}
      # PostgreSQL backend configuration
      - AUTHELIA_STORAGE_POSTGRES_HOST=postgres
      - AUTHELIA_STORAGE_POSTGRES_PORT=5432
      - AUTHELIA_STORAGE_POSTGRES_DATABASE=authelia
      - AUTHELIA_STORAGE_POSTGRES_USERNAME=authelia
      - AUTHELIA_STORAGE_POSTGRES_PASSWORD=${AUTHELIA_DB_PASSWORD}
      # OIDC private key is configured in the jwks section of configuration.yml
      # - AUTHELIA_IDENTITY_PROVIDERS_OIDC_ISSUER_PRIVATE_KEY=${AUTHELIA_IDENTITY_PROVIDERS_OIDC_ISSUER_PRIVATE_KEY:-}
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9091/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  mariadb:
    image: mariadb:11.6.2
    container_name: mariadb
    restart: unless-stopped
    profiles:
      - databases
    networks:
      - database
    environment:
      - MYSQL_ROOT_PASSWORD=${STACK_ADMIN_PASSWORD}
      - MYSQL_DATABASE=datamancy
      - MYSQL_USER=${STACK_ADMIN_USER}
      - MYSQL_PASSWORD=${STACK_ADMIN_PASSWORD}
    volumes:
      - mariadb_data:/var/lib/mysql
      - ./configs/databases/mariadb/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    command: --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci
    healthcheck:
      test: ["CMD", "mariadb-admin", "ping", "-h", "localhost", "-u", "root", "-p${STACK_ADMIN_PASSWORD}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  postgres:
    image: postgres:16.11
    container_name: postgres
    restart: unless-stopped
    profiles:
      - bootstrap
      - databases
    networks:
      database:
        aliases:
          - db
      backend: {}
    environment:
      - POSTGRES_PASSWORD=${STACK_ADMIN_PASSWORD}
      - POSTGRES_USER=${STACK_ADMIN_USER}
      - POSTGRES_DB=postgres
      - PLANKA_DB_PASSWORD=${PLANKA_DB_PASSWORD}
      - SYNAPSE_DB_PASSWORD=${SYNAPSE_DB_PASSWORD}
      - MAILU_DB_PASSWORD=${MAILU_DB_PASSWORD}
      - AUTHELIA_DB_PASSWORD=${AUTHELIA_DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./configs/databases/postgres/init-db.sh:/docker-entrypoint-initdb.d/init-db.sh:ro
      - ./configs/databases/postgres/init-mailu-schema.sql:/docker-entrypoint-initdb.d/init-mailu-schema.sql:ro
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
    command: postgres -c 'max_connections=300' -c 'shared_buffers=1GB'
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${STACK_ADMIN_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  couchdb:
    image: couchdb:3.5.1
    container_name: couchdb
    restart: unless-stopped
    profiles:
      - databases
    networks:
      - database
    environment:
      - COUCHDB_USER=${STACK_ADMIN_USER}
      - COUCHDB_PASSWORD=${STACK_ADMIN_PASSWORD}
    volumes:
      - couchdb_data:/opt/couchdb/data
      - ./configs/databases/couchdb/init-wrapper.sh:/usr/local/bin/init-wrapper.sh:ro
    entrypoint: ["/bin/bash", "/usr/local/bin/init-wrapper.sh"]
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://127.0.0.1:5984/_up | grep -q '\"status\":\"ok\"' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s


  memcached:
    image: memcached:1.6.39
    container_name: seafile-memcached
    restart: unless-stopped
    profiles:
      - databases
    networks:
      - backend
    healthcheck:
      test: ["CMD-SHELL", "timeout 1 bash -c 'cat < /dev/null > /dev/tcp/127.0.0.1/11211'"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  mariadb-seafile:
    image: mariadb:11.6.2
    container_name: mariadb-seafile
    restart: unless-stopped
    profiles:
      - databases
    environment:
      - MYSQL_ROOT_PASSWORD=${STACK_ADMIN_PASSWORD}
      - MYSQL_DATABASE=seafile
      - MYSQL_USER=${STACK_ADMIN_USER}
      - MYSQL_PASSWORD=${STACK_ADMIN_PASSWORD}
    volumes:
      - seafile_mysql_data:/var/lib/mysql
    networks:
      - backend
    healthcheck:
      test: ["CMD", "mariadb-admin", "ping", "-h", "localhost", "-u", "root", "-p${STACK_ADMIN_PASSWORD}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  redis-synapse:
    image: valkey/valkey:8.0.1
    container_name: redis-synapse
    restart: unless-stopped
    profiles:
      - databases
    networks:
      - backend
    healthcheck:
      test: ["CMD", "valkey-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  mailu-front:
    image: ghcr.io/mailu/nginx:2024.06.45
    container_name: mailu-front
    restart: unless-stopped
    profiles:
      - applications
    env_file:
      - configs/applications/mailu/mailu.env
    ports:
      - "25:25"
      - "465:465"
      - "587:587"
      - "110:110"
      - "995:995"
      - "143:143"
      - "993:993"
    volumes:
      - mailu_cert:/certs
      - mailu_dkim:/dkim
    networks:
      - backend
    depends_on:
      mailu-admin:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  mailu-redis:
    image: valkey/valkey:8.0.1
    container_name: mailu-redis
    restart: unless-stopped
    profiles:
      - databases
    networks:
      backend:
        aliases:
          - redis
          - mailu-redis
    healthcheck:
      test: ["CMD", "valkey-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  mailu-antispam:
    image: ghcr.io/mailu/rspamd:2024.06.45
    container_name: mailu-antispam
    restart: unless-stopped
    profiles:
      - applications
    env_file:
      - configs/applications/mailu/mailu.env
    volumes:
      - mailu_filter:/var/lib/rspamd
    networks:
      - backend
    depends_on:
      mailu-admin:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "rspamc", "stat"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  mailu-antivirus:
    image: ghcr.io/mailu/clamav:2.0.43
    container_name: mailu-antivirus
    restart: unless-stopped
    profiles:
      - applications
    env_file:
      - configs/applications/mailu/mailu.env
    networks:
      - backend
    healthcheck:
      test: ["CMD", "clamdscan", "--ping", "1"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 5m

  mailu-imap:
    image: ghcr.io/mailu/dovecot:2024.06.45
    container_name: mailu-imap
    restart: unless-stopped
    profiles:
      - applications
    env_file:
      - configs/applications/mailu/mailu.env
    volumes:
      - mailu_data:/data
    networks:
      - backend
    depends_on:
      mailu-admin:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "doveadm", "director", "status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  mailu-smtp:
    image: ghcr.io/mailu/postfix:2024.06.45
    container_name: mailu-smtp
    restart: unless-stopped
    profiles:
      - applications
    env_file:
      - configs/applications/mailu/mailu.env
    volumes:
      - mailu_data:/data
      - mailu_dkim:/dkim
    networks:
      - backend
    depends_on:
      mailu-admin:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "postfix", "status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  mailu-admin:
    image: ghcr.io/mailu/admin:2024.06.45
    container_name: mailu-admin
    restart: unless-stopped
    profiles:
      - applications
    env_file:
      - configs/applications/mailu/mailu.env
    volumes:
      - mailu_data:/data
    networks:
      - backend
    depends_on:
      postgres:
        condition: service_healthy
      mailu-redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "-L", "http://localhost:8080/admin/ui"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s

  mailu-webmail:
    image: ghcr.io/mailu/webmail:2024.06.45
    container_name: mailu-webmail
    restart: unless-stopped
    profiles:
      - applications
    env_file:
      - configs/applications/mailu/mailu.env
    volumes:
      - ./configs/applications/mailu/nginx-override.conf:/overrides/nginx/roundcube.conf:ro
    networks:
      - backend
    depends_on:
      mailu-admin:
        condition: service_healthy
      mailu-imap:
        condition: service_healthy
      mailu-front:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  sogo:
    image: pmietlicki/sogo:v5
    container_name: sogo
    restart: unless-stopped
    profiles:
      - applications
    environment:
      - SOGO_WORKERS=1
      - SOGO_SIEVE_SCRIPTS_ENABLED=YES
      - SOGO_DOMAIN=${MAIL_DOMAIN}
      - SOGO_WEB_LOGIN_ENABLE_EMAIL_LOGIN=YES
      - SOGO_MAIL_SERVER=mailu-imap
      - SOGO_SOGOIMAPSERVERS=mailu-imap:143
      - SOGO_SOGOSMTPSERVERS=mailu-smtp:587
      - SOGO_SOGoSMTPAuthenticationType=PLAIN
      - SOGO_SOGoIMAPServer=imap
      - TZ=UTC
    volumes:
      - sogo_data:/var/lib/sogo
    depends_on:
      mailu-imap:
        condition: service_healthy
      mailu-smtp:
        condition: service_healthy
    networks:
      - backend
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:20000/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  dockge:
    image: louislam/dockge:1
    container_name: dockge
    restart: unless-stopped
    profiles:
      - infrastructure
    networks:
      - backend
    depends_on:
      - docker-proxy
    volumes:
      - dockge_data:/app/data
      - ./stacks:/opt/stacks
    environment:
      - DOCKGE_STACKS_DIR=/opt/stacks
      - DOCKER_HOST=tcp://docker-proxy:2375
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001"]
      interval: 5s
      timeout: 3s
      retries: 30
      start_period: 10s

  dockge-init:
    image: node:20.18.1
    container_name: dockge-init
    profiles:
      - infrastructure
    networks:
      - backend
    depends_on:
      dockge:
        condition: service_healthy
    volumes:
      - ./configs/applications/dockge/init-dockge.js:/init-dockge.js:ro
    environment:
      - DOCKGE_HOST=dockge
      - DOCKGE_PORT=5001
      - DOCKGE_ADMIN_USER=${STACK_ADMIN_USER}
      - DOCKGE_ADMIN_PASSWORD=${STACK_ADMIN_PASSWORD}
    command: node /init-dockge.js
    restart: "no"
    healthcheck:
      test: ["CMD", "node", "-v"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 5s

  kopia:
    image: kopia/kopia:0.18.2
    container_name: kopia
    restart: unless-stopped
    profiles:
      - infrastructure
    networks:
      - backend
    volumes:
      - kopia_data:/app/config
      - kopia_cache:/app/cache
      # Mount volumes to backup (read-only)
      - postgres_data:/backup/postgres:ro
      - mariadb_data:/backup/mariadb:ro
      - grafana_data:/backup/grafana:ro
      - open_webui_data:/backup/open_webui:ro
      - vaultwarden_data:/backup/vaultwarden:ro
      - kopia_repository:/repository
      - ./configs/applications/kopia/init-kopia.sh:/init-kopia.sh:ro
    environment:
      - KOPIA_PASSWORD=${STACK_ADMIN_PASSWORD}
      - KOPIA_REPO_PATH=/repository
      - USER=kopia
    entrypoint: ["sh", "/init-kopia.sh"]
    healthcheck:
      test: ["CMD", "kopia", "repository", "status"]
      interval: 5m
      timeout: 30s
      retries: 3
      start_period: 1m

  docker-proxy:
    image: tecnativa/docker-socket-proxy:v0.4.1
    container_name: docker-proxy
    restart: unless-stopped
    profiles:
      - bootstrap
      - infrastructure
    networks:
      - backend
    environment:
      - CONTAINERS=1
      - IMAGES=1
      - NETWORKS=1
      - VOLUMES=1
      - POST=0
      - BUILD=0
      - COMMIT=0
      - CONFIGS=0
      - DISTRIBUTION=0
      - EXEC=0
      - GRPC=0
      - INFO=1
      - PLUGINS=0
      - SERVICES=0
      - SESSION=0
      - SWARM=0
      - SYSTEM=0
      - TASKS=0
      - VERSION=1
    volumes:
      - ${DOCKER_SOCKET:-/var/run/docker.sock}:/var/run/docker.sock:ro
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:2375/version"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  portainer:
    image: portainer/portainer-ce:2.24.0
    container_name: portainer
    restart: unless-stopped
    profiles:
      - bootstrap
      - infrastructure
    networks:
      - backend
      - frontend
    depends_on:
      authelia:
        condition: service_healthy
      docker-proxy:
        condition: service_healthy
    volumes:
      - portainer_data:/data
    environment:
      - DOCKER_HOST=tcp://docker-proxy:2375
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9000/api/system/status"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 20s


  grafana:
    image: grafana/grafana:11.6.7
    container_name: grafana
    restart: unless-stopped
    profiles:
      - applications
    networks:
      - backend
      - database
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    depends_on:
      postgres:
        condition: service_healthy
      authelia:
        condition: service_healthy
    environment:
      - GF_DEFAULT_LOCALE=en_US
      - GF_SERVER_ROOT_URL=https://grafana.${DOMAIN}
      - GF_SERVER_DOMAIN=grafana.${DOMAIN}
      # PostgreSQL database
      - GF_DATABASE_TYPE=postgres
      - GF_DATABASE_HOST=postgres:5432
      - GF_DATABASE_NAME=grafana
      - GF_DATABASE_USER=grafana
      - GF_DATABASE_PASSWORD=${GRAFANA_DB_PASSWORD}
      - GF_DATABASE_SSL_MODE=disable
      - GF_AUTH_GENERIC_OAUTH_ENABLED=true
      - GF_AUTH_GENERIC_OAUTH_NAME=Authelia
      - GF_AUTH_GENERIC_OAUTH_CLIENT_ID=grafana
      - GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET=${GRAFANA_OAUTH_SECRET}
      - GF_AUTH_GENERIC_OAUTH_SCOPES=openid profile email groups
      - GF_AUTH_GENERIC_OAUTH_AUTH_URL=https://auth.${DOMAIN}/api/oidc/authorization
      - GF_AUTH_GENERIC_OAUTH_TOKEN_URL=http://authelia:9091/api/oidc/token
      - GF_AUTH_GENERIC_OAUTH_API_URL=http://authelia:9091/api/oidc/userinfo
      - GF_AUTH_GENERIC_OAUTH_LOGIN_ATTRIBUTE_PATH=email
      - GF_AUTH_GENERIC_OAUTH_EMAIL_ATTRIBUTE_PATH=email
      - GF_AUTH_GENERIC_OAUTH_GROUPS_ATTRIBUTE_PATH=groups
      - GF_AUTH_GENERIC_OAUTH_NAME_ATTRIBUTE_PATH=name
      - GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_PATH=contains(groups[*], 'admins') && 'Admin' || contains(groups[*], 'users') && 'Editor' || 'Viewer'
      - GF_AUTH_GENERIC_OAUTH_USE_PKCE=true
      - GF_AUTH_SIGNOUT_REDIRECT_URL=https://auth.${DOMAIN}/logout
      - GF_AUTH_GENERIC_OAUTH_SKIP_ORG_ROLE_SYNC=false
      - GF_AUTH_GENERIC_OAUTH_AUTO_LOGIN=false
      - GF_AUTH_GENERIC_OAUTH_ALLOW_SIGN_UP=true
      - GF_USERS_ALLOW_SIGN_UP=true
      - GF_USERS_AUTO_ASSIGN_ORG=true
      - GF_USERS_AUTO_ASSIGN_ORG_ROLE=Viewer
      - GF_AUTH_DISABLE_LOGIN_FORM=false
      # Consolidated admin credentials: use stack-wide admin user/pass
      - GF_SECURITY_ADMIN_USER=${STACK_ADMIN_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${STACK_ADMIN_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./configs/applications/grafana/grafana.ini:/etc/grafana/grafana.ini:ro
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  open-webui:
    image: ghcr.io/open-webui/open-webui:0.3.32
    container_name: open-webui
    restart: unless-stopped
    networks:
      - backend
      - frontend
      - database
    profiles:
      - bootstrap
      - applications
    dns:
      - 8.8.8.8
      - 1.1.1.1
    depends_on:
      postgres:
        condition: service_healthy
      litellm:
        condition: service_healthy
      authelia:
        condition: service_healthy
    environment:
      - WEBUI_URL=${OPENWEBUI_WEBUI_URL:-https://open-webui.${DOMAIN}}
      # PostgreSQL database
      - DATABASE_URL=postgresql://openwebui:${OPENWEBUI_DB_PASSWORD}@postgres:5432/openwebui
      - ENABLE_OAUTH_SIGNUP=${OPENWEBUI_ENABLE_OAUTH_SIGNUP:-false}
      - DEFAULT_USER_ROLE=${OPENWEBUI_DEFAULT_USER_ROLE:-admin}
      - ENABLE_OPENAI_API=true
      - OPENAI_API_BASE_URL=${OPENWEBUI_OPENAI_API_BASE_URL:-http://litellm:4000/v1}
      - OPENAI_API_KEY=${LITELLM_MASTER_KEY}
    volumes:
      - open_webui_data:/app/backend/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  vaultwarden:
    image: vaultwarden/server:1.34.3
    container_name: vaultwarden
    restart: unless-stopped
    profiles:
      - applications
    networks:
      - backend
      - database
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    depends_on:
      postgres:
        condition: service_healthy
      authelia:
        condition: service_healthy
      mailu-smtp:
        condition: service_healthy
    environment:
      - DOMAIN=https://vaultwarden.${DOMAIN}
      # PostgreSQL database
      - DATABASE_URL=postgresql://vaultwarden:${VAULTWARDEN_DB_PASSWORD}@postgres:5432/vaultwarden
      - SIGNUPS_ALLOWED=false
      - SIGNUPS_VERIFY=true
      - SIGNUPS_DOMAINS_WHITELIST=${MAIL_DOMAIN}
      - INVITATIONS_ALLOWED=true
      - SHOW_PASSWORD_HINT=false
      - SSO_ENABLED=true
      - SSO_ONLY=false
      - SSO_SIGNUPS_MATCH_EMAIL=true
      - SSO_AUTHORITY=https://auth.${DOMAIN}
      - SSO_ISSUER=https://auth.${DOMAIN}
      - SSO_CLIENT_ID=vaultwarden
      - SSO_CLIENT_SECRET=${VAULTWARDEN_OAUTH_SECRET}
      - SSO_SCOPES=openid email profile
      - SSO_CALLBACK_PATH=/identity/connect/oidc-signin
      - ADMIN_TOKEN=${VAULTWARDEN_ADMIN_TOKEN}
      - SMTP_HOST=mailu-smtp
      - SMTP_FROM=vaultwarden@${MAIL_DOMAIN}
      - SMTP_PORT=587
      - SMTP_SECURITY=starttls
      - SMTP_USERNAME=vaultwarden@${MAIL_DOMAIN}
      - SMTP_PASSWORD=${VAULTWARDEN_SMTP_PASSWORD}
    volumes:
      - vaultwarden_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/alive"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  planka:
    image: ghcr.io/plankanban/planka:1.26.3
    container_name: planka
    restart: unless-stopped
    profiles:
      - applications
    networks:
      - backend
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    depends_on:
      postgres:
        condition: service_healthy
      authelia:
        condition: service_healthy
    environment:
      - BASE_URL=https://planka.${DOMAIN}
      - DATABASE_URL=postgresql://planka:${PLANKA_DB_PASSWORD}@postgres:5432/planka
      - SECRET_KEY=${PLANKA_SECRET_KEY}
      - OIDC_ISSUER=https://auth.${DOMAIN}
      - OIDC_CLIENT_ID=planka
      - OIDC_CLIENT_SECRET=${PLANKA_OAUTH_SECRET}
      - OIDC_SCOPES=openid profile email groups
      - OIDC_ADMIN_ROLES=planka-admin
      - OIDC_EMAIL_ATTRIBUTE=email
      - OIDC_NAME_ATTRIBUTE=name
      - OIDC_USERNAME_ATTRIBUTE=preferred_username
      # NODE_TLS_REJECT_UNAUTHORIZED=0 removed - trust Caddy CA cert instead
      - NODE_EXTRA_CA_CERTS=/usr/local/share/ca-certificates/caddy-ca.crt
    volumes:
      - planka_data:/app/public/user-avatars
      - ./configs/applications/planka/caddy-ca.crt:/usr/local/share/ca-certificates/caddy-ca.crt:ro
    healthcheck:
      test: ["CMD", "wget", "-q", "-O", "/dev/null", "http://localhost:1337/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  bookstack:
    image: lscr.io/linuxserver/bookstack:24.12.1
    container_name: bookstack
    restart: unless-stopped
    profiles:
      - applications
    networks:
      - backend
    depends_on:
      mariadb:
        condition: service_healthy
      authelia:
        condition: service_healthy
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=UTC
      - APP_URL=https://bookstack.${DOMAIN}
      - DB_HOST=mariadb
      - DB_PORT=3306
      - DB_USER=bookstack
      - DB_PASS=${BOOKSTACK_DB_PASSWORD}
      - DB_DATABASE=bookstack
    volumes:
      - bookstack_data:/config
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  seafile:
    image: seafileltd/seafile-mc:13.0.12
    container_name: seafile
    restart: unless-stopped
    profiles:
      - applications
    depends_on:
      memcached:
        condition: service_healthy
      mariadb-seafile:
        condition: service_healthy
      authelia:
        condition: service_healthy
    environment:
      - TZ=UTC
      - DB_HOST=mariadb-seafile
      - DB_ROOT_PASSWD=${MARIADB_SEAFILE_ROOT_PASSWORD}
      - SEAFILE_SERVER_HOSTNAME=seafile.${DOMAIN}
      # Consolidated admin credentials: use stack-wide admin email/pass
      - SEAFILE_ADMIN_EMAIL=${STACK_ADMIN_EMAIL}
      - SEAFILE_ADMIN_PASSWORD=${STACK_ADMIN_PASSWORD}
      - TIME_ZONE=UTC
    volumes:
      - seafile_data:/shared
      - ./configs/applications/seafile/seahub_settings.py:/opt/seafile/conf/seahub_settings.py:ro
    networks:
      - backend
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  onlyoffice:
    image: onlyoffice/documentserver:8.2.2
    container_name: onlyoffice
    restart: unless-stopped
    profiles:
      - applications
    environment:
      - TZ=UTC
      - JWT_ENABLED=true
      - JWT_SECRET=${ONLYOFFICE_JWT_SECRET}
      - JWT_HEADER=Authorization
    volumes:
      - onlyoffice_data:/var/www/onlyoffice/Data
    networks:
      - backend
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/healthcheck"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 2m

  synapse:
    build:
      dockerfile: ./src/synapse/Dockerfile
    container_name: synapse
    restart: unless-stopped
    profiles:
      - applications
    depends_on:
      redis-synapse:
        condition: service_healthy
      postgres:
        condition: service_healthy
      ldap:
        condition: service_healthy
    environment:
      - SYNAPSE_SERVER_NAME=matrix.${DOMAIN}
      - SYNAPSE_REPORT_STATS=yes
      - SYNAPSE_ENABLE_REGISTRATION=${MATRIX_ENABLE_REGISTRATION:-false}
      - SYNAPSE_TRUSTED_PROXIES=172.18.0.0/16,172.21.0.0/24
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=synapse
      - POSTGRES_USER=synapse
      - POSTGRES_PASSWORD=${SYNAPSE_DB_PASSWORD}
      - REDIS_HOST=redis-synapse
      - STACK_ADMIN_PASSWORD=${STACK_ADMIN_PASSWORD}
    volumes:
      - synapse_data:/data
      - ./configs/applications/synapse/homeserver.yaml:/data/homeserver.yaml:ro
      - ./configs/applications/synapse/entrypoint.sh:/entrypoint.sh:ro
    entrypoint: ["/entrypoint.sh"]
    networks:
      - backend
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8008/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  mastodon-web:
    image: ghcr.io/mastodon/mastodon:v4.3.3
    container_name: mastodon-web
    restart: unless-stopped
    profiles:
      - applications
    networks:
      - backend
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      authelia:
        condition: service_healthy
      mailu-smtp:
        condition: service_healthy
    env_file:
      - configs/applications/mastodon/mastodon.env
    command: bash -c "rm -f /mastodon/tmp/pids/server.pid; bundle exec rails s -p 3000 -b 0.0.0.0"
    volumes:
      - mastodon_data:/mastodon/public/system
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '2.0'
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider --proxy=off localhost:3000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  mastodon-streaming:
    image: ghcr.io/mastodon/mastodon:v4.3.3
    container_name: mastodon-streaming
    restart: unless-stopped
    profiles:
      - applications
    networks:
      - backend
    depends_on:
      mastodon-web:
        condition: service_healthy
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
    env_file:
      - configs/applications/mastodon/mastodon.env
    command: node ./streaming
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider --proxy=off localhost:4000/api/v1/streaming/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  mastodon-sidekiq:
    image: ghcr.io/mastodon/mastodon:v4.3.3
    container_name: mastodon-sidekiq
    restart: unless-stopped
    profiles:
      - applications
    networks:
      - backend
    depends_on:
      mastodon-web:
        condition: service_healthy
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
    env_file:
      - configs/applications/mastodon/mastodon.env
    command: bundle exec sidekiq
    volumes:
      - mastodon_data:/mastodon/public/system
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '2.0'
    healthcheck:
      # Check for sidekiq process responding (not just running)
      # Sidekiq heartbeat is stored in Redis - check it's updating
      test: ["CMD-SHELL", "bundle exec rails runner 'exit(Sidekiq::ProcessSet.new.size > 0 ? 0 : 1)' || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s

  mastodon-init:
    image: docker:27-cli
    container_name: mastodon-init
    profiles:
      - applications
    networks:
      - backend
    depends_on:
      mastodon-web:
        condition: service_healthy
      docker-proxy:
        condition: service_healthy
    environment:
      - DOCKER_HOST=tcp://docker-proxy:2375
    volumes:
      - ./configs/applications/mastodon/init-mastodon.sh:/init-mastodon.sh:ro
    command: sh /init-mastodon.sh
    restart: "no"
    healthcheck:
      test: ["CMD", "sh", "-c", "echo ok"]
      interval: 1m
      timeout: 10s
      retries: 1
      start_period: 5s

  homepage:
    image: ghcr.io/gethomepage/homepage:v1.7.0
    container_name: homepage
    restart: unless-stopped
    profiles:
      - applications
    networks:
      - backend
    depends_on:
      docker-proxy:
        condition: service_healthy
    environment:
      - PUID=1000
      - PGID=1000
      - HOMEPAGE_ALLOWED_HOSTS=homepage.${DOMAIN}
      - DOCKER_HOST=tcp://docker-proxy:2375
    volumes:
      - ./configs/applications/homepage/services.yaml:/app/config/services.yaml:ro
      - ./configs/applications/homepage/settings.yaml:/app/config/settings.yaml:ro
      - ./configs/applications/homepage/widgets.yaml:/app/config/widgets.yaml:ro
      - ./configs/applications/homepage/bookmarks.yaml:/app/config/bookmarks.yaml:ro
      - ./configs/applications/homepage/docker.yaml:/app/config/docker.yaml:ro
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://127.0.0.1:3000/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  jupyterhub:
    build:
      dockerfile: ./src/jupyterhub/Dockerfile
    container_name: jupyterhub
    restart: unless-stopped
    profiles:
      - applications
    networks:
      - backend
    depends_on:
      authelia:
        condition: service_healthy
      docker-proxy:
        condition: service_started
    environment:
      - DOMAIN=${DOMAIN}
      - JUPYTERHUB_OAUTH_SECRET=${JUPYTERHUB_OAUTH_SECRET}
      - DOCKER_HOST=tcp://docker-proxy:2375
    volumes:
      - jupyterhub_data:/srv/jupyterhub
      - ./src/jupyterhub/jupyterhub_config.py:/srv/jupyterhub/jupyterhub_config.py:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/hub/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  homeassistant:
    image: ghcr.io/home-assistant/home-assistant:stable
    container_name: homeassistant
    restart: unless-stopped
    profiles:
      - applications
    networks:
      - backend
    depends_on:
      authelia:
        condition: service_healthy
      postgres:
        condition: service_healthy
    environment:
      - TZ=UTC
    volumes:
      - homeassistant_config:/config
      - /etc/localtime:/etc/localtime:ro
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8123/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  vllm:
    image: vllm/vllm-openai:v0.6.5
    container_name: vllm
    restart: unless-stopped
    networks:
      - backend
    profiles:
      - bootstrap
      - applications
    depends_on:
      authelia:
        condition: service_healthy
    environment:
      - HUGGINGFACEHUB_API_TOKEN=${HUGGINGFACEHUB_API_TOKEN}
      - HF_HUB_ENABLE_HF_TRANSFER=0
    command: [
      "--model", "solidrust/Hermes-2-Pro-Mistral-7B-AWQ",
      "--served-model-name", "hermes-2-pro-mistral-7b",
      "--max-model-len", "4096",
      "--quantization", "awq",
      "--gpu-memory-utilization", "0.80",
      "--max-num-seqs", "128",
      "--dtype", "auto",
      "--enable-auto-tool-choice",
      "--tool-call-parser", "hermes"
    ]
    volumes:
      - ${VOLUMES_ROOT}/vllm/hf-cache:/root/.cache/huggingface
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4.0'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 60s
      timeout: 10s
      retries: 5
      start_period: 2m

  vllm-router:
    build:
      dockerfile: ./src/vllm-router/Dockerfile
    container_name: vllm-router
    restart: unless-stopped
    networks:
      - backend
    profiles:
      - bootstrap
      - applications
    depends_on:
      vllm:
        condition: service_healthy
      authelia:
        condition: service_healthy
    environment:
      - VLLM_BASE_URL=http://vllm:8000
      - PORT=8010
    healthcheck:
      test: ["CMD", "wget", "-q", "-O", "/dev/null", "http://localhost:8010/health"]
      interval: 30s
      timeout: 5s
      retries: 10
      start_period: 30s

  embedding-service:
    image: ghcr.io/huggingface/text-embeddings-inference:cpu-1.2
    container_name: embedding-service
    restart: unless-stopped
    networks:
      - backend
    profiles:
      - bootstrap
      - bootstrap_vector_dbs
      - applications
    command: --model-id sentence-transformers/all-MiniLM-L6-v2 --port 8080
    volumes:
      - ${VOLUMES_ROOT}/embeddings/models:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  litellm:
    image: ghcr.io/berriai/litellm:v1.74.9-stable.patch.1
    container_name: litellm
    restart: unless-stopped
    networks:
      - backend
      - frontend
    profiles:
      - bootstrap
      - applications
    depends_on:
      vllm-router:
        condition: service_healthy
      authelia:
        condition: service_healthy
      embedding-service:
        condition: service_healthy
    volumes:
      - litellm_config:/app/config
      - ./configs/infrastructure/litellm/config.yaml:/app/config.yaml:ro
    environment:
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}
      # Consolidated admin credentials for UI login
      - UI_USERNAME=${STACK_ADMIN_USER}
      - UI_PASSWORD=${STACK_ADMIN_PASSWORD}
    command: --config /app/config.yaml --port 4000 --num_workers 4
    healthcheck:
      test: ["CMD-SHELL", "python3 -c 'import socket; s=socket.socket(); s.settimeout(1); s.connect((\"127.0.0.1\", 4000)); s.close()' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 15s


  ssh-key-bootstrap:
    image: alpine:3.20
    container_name: ssh-key-bootstrap
    profiles:
      - bootstrap
      - applications
    restart: "no"
    networks:
      - backend
    environment:
      - TOOLSERVER_SSH_HOST=${TOOLSERVER_SSH_HOST:-host.docker.internal}
    volumes:
      - ${VOLUMES_ROOT}/agent_tool_server:/app:rw
      - ./configs/infrastructure/ssh/bootstrap_known_hosts.sh:/bootstrap_known_hosts.sh:ro
    extra_hosts:
      - "host.docker.internal:host-gateway"
    command: ["sh", "-c", "apk add --no-cache openssh-client && sh /bootstrap_known_hosts.sh /app/known_hosts"]
    healthcheck:
      test: ["CMD", "test", "-f", "/app/known_hosts"]
      interval: 5s
      timeout: 5s
      retries: 1
      start_period: 5s

  agent-tool-server:
    build:
      dockerfile: ./src/agent-tool-server/Dockerfile
    container_name: agent-tool-server
    restart: unless-stopped
    networks:
      - backend
      - frontend
    profiles:
      - bootstrap
      - applications
    user: "1000:1000"
    group_add:
      - "985"  # docker group on host for socket access
    depends_on:
      ssh-key-bootstrap:
        condition: service_completed_successfully
      authelia:
        condition: service_healthy
      playwright:
        condition: service_healthy
      litellm:
        condition: service_healthy
    environment:
      - TOOLSERVER_PORT=8081
      - TOOLSERVER_PLUGINS_DIR=/app/plugins
      - TOOLSERVER_ALLOW_CAPS=host.shell.read,host.docker.write,host.docker.inspect,host.network.http,host.network.ssh
      # SSH to host via forced-command wrapper (see scripts)
      - TOOLSERVER_SSH_HOST=${TOOLSERVER_SSH_HOST:-host.docker.internal}
      - TOOLSERVER_SSH_USER=${TOOLSERVER_SSH_USER:-stackops}
      - TOOLSERVER_SSH_KEY_PATH=/app/keys/stackops_ed25519
      - TOOLSERVER_SSH_KNOWN_HOSTS=/app/known_hosts
      - TOOLSERVER_SSH_TIMEOUT_MS=20000
      # Browser service timeouts (login operations need more time)
      - TOOLSERVER_BROWSER_HTTP_TIMEOUT_MS=90000
      - TOOLSERVER_BROWSER_URL=http://playwright:3000
      - TOOLSERVER_SCREENSHOTS_DIR=/app/proofs/screenshots
      - TOOLSERVER_DEBUG=true
      # Browser service base URL (Playwright with Firefox)
      - TOOLSERVER_BROWSERLESS_URL=http://playwright:3000
      # LiteLLM connection for LLM completions
      - LITELLM_BASE_URL=http://litellm:4000
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}
    volumes:
      - ${DOCKER_SOCKET:-/var/run/docker.sock}:/var/run/docker.sock:ro
      - ${VOLUMES_ROOT}/secrets/stackops_ed25519:/app/keys/stackops_ed25519:ro
      - ${VOLUMES_ROOT}/agent_tool_server/known_hosts:/app/known_hosts:ro
      - ./configs/infrastructure/ssh/bootstrap_known_hosts.sh:/app/scripts/bootstrap_known_hosts.sh:ro
      - proofs:/app/proofs
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "wget", "-q", "-O", "/dev/null", "http://localhost:8081/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s


  probe-orchestrator:
    build:
      dockerfile: ./src/probe-orchestrator/Dockerfile
    container_name: probe-orchestrator
    restart: unless-stopped
    networks:
      - backend
    profiles:
      - bootstrap
      - applications
    depends_on:
      agent-tool-server:
        condition: service_healthy
      litellm:
        condition: service_healthy
      playwright:
        condition: service_healthy
    environment:
      - DOMAIN=${DOMAIN}
      - LLM_BASE_URL=${LLM_BASE_URL:-http://litellm:4000/v1}
      - LLM_API_KEY=${LITELLM_MASTER_KEY}
      - LLM_MODEL=${LLM_MODEL:-hermes-2-pro-mistral-7b}
      # Optional: OCR via LLM vision model (disabled by default)
      - OCR_MODEL=${OCR_MODEL:-none}
      - KFUN_URL=${KFUN_URL:-http://agent-tool-server:8081}
      - PROOFS_DIR=/proofs
      - SERVICES_MANIFEST_PATH=/app/configs/probe-orchestrator/services_manifest.json
      - MAX_STEPS=${MAX_STEPS:-8}
      - HTTP_TIMEOUT=${HTTP_TIMEOUT:-30}
    volumes:
      - proofs:/proofs
      - ./configs/probe-orchestrator:/app/configs/probe-orchestrator:ro
    healthcheck:
      test: ["CMD", "wget", "-q", "-O", "/dev/null", "http://localhost:8089/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Kotlin speech gateway service (Whisper ASR + Piper TTS proxy)
  ktspeechgateway:
    build:
      dockerfile: ./src/speech-gateway/Dockerfile
    container_name: ktspeechgateway
    restart: unless-stopped
    networks:
      - backend
    profiles:
      - infrastructure
    depends_on:
      whisper:
        condition: service_healthy
      piper:
        condition: service_healthy
    environment:
      - WHISPER_BASE_URL=${WHISPER_BASE_URL:-http://whisper:9000}
      - PIPER_BASE_URL=${PIPER_BASE_URL:-http://piper:8080}
      - PIPER_VOICE=${PIPER_VOICE:-en_US-lessac-medium}
    healthcheck:
      test: ["CMD", "wget", "-q", "-O", "/dev/null", "http://localhost:8091/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Whisper ASR service (OpenAI-compatible)
  whisper:
    image: onerahmet/openai-whisper-asr-webservice:v1.6.0
    container_name: whisper
    restart: unless-stopped
    networks:
      - backend
    profiles:
      - infrastructure
    environment:
      - ASR_MODEL=base
      - ASR_ENGINE=faster_whisper
    volumes:
      - ${VOLUMES_ROOT}/whisper/models:/root/.cache/whisper
    healthcheck:
      test: ["CMD", "wget", "-q", "-O", "/dev/null", "http://localhost:9000/"]
      interval: 60s
      timeout: 10s
      retries: 5
      start_period: 30s

  # Piper TTS HTTP service
  piper:
    image: rhasspy/wyoming-piper
    container_name: piper
    restart: unless-stopped
    networks:
      - backend
    profiles:
      - infrastructure
    environment:
      - PIPER_VOICE=${PIPER_VOICE:-en_US-lessac-medium}
    command: ["--voice", "en_US-lessac-medium"]
    volumes:
      - ${VOLUMES_ROOT}/piper/data:/data
    healthcheck:
      test: ["CMD-SHELL", "timeout 1 bash -c 'cat < /dev/null > /dev/tcp/127.0.0.1/10200'"]
      interval: 60s
      timeout: 10s
      retries: 5
      start_period: 30s


  ###############################################
  # Vector Stack (profile: bootstrap_vector_dbs)
  ###############################################
  qdrant:
    image: qdrant/qdrant:v1.12.5
    container_name: qdrant
    restart: unless-stopped
    networks:
      - database
      - backend
    profiles:
      - bootstrap_vector_dbs
      - databases
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
    # Do not expose ports publicly; access internally via service name
    volumes:
      - qdrant_data:/qdrant/storage
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '2.0'
    healthcheck:
      test: ["CMD-SHELL", "timeout 5 bash -c '</dev/tcp/127.0.0.1/6333' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  clickhouse:
    image: clickhouse/clickhouse-server:24.8
    container_name: clickhouse
    restart: unless-stopped
    networks:
      - database
    profiles:
      - bootstrap_vector_dbs
      - databases
    environment:
      - CLICKHOUSE_DB=default
      - CLICKHOUSE_USER=${STACK_ADMIN_USER}
      - CLICKHOUSE_PASSWORD=${STACK_ADMIN_PASSWORD}
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - ./configs/databases/clickhouse/config.xml:/etc/clickhouse-server/config.d/config.xml:ro
      - ./configs/databases/clickhouse/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    # Do not expose native TCP publicly; use internal networking
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8123/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  vector-bootstrap:
    build:
      dockerfile: ./src/debiankotlin/Dockerfile
    container_name: vector-bootstrap
    profiles:
      - bootstrap_vector_dbs
      - infrastructure
    restart: "no"
    networks:
      - backend
      - database
    depends_on:
      qdrant:
        condition: service_started
    environment:
      - QDRANT_URL=http://qdrant:6333
      - VECTOR_SIZE=${VECTOR_EMBED_SIZE:-384}
      - QDRANT_API_KEY=${QDRANT_API_KEY:-}
    volumes:
      - ./configs/databases/vectors/collections.yaml:/configs/collections.yaml:ro
      - ./configs/databases/vectors/bootstrap_vectors.main.kts:/scripts/bootstrap_vectors.main.kts:ro
    command: ["kotlin", "/scripts/bootstrap_vectors.main.kts", "/configs/collections.yaml"]
    healthcheck:
      test: ["CMD", "kotlin", "-version"]
      interval: 1m
      timeout: 10s
      retries: 1
      start_period: 5s

  benthos:
    image: jeffail/benthos:4.27.0
    container_name: benthos
    restart: unless-stopped
    networks:
      - backend
      - database
    profiles:
      - bootstrap_vector_dbs
      - infrastructure
    volumes:
      - ./configs/infrastructure/benthos/benthos.yaml:/benthos.yaml:ro
      - benthos_data:/data
    command: -c /benthos.yaml
    environment:
      - QDRANT_URL=${QDRANT_URL:-http://qdrant:6333}
      - QDRANT_API_KEY=${QDRANT_API_KEY:-}
      - LITELLM_URL=${LITELLM_URL:-http://litellm:4000/v1}
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}
      - CLICKHOUSE_DSN=${CLICKHOUSE_DSN:-clickhouse://${STACK_ADMIN_USER}:${STACK_ADMIN_PASSWORD}@clickhouse:9000/default?dial_timeout=5s&read_timeout=10s&compress=true}
      - EMBED_MODEL=${EMBED_MODEL:-embed-small}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      qdrant:
        condition: service_started
      clickhouse:
        condition: service_healthy
      litellm:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:4195/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Headless browser for automation - Playwright with Firefox
  playwright:
    build:
      dockerfile: ./src/playwright-controller/Dockerfile
    container_name: playwright
    restart: unless-stopped
    profiles:
      - bootstrap
      - infrastructure
      - applications
    networks:
      - backend
    shm_size: '2gb'
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:3000/healthz')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  # LDAP Sync Service - Syncs LDAP users to services without LDAP support
  ldap-sync-service:
    build:
      dockerfile: ./src/ldap-sync-service/Dockerfile
      context: .
    container_name: ldap-sync-service
    restart: "no"  # Run on-demand or via cron
    profiles:
      - infrastructure
    networks:
      - backend
    depends_on:
      ldap:
        condition: service_healthy
      mailu-admin:
        condition: service_healthy
      docker-proxy:
        condition: service_healthy
    environment:
      # LDAP connection
      - LDAP_HOST=ldap
      - LDAP_PORT=389
      - LDAP_BIND_DN=cn=admin,dc=stack,dc=local
      - LDAP_BIND_PASSWORD=${STACK_ADMIN_PASSWORD}
      - LDAP_BASE_DN=dc=stack,dc=local
      - LDAP_USERS_OU=ou=users
      - LDAP_GROUPS_OU=ou=groups
      # Enable plugins
      - ENABLE_MAILU_SYNC=true
      # Mailu configuration
      - MAILU_ADMIN_CONTAINER=mailu-admin
      - MAIL_DOMAIN=${MAIL_DOMAIN}
      - MAILU_DEFAULT_QUOTA_MB=5000
      - MAILU_DEFAULT_PASSWORD=ChangeMe123!
      # Docker socket for Mailu sync (via proxy)
      - DOCKER_HOST=tcp://docker-proxy:2375
    volumes:
      - ${DOCKER_SOCKET:-/var/run/docker.sock}:/var/run/docker.sock:ro
    command: ["sync"]

  # VM Provisioner - Manages VMs via libvirt/QEMU with SSH key management
  vm-provisioner:
    build:
      dockerfile: ./src/vm-provisioner/Dockerfile
      context: ./src/vm-provisioner
    container_name: vm-provisioner
    restart: unless-stopped
    profiles:
      - infrastructure
      - compute
    networks:
      - backend
    privileged: true  # Required for libvirt/QEMU access
    volumes:
      - /var/run/libvirt/libvirt-sock:/var/run/libvirt/libvirt-sock
      - ${VOLUMES_ROOT}/vm-provisioner/ssh_keys:/app/ssh_keys
      - ${VOLUMES_ROOT}/vm-provisioner/vms:/var/lib/libvirt/images
    environment:
      - VM_PROVISIONER_PORT=8092
      - LIBVIRT_URI=qemu:///system
      - SSH_KEY_DIR=/app/ssh_keys
      - LIBVIRT_STORAGE_POOL=default
      - LIBVIRT_STORAGE_PATH=/var/lib/libvirt/images
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8092/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
