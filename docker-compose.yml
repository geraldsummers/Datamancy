###############################################
# Datamancy Docker Compose Configuration
#
# This file is copied to ~/.datamancy/ during installation.
# All runtime configs live in ~/.datamancy/configs/ (generated from configs.templates/)
#
# To update this file:
#   1. Edit in git repository
#   2. Run: ./install-datamancy.main.kts
#
# Stack management:
#   datamancy-controller up         - Start stack
#   datamancy-controller down       - Stop stack
#   datamancy-controller obliterate - Complete cleanup
###############################################
networks:
  frontend:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/24
          gateway: 172.20.0.1
  backend:
    driver: bridge
    ipam:
      config:
        - subnet: 172.21.0.0/24
          gateway: 172.21.0.1
  database:
    driver: bridge
    ipam:
      config:
        - subnet: 172.22.0.0/24
          gateway: 172.22.0.1

volumes:
  caddy_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${HOME}/.caddy_data
  caddy_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/caddy_config
  grafana_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/grafana_data
  open_webui_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/open_webui_data
  vaultwarden_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/vaultwarden_data
  planka_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/planka_data
  bookstack_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/bookstack_data
  seafile_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/seafile_data
  onlyoffice_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/onlyoffice_data
  synapse_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/synapse_data
  element_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/element_data
  mastodon_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/mastodon_data
  jupyterhub_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/jupyterhub_data
  homeassistant_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/homeassistant_config
  litellm_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/litellm_config
  ldap_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/ldap_data
  ldap_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/ldap_config
  lam_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/lam_config
  mariadb_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${NON_VECTOR_DBS_PATH:-${VOLUMES_ROOT:-./volumes}/databases}/mariadb_data
  postgres_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${NON_VECTOR_DBS_PATH:-${VOLUMES_ROOT:-./volumes}/databases}/postgres_data
  couchdb_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${NON_VECTOR_DBS_PATH:-${VOLUMES_ROOT:-./volumes}/databases}/couchdb_data
  clickhouse_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${NON_VECTOR_DBS_PATH:-${VOLUMES_ROOT:-./volumes}/databases}/clickhouse_data
  qdrant_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VECTOR_DBS_PATH:-${VOLUMES_ROOT:-./volumes}/vector-dbs}/qdrant_data
  benthos_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/benthos_data
  kopia_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/kopia_data
  kopia_cache:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/kopia_cache
  kopia_repository:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/kopia_repository
  redis_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${NON_VECTOR_DBS_PATH:-${VOLUMES_ROOT:-./volumes}/databases}/redis_data
  mailserver_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/mailserver_data
  mailserver_state:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/mailserver_state
  mailserver_logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/mailserver_logs
  mailserver_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/mailserver_config
#  sogo_data:
#    driver: local
#    driver_opts:
#      type: none
#      o: bind
#      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/sogo_data
  roundcube_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/roundcube_data
  radicale_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/radicale_data
  forgejo_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/forgejo_data
  qbittorrent_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/qbittorrent_config
  qbittorrent_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/qbittorrent_data
  proofs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/proofs
  data_fetcher:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/data_fetcher
  gradle-test-cache:
    driver: local
  kotlin-cache:
    driver: local

services:
  caddy:
    image: caddy:2.8.4
    container_name: caddy
    restart: unless-stopped
    networks:
      frontend: {}
      backend:
        aliases:
          - www.${DOMAIN}
          - ${DOMAIN}
          - grafana.${DOMAIN}
          - open-webui.${DOMAIN}
          - app.vaultwarden.${DOMAIN}
          - vaultwarden.${DOMAIN}
          - planka.${DOMAIN}
          - bookstack.${DOMAIN}
          - jupyterhub.${DOMAIN}
          - homepage.${DOMAIN}
          - seafile.${DOMAIN}
          - onlyoffice.${DOMAIN}
          - matrix.${DOMAIN}
          - element.${DOMAIN}
          - sogo.${DOMAIN}
          - homeassistant.${DOMAIN}
          - litellm.${DOMAIN}
          - api.litellm.${DOMAIN}
          - auth.${DOMAIN}
          - kopia.${DOMAIN}
          - portainer.${DOMAIN}
          - mail.${DOMAIN}
          - agent-tool-server.${DOMAIN}
          - lam.${DOMAIN}
          - forgejo.${DOMAIN}
          - qbittorrent.${DOMAIN}
    ports:
      - "80:80"
      - "443:443"
      - "443:443/udp"
    volumes:
      - caddy_data:/data
      - caddy_config:/config
      # Provide the static Caddyfile. Defaults to self-signed (internal CA).
      # To enable Let's Encrypt for a specific site, edit configs/infrastructure/caddy/Caddyfile
      # and add `tls you@example.com` inside that site's block.
      - ${HOME}/.datamancy/configs/infrastructure/caddy/Caddyfile:/etc/caddy/Caddyfile:ro
      # Custom landing page for Vaultwarden SSO auto-redirect
      - ${HOME}/.datamancy/configs/applications/vaultwarden/index.html:/srv/vaultwarden/index.html:ro
    environment:
      # Domain for vhosts in Caddyfile
      DOMAIN: "${DOMAIN}"
      # Default IP allowlist for api.litellm subdomain (space-separated CIDRs)
      API_LITELLM_ALLOWLIST: "${API_LITELLM_ALLOWLIST:-10.0.0.0/8 172.16.0.0/12 192.168.0.0/16 100.64.0.0/10 ::1 fc00::/7 fe80::/10}"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      # Use the caddy binary as a lightweight readiness signal
      test: ["CMD", "caddy", "version"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s


  ldap:
    image: osixia/openldap:1.5.0
    container_name: ldap
    restart: unless-stopped
    networks:
      - backend
    environment:
      - LDAP_ORGANISATION=Datamancy
      - LDAP_DOMAIN=stack.local
      - LDAP_BASE_DN=dc=stack,dc=local
      - LDAP_ADMIN_PASSWORD=${LDAP_ADMIN_PASSWORD}
      - LDAP_CONFIG_PASSWORD=${LDAP_ADMIN_PASSWORD}
      - LDAP_TLS=false
      # Configure memberOf overlay for Authelia group membership detection
      - LDAP_MEMBEROF_OVERLAY=true
      - LDAP_MEMBEROF_GROUP_OC=groupOfNames
      - LDAP_MEMBEROF_MEMBER_AD=member
    volumes:
      - ldap_data:/var/lib/ldap
      - ldap_config:/etc/ldap/slapd.d
      - ${HOME}/.datamancy/bootstrap_ldap.ldif:/container/service/slapd/assets/config/bootstrap/ldif/custom/bootstrap_ldap.ldif:ro
    command: --copy-service --loglevel info
    healthcheck:
      test: ["CMD", "ldapsearch", "-x", "-H", "ldap://localhost:389", "-b", "dc=stack,dc=local", "-D", "cn=admin,dc=stack,dc=local", "-w", "${LDAP_ADMIN_PASSWORD}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  ldap-account-manager:
    image: ghcr.io/ldapaccountmanager/lam:9.3
    container_name: ldap-account-manager
    restart: unless-stopped
    networks:
      - backend
      - frontend
    environment:
      - LDAP_DOMAIN=stack.local
      - LDAP_BASE_DN=dc=stack,dc=local
      - LDAP_USER=cn=admin,dc=stack,dc=local
      - LDAP_PASSWORD=${LDAP_ADMIN_PASSWORD}
      - LAM_PASSWORD=${LDAP_ADMIN_PASSWORD}
      - LAM_LANG=en_US
    # Note: No volume mount - LAM uses container-internal storage and connects to persistent LDAP
    depends_on:
      - ldap
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:80/lam/"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  valkey:
    image: valkey/valkey:8.0.1
    container_name: valkey
    restart: unless-stopped
    networks:
      backend:
        aliases:
          - redis
          - redis-synapse
    volumes:
      - redis_data:/data
    command: valkey-server --appendonly yes --dir /data --databases 16
    healthcheck:
      test: ["CMD", "valkey-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  authelia:
    image: authelia/authelia:4.39.13
    container_name: authelia
    restart: unless-stopped
    networks:
      - backend
      - frontend
      - database
    depends_on:
      - ldap
      - postgres
      - valkey
    volumes:
      - ${VOLUMES_ROOT}/authelia:/config
      - ${HOME}/.datamancy/configs/applications/authelia/configuration.yml:/config/configuration.yml:ro
      - ${HOME}/.datamancy/configs/applications/authelia/entrypoint.sh:/entrypoint.sh:ro
    entrypoint: ["/entrypoint.sh"]
    command: ["--config", "/config/configuration.yml"]
    environment:
      - TZ=UTC
      - AUTHELIA_JWT_SECRET=${AUTHELIA_JWT_SECRET}
      - AUTHELIA_SESSION_SECRET=${AUTHELIA_SESSION_SECRET}
      - AUTHELIA_STORAGE_ENCRYPTION_KEY=${AUTHELIA_STORAGE_ENCRYPTION_KEY}
      - AUTHELIA_AUTHENTICATION_BACKEND_LDAP_PASSWORD=${LDAP_ADMIN_PASSWORD}
      - AUTHELIA_IDENTITY_PROVIDERS_OIDC_HMAC_SECRET=${AUTHELIA_OIDC_HMAC_SECRET}
      - AUTHELIA_IDENTITY_PROVIDERS_OIDC_ISSUER_PRIVATE_KEY=${AUTHELIA_IDENTITY_PROVIDERS_OIDC_ISSUER_PRIVATE_KEY}
      - AUTHELIA_STORAGE_POSTGRES_HOST=postgres
      - AUTHELIA_STORAGE_POSTGRES_PORT=5432
      - AUTHELIA_STORAGE_POSTGRES_DATABASE=authelia
      - AUTHELIA_STORAGE_POSTGRES_USERNAME=authelia
      - AUTHELIA_STORAGE_POSTGRES_PASSWORD=${AUTHELIA_DB_PASSWORD}
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9091/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  mariadb:
    image: mariadb:11.6.2
    container_name: mariadb
    restart: unless-stopped
    networks:
      database: {}
      backend:
        aliases:
          - mysql
    environment:
      - MYSQL_ROOT_PASSWORD=${MARIADB_ROOT_PASSWORD}
      - MYSQL_DATABASE=datamancy
      - MYSQL_USER=${STACK_ADMIN_USER}
      - MYSQL_PASSWORD=${MARIADB_ROOT_PASSWORD}
    volumes:
      - mariadb_data:/var/lib/mysql
      - ${HOME}/.datamancy/configs/databases/mariadb/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    command: --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci
    healthcheck:
      test: ["CMD", "mariadb-admin", "ping", "-h", "localhost", "-u", "root", "-p${MARIADB_ROOT_PASSWORD}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  postgres:
    image: postgres:16.11
    container_name: postgres
    restart: unless-stopped
    networks:
      database:
        aliases:
          - db
      backend: {}
    environment:
      - POSTGRES_PASSWORD=${POSTGRES_ROOT_PASSWORD}
      - POSTGRES_USER=${STACK_ADMIN_USER}
      - POSTGRES_DB=postgres
      - POSTGRES_ROOT_PASSWORD=${POSTGRES_ROOT_PASSWORD}
      - PLANKA_DB_PASSWORD=${PLANKA_DB_PASSWORD}
      - SYNAPSE_DB_PASSWORD=${SYNAPSE_DB_PASSWORD}
      - AUTHELIA_DB_PASSWORD=${AUTHELIA_DB_PASSWORD}
      - GRAFANA_DB_PASSWORD=${GRAFANA_DB_PASSWORD}
      - VAULTWARDEN_DB_PASSWORD=${VAULTWARDEN_DB_PASSWORD}
      - OPENWEBUI_DB_PASSWORD=${OPENWEBUI_DB_PASSWORD}
      - MASTODON_DB_PASSWORD=${MASTODON_DB_PASSWORD}
      - FORGEJO_DB_PASSWORD=${FORGEJO_DB_PASSWORD}
      - HOMEASSISTANT_DB_PASSWORD=${HOMEASSISTANT_DB_PASSWORD:-}
      - STACK_ADMIN_PASSWORD=${STACK_ADMIN_PASSWORD}
      - AGENT_POSTGRES_OBSERVER_PASSWORD=${AGENT_POSTGRES_OBSERVER_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ${HOME}/.datamancy/configs/databases/postgres/init-db.sh:/docker-entrypoint-initdb.d/init-db.sh:ro
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
    command: postgres -c 'max_connections=300' -c 'shared_buffers=1GB'
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${STACK_ADMIN_USER} -d postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  couchdb:
    image: couchdb:3.5.1
    container_name: couchdb
    restart: unless-stopped
    networks:
      - database
    environment:
      - COUCHDB_USER=${STACK_ADMIN_USER}
      - COUCHDB_PASSWORD=${COUCHDB_ADMIN_PASSWORD}
    volumes:
      - couchdb_data:/opt/couchdb/data
      - ${HOME}/.datamancy/configs/databases/couchdb/init-wrapper.sh:/usr/local/bin/init-wrapper.sh:ro
    entrypoint: ["/bin/bash", "/usr/local/bin/init-wrapper.sh"]
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://127.0.0.1:5984/_up | grep -q '\"status\":\"ok\"' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s


  memcached:
    image: memcached:1.6.39
    container_name: seafile-memcached
    restart: unless-stopped
    networks:
      - backend
    healthcheck:
      test: ["CMD-SHELL", "timeout 2 bash -c '</dev/tcp/127.0.0.1/11211' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  mailserver-cert-init:
    image: alpine:latest
    container_name: mailserver-cert-init
    restart: "no"
    networks:
      - backend
    depends_on:
      caddy:
        condition: service_healthy
    environment:
      - DOMAIN=${DOMAIN}
    command: >
      sh -c "
        apk add --no-cache curl &&
        echo 'Triggering Caddy to generate certificate for mail.${DOMAIN}...' &&
        curl -k https://mail.${DOMAIN}/ -o /dev/null || true &&
        echo 'Certificate generation triggered successfully'
      "

  mailserver:
    image: ghcr.io/docker-mailserver/docker-mailserver:14.0.0
    container_name: mailserver
    hostname: mail.${DOMAIN}
    restart: unless-stopped
    depends_on:
      ldap:
        condition: service_started
      mailserver-cert-init:
        condition: service_completed_successfully
    ports:
      - "25:25"
      - "465:465"
      - "587:587"
      - "143:143"
      - "993:993"
    volumes:
      - mailserver_data:/var/mail/
      - mailserver_state:/var/mail-state/
      - mailserver_logs:/var/log/mail/
      - mailserver_config:/tmp/docker-mailserver/
      - /etc/localtime:/etc/localtime:ro
      # Mount Caddy's Let's Encrypt certificates
      - caddy_data:/caddy-certs:ro
      # Dovecot master user configuration for SOGo
      - ${HOME}/.datamancy/configs/applications/mailserver/dovecot-master-users.conf:/tmp/docker-mailserver/dovecot-master-users.conf:ro
      - ${HOME}/.datamancy/configs/applications/mailserver/local.conf:/etc/dovecot/local.conf:ro
    environment:
      - ENABLE_RSPAMD=1
      - ENABLE_CLAMAV=1
      - ENABLE_FAIL2BAN=1
      - ENABLE_POSTGREY=0
      - ENABLE_SPAMASSASSIN=0
      - SPOOF_PROTECTION=0
      - ENABLE_SRS=0
      - PERMIT_DOCKER=network
      - SSL_TYPE=manual
      - SSL_CERT_PATH=/caddy-certs/caddy/certificates/acme.zerossl.com-v2-dv90/mail.${DOMAIN}/mail.${DOMAIN}.crt
      - SSL_KEY_PATH=/caddy-certs/caddy/certificates/acme.zerossl.com-v2-dv90/mail.${DOMAIN}/mail.${DOMAIN}.key
      - OVERRIDE_HOSTNAME=mail.${DOMAIN}
      - POSTMASTER_ADDRESS=postmaster@${MAIL_DOMAIN}
      - ACCOUNT_PROVISIONER=LDAP
      - LDAP_SERVER_HOST=ldap://ldap:389
      - LDAP_BIND_DN=cn=admin,dc=stack,dc=local
      - LDAP_BIND_PW=${LDAP_ADMIN_PASSWORD}
      - LDAP_SEARCH_BASE=ou=users,dc=stack,dc=local
      - LDAP_QUERY_FILTER_USER=(&(objectClass=inetOrgPerson)(mail=%s))
      - LDAP_QUERY_FILTER_GROUP=(&(objectClass=groupOfNames)(member=%s))
      - LDAP_QUERY_FILTER_ALIAS=(&(objectClass=inetOrgPerson)(mail=%s))
      - DOVECOT_MAILBOX_FORMAT=maildir
      - DOVECOT_USER_FILTER=(&(objectClass=inetOrgPerson)(mail=%u))
      - DOVECOT_PASS_FILTER=(&(objectClass=inetOrgPerson)(mail=%u))
      - DOVECOT_PASS_ATTRS=mail=user,userPassword=password
      - DOVECOT_USER_ATTRS==home=/var/mail/%Ln,=mail=maildir:~/Maildir
      - ENABLE_QUOTAS=1
      - POSTFIX_MESSAGE_SIZE_LIMIT=50000000
    networks:
      backend:
        aliases:
          - smtp
          - imap
    cap_add:
      - NET_ADMIN
    healthcheck:
      test: ["CMD-SHELL", "supervisorctl status postfix | grep -q RUNNING && supervisorctl status dovecot | grep -q RUNNING"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 90s


  roundcube:
    image: roundcube/roundcubemail:latest
    container_name: roundcube
    restart: unless-stopped
    environment:
      - ROUNDCUBEMAIL_DB_TYPE=pgsql
      - ROUNDCUBEMAIL_DB_HOST=postgres
      - ROUNDCUBEMAIL_DB_PORT=5432
      - ROUNDCUBEMAIL_DB_USER=roundcube
      - ROUNDCUBEMAIL_DB_PASSWORD=${STACK_ADMIN_PASSWORD}
      - ROUNDCUBEMAIL_DB_NAME=roundcube
      - ROUNDCUBEMAIL_DEFAULT_HOST=mailserver
      - ROUNDCUBEMAIL_DEFAULT_PORT=143
      - ROUNDCUBEMAIL_SMTP_SERVER=mailserver
      - ROUNDCUBEMAIL_SMTP_PORT=587
      - ROUNDCUBEMAIL_UPLOAD_MAX_FILESIZE=50M
    volumes:
      - roundcube_data:/var/www/html
      - ${HOME}/.datamancy/configs/applications/roundcube/config.inc.php:/var/www/html/config/config.inc.php:ro
    networks:
      - backend
    depends_on:
      - postgres
      - mailserver
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  radicale:
    image: tomsquest/docker-radicale:latest
    container_name: radicale
    restart: unless-stopped
    environment:
      - TZ=UTC
    volumes:
      - radicale_data:/data
      - ${HOME}/.datamancy/configs/applications/radicale/config:/config/config:ro
      - ${HOME}/.datamancy/configs/applications/radicale/users:/data/users
    networks:
      - backend
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5232/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

#  sogo:
#    image: pmietlicki/sogo:v5
#    container_name: sogo
#    restart: unless-stopped
#    environment:
#      - TZ=UTC
#    volumes:
#      - sogo_data:/var/lib/sogo
#      - ${HOME}/.datamancy/configs/applications/sogo/sogo.conf:/etc/sogo/sogo.conf:ro
#      - ${HOME}/.datamancy/configs/applications/sogo/init-apache.sh:/docker-entrypoint.d/init-apache.sh:ro
#    networks:
#      - backend
#    healthcheck:
#      test: ["CMD-SHELL", "timeout 2 bash -c '</dev/tcp/127.0.0.1/20000' || exit 1"]
#      interval: 30s
#      timeout: 10s
#      retries: 3
#      start_period: 60s
#
#  sogo-init:
#    image: docker:27-cli
#    container_name: sogo-init
#    restart: "no"
#    depends_on:
#      sogo:
#        condition: service_healthy
#    volumes:
#      - ${DOCKER_SOCKET:-/var/run/docker.sock}:/var/run/docker.sock:ro
#    command: docker exec sogo bash /docker-entrypoint.d/init-apache.sh


  kopia:
    image: kopia/kopia:0.18.2
    container_name: kopia
    restart: "no"  # Manual start after all services are up to avoid volume mount race conditions
    networks:
      - backend
    volumes:
      - kopia_data:/app/config
      - kopia_cache:/app/cache
      # Mount volumes to backup (read-only)
      - postgres_data:/backup/postgres:ro
      - mariadb_data:/backup/mariadb:ro
      - grafana_data:/backup/grafana:ro
      - open_webui_data:/backup/open_webui:ro
      - vaultwarden_data:/backup/vaultwarden:ro
      - kopia_repository:/repository
      - ${HOME}/.datamancy/configs/applications/kopia/init-kopia.sh:/init-kopia.sh:ro
    environment:
      - KOPIA_PASSWORD=${STACK_ADMIN_PASSWORD}
      - KOPIA_REPO_PATH=/repository
      - USER=kopia
    entrypoint: ["sh", "/init-kopia.sh"]
    healthcheck:
      test: ["CMD", "kopia", "repository", "status"]
      interval: 5m
      timeout: 30s
      retries: 3
      start_period: 1m

  docker-proxy:
    image: tecnativa/docker-socket-proxy:v0.4.1
    container_name: docker-proxy
    restart: unless-stopped
    networks:
      - backend
    environment:
      # Minimal read-only permissions for security
      # See: https://github.com/Tecnativa/docker-socket-proxy
      - CONTAINERS=1  # Read container status (needed for health checks, logs)
      - IMAGES=1      # Enabled for JupyterHub spawner to check image availability
      - NETWORKS=1    # Enabled for JupyterHub spawner to attach containers
      - VOLUMES=1     # Enabled for JupyterHub to mount volumes
      - POST=1        # Enabled for JupyterHub to spawn notebook containers
      - DELETE=1      # Enabled for JupyterHub to stop/remove containers
      - BUILD=0       # Disabled - no image builds
      - COMMIT=0
      - CONFIGS=0
      - DISTRIBUTION=0
      - EXEC=0        # Disabled - prevents arbitrary command execution
      - GRPC=0
      - INFO=1        # Enabled for JupyterHub to query Docker info
      - PLUGINS=0
      - SERVICES=0
      - SESSION=0
      - SWARM=0
      - SYSTEM=0
      - TASKS=0
      - VERSION=1     # Enabled - allows version checks for compatibility
    volumes:
      - ${DOCKER_SOCKET:-/var/run/docker.sock}:/var/run/docker.sock:ro
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:2375/version"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  grafana:
    image: grafana/grafana:11.6.7
    container_name: grafana
    restart: unless-stopped
    networks:
      - backend
      - database
    depends_on:
      - postgres
      - authelia
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    environment:
      - GF_DEFAULT_LOCALE=en_US
      - GF_SERVER_ROOT_URL=https://grafana.${DOMAIN}
      - GF_SERVER_DOMAIN=grafana.${DOMAIN}
      - GF_DATABASE_TYPE=postgres
      - GF_DATABASE_HOST=postgres:5432
      - GF_DATABASE_NAME=grafana
      - GF_DATABASE_USER=grafana
      - GF_DATABASE_PASSWORD=${GRAFANA_DB_PASSWORD}
      - GF_DATABASE_SSL_MODE=disable
      - GF_AUTH_GENERIC_OAUTH_ENABLED=true
      - GF_AUTH_GENERIC_OAUTH_NAME=Authelia
      - GF_AUTH_GENERIC_OAUTH_CLIENT_ID=grafana
      - GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET=${GRAFANA_OAUTH_SECRET}
      - GF_AUTH_GENERIC_OAUTH_SCOPES=openid profile email groups
      - GF_AUTH_GENERIC_OAUTH_AUTH_URL=https://auth.${DOMAIN}/api/oidc/authorization
      - GF_AUTH_GENERIC_OAUTH_TOKEN_URL=http://authelia:9091/api/oidc/token
      - GF_AUTH_GENERIC_OAUTH_API_URL=http://authelia:9091/api/oidc/userinfo
      - GF_AUTH_GENERIC_OAUTH_LOGIN_ATTRIBUTE_PATH=email
      - GF_AUTH_GENERIC_OAUTH_EMAIL_ATTRIBUTE_PATH=email
      - GF_AUTH_GENERIC_OAUTH_GROUPS_ATTRIBUTE_PATH=groups
      - GF_AUTH_GENERIC_OAUTH_NAME_ATTRIBUTE_PATH=name
      - GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_PATH=contains(groups[*], 'admins') && 'Admin' || contains(groups[*], 'users') && 'Editor' || 'Viewer'
      - GF_AUTH_GENERIC_OAUTH_USE_PKCE=true
      - GF_AUTH_GENERIC_OAUTH_AUTH_STYLE=2  # 2 = pass client credentials in POST body (client_secret_post)
      - GF_AUTH_SIGNOUT_REDIRECT_URL=https://auth.${DOMAIN}/logout
      - GF_AUTH_GENERIC_OAUTH_SKIP_ORG_ROLE_SYNC=false
      - GF_AUTH_GENERIC_OAUTH_AUTO_LOGIN=false
      - GF_AUTH_GENERIC_OAUTH_ALLOW_SIGN_UP=true
      - GF_USERS_ALLOW_SIGN_UP=true
      - GF_USERS_AUTO_ASSIGN_ORG=true
      - GF_USERS_AUTO_ASSIGN_ORG_ROLE=Viewer
      - GF_AUTH_DISABLE_LOGIN_FORM=false
      - GF_SECURITY_ADMIN_USER=${STACK_ADMIN_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${STACK_ADMIN_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana
      - ${HOME}/.datamancy/configs/applications/grafana/grafana.ini:/etc/grafana/grafana.ini:ro
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  open-webui:
     image: ghcr.io/open-webui/open-webui:0.3.32
     container_name: open-webui
     restart: unless-stopped
     networks:
       - backend
       - frontend
       - database
     depends_on:
       - postgres
       - litellm
       - authelia
     dns:
       - 8.8.8.8
       - 1.1.1.1
     environment:
       - WEBUI_URL=${OPENWEBUI_WEBUI_URL:-https://open-webui.${DOMAIN}}
       # PostgreSQL database - use URL-encoded password for special characters
       - DATABASE_URL=postgresql://openwebui:${OPENWEBUI_DB_PASSWORD_ENCODED}@postgres:5432/openwebui
       - ENABLE_OAUTH_SIGNUP=${OPENWEBUI_ENABLE_OAUTH_SIGNUP:-true}
       - DEFAULT_USER_ROLE=${OPENWEBUI_DEFAULT_USER_ROLE:-admin}
       - ENABLE_OPENAI_API=true
       - OPENAI_API_BASE_URL=${OPENWEBUI_OPENAI_API_BASE_URL:-http://litellm:4000/v1}
       - OPENAI_API_KEY=${LITELLM_MASTER_KEY}
       # OIDC SSO Configuration
       - OPENID_PROVIDER_URL=https://auth.${DOMAIN}
       - OAUTH_CLIENT_ID=open-webui
       - OAUTH_CLIENT_SECRET=${OPENWEBUI_OAUTH_SECRET}
       - OAUTH_PROVIDER_NAME=Authelia
       - OAUTH_SCOPES=openid profile email groups
       # Seamless SSO - skip login form, go straight to OAuth
       - ENABLE_LOGIN_FORM=false
       - WEBUI_AUTH=false
     volumes:
       - open_webui_data:/app/backend/data
     healthcheck:
       test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
       interval: 30s
       timeout: 10s
       retries: 3
       start_period: 30s

  vaultwarden:
    image: vaultwarden/server:testing
    container_name: vaultwarden
    restart: unless-stopped
    networks:
      - backend
      - database
    depends_on:
      - postgres
      - mailserver
      - authelia
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    environment:
      - DOMAIN=https://app.vaultwarden.${DOMAIN}
      - DATABASE_URL=postgresql://vaultwarden:${VAULTWARDEN_DB_PASSWORD}@postgres:5432/vaultwarden
      - SIGNUPS_ALLOWED=false
      - SIGNUPS_VERIFY=true
      - SIGNUPS_DOMAINS_WHITELIST=${MAIL_DOMAIN}
      - INVITATIONS_ALLOWED=false
      - SHOW_PASSWORD_HINT=false
      # SSO/OIDC Configuration
      - SSO_ENABLED=true
      - SSO_ONLY=true
      - SSO_AUTHORITY=https://auth.${DOMAIN}
      - SSO_CLIENT_ID=vaultwarden
      - SSO_CLIENT_SECRET=${VAULTWARDEN_OAUTH_SECRET}
      - SSO_SCOPES=openid email profile
      - SSO_CALLBACK_PATH=/identity/connect/oidc-signin
      - SSO_SIGNUPS_MATCH_EMAIL=true
      # Admin and SMTP
      - ADMIN_TOKEN=${VAULTWARDEN_ADMIN_TOKEN}
      - SMTP_HOST=mailserver
      - SMTP_FROM=vaultwarden@${MAIL_DOMAIN}
      - SMTP_PORT=587
      - SMTP_SECURITY=starttls
      - SMTP_USERNAME=vaultwarden@${MAIL_DOMAIN}
      - SMTP_PASSWORD=${VAULTWARDEN_SMTP_PASSWORD}
    volumes:
      - vaultwarden_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/alive"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  planka:
    image: ghcr.io/plankanban/planka:1.26.3
    container_name: planka
    restart: unless-stopped
    networks:
      - backend
    depends_on:
      - postgres
      - authelia
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    environment:
      - BASE_URL=https://planka.${DOMAIN}
      # Use URL-encoded password for PostgreSQL connection string (encoded version of PLANKA_DB_PASSWORD)
      - DATABASE_URL=postgresql://planka:${PLANKA_DB_PASSWORD_ENCODED:-${PLANKA_DB_PASSWORD}}@postgres:5432/planka
      - SECRET_KEY=${PLANKA_SECRET_KEY}
      - OIDC_ISSUER=https://auth.${DOMAIN}
      - OIDC_CLIENT_ID=planka
      - OIDC_CLIENT_SECRET=${PLANKA_OAUTH_SECRET}
      - OIDC_SCOPES=openid profile email groups
      - OIDC_ADMIN_ROLES=admins
      - OIDC_EMAIL_ATTRIBUTE=email
      - OIDC_NAME_ATTRIBUTE=name
      - OIDC_USERNAME_ATTRIBUTE=preferred_username
      - OIDC_REDIRECT_URI=https://planka.${DOMAIN}/oidc-callback
      - OIDC_AUTHORIZATION_ENDPOINT=https://auth.${DOMAIN}/api/oidc/authorization
      - OIDC_TOKEN_ENDPOINT=http://authelia:9091/api/oidc/token
      - OIDC_USERINFO_ENDPOINT=http://authelia:9091/api/oidc/userinfo
      - OIDC_TOKEN_ENDPOINT_AUTH_METHOD=client_secret_post
      - OIDC_ENFORCED=false
      - NODE_EXTRA_CA_CERTS=/usr/local/share/ca-certificates/caddy-ca.crt
    volumes:
      - planka_data:/app/public/user-avatars
      - ${HOME}/.datamancy/configs/applications/planka/caddy-ca.crt:/usr/local/share/ca-certificates/caddy-ca.crt:ro
    healthcheck:
      test: ["CMD", "wget", "-q", "-O", "/dev/null", "http://localhost:1337/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  bookstack:
    image: lscr.io/linuxserver/bookstack:24.12.1
    container_name: bookstack
    hostname: bookstack
    restart: unless-stopped
    networks:
      - backend
    depends_on:
      - mariadb
    links:
      - mariadb:mariadb
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=UTC
      - APP_URL=https://bookstack.${DOMAIN}
      - APP_KEY=${BOOKSTACK_APP_KEY}
      - DB_HOST=mariadb
      - DB_PORT=3306
      - DB_USER=bookstack
      - DB_PASS=${BOOKSTACK_DB_PASSWORD}
      - DB_DATABASE=bookstack
      - LDAP_ADMIN_PASSWORD=${LDAP_ADMIN_PASSWORD}
      - BOOKSTACK_OAUTH_SECRET=${BOOKSTACK_OAUTH_SECRET}
      # OIDC SSO configuration - init script will configure .env file
      # AUTH_METHOD, OIDC_* settings are configured by 50-fix-env.sh
    volumes:
      - bookstack_data:/config
      - ${VOLUMES_ROOT}/bookstack_init:/custom-cont-init.d:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 90s

  seafile:
    image: seafileltd/seafile-mc:13.0.12
    container_name: seafile
    restart: unless-stopped
    depends_on:
      - mariadb
      - memcached
    environment:
      - TZ=UTC
      - DB_HOST=mysql  # Use alias for mariadb-seafile
      - DB_ROOT_PASSWD=${MARIADB_ROOT_PASSWORD}
      - SEAFILE_MYSQL_DB_HOST=mysql
      - SEAFILE_MYSQL_DB_PORT=3306
      - SEAFILE_MYSQL_DB_USER=seafile
      - SEAFILE_MYSQL_DB_PASSWORD=${MARIADB_SEAFILE_PASSWORD}
      - INIT_SEAFILE_MYSQL_ROOT_PASSWORD=${MARIADB_ROOT_PASSWORD}
      - SEAFILE_SERVER_HOSTNAME=seafile.${DOMAIN}
      - SEAFILE_ADMIN_EMAIL=${STACK_ADMIN_EMAIL}
      - SEAFILE_ADMIN_PASSWORD=${STACK_ADMIN_PASSWORD}
      - JWT_PRIVATE_KEY=${SEAFILE_JWT_KEY}
      - TIME_ZONE=UTC
    volumes:
      - seafile_data:/shared
      # Mount database settings template (will be applied by init container)
      - ${HOME}/.datamancy/configs/applications/seafile/seahub_settings.py:/tmp/seahub_settings_template.py:ro
    networks:
      - backend
    healthcheck:
      test: ["CMD-SHELL", "curl -s -o /dev/null -w '%{http_code}' http://localhost:8000/ | grep -E '^(200|302)$'"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 180s

  seafile-db-init:
    image: seafileltd/seafile-mc:13.0.12
    container_name: seafile-db-init
    restart: "no"
    depends_on:
      seafile:
        condition: service_started
    volumes:
      - seafile_data:/shared
      - ${HOME}/.datamancy/configs/applications/seafile/seahub_settings.py:/tmp/seahub_settings_template.py:ro
      - ${HOME}/.datamancy/configs/applications/seafile/init/apply-db-config.sh:/apply-db-config.sh:ro
    command: ["bash", "/apply-db-config.sh"]
    networks:
      - backend

  onlyoffice:
    image: onlyoffice/documentserver:8.2.2
    container_name: onlyoffice
    restart: unless-stopped
    depends_on:
      - seafile
    environment:
      - TZ=UTC
      - JWT_ENABLED=true
      - JWT_SECRET=${ONLYOFFICE_JWT_SECRET}
      - JWT_HEADER=Authorization
    volumes:
      - onlyoffice_data:/var/www/onlyoffice/Data
    networks:
      - backend
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/healthcheck"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 2m

  synapse-init:
    image: alpine:latest
    container_name: synapse-init
    volumes:
      - synapse_data:/data
    command: chown -R 991:991 /data

  synapse:
    build:
      dockerfile: ./src/synapse/Dockerfile
      context: .
    container_name: synapse
    restart: unless-stopped
    depends_on:
      synapse-init:
        condition: service_completed_successfully
      postgres:
        condition: service_started
      valkey:
        condition: service_started
      ldap:
        condition: service_started
    environment:
      - SYNAPSE_SERVER_NAME=matrix.${DOMAIN}
      - SYNAPSE_REPORT_STATS=yes
      - SYNAPSE_ENABLE_REGISTRATION=${MATRIX_ENABLE_REGISTRATION:-false}
      - SYNAPSE_TRUSTED_PROXIES=172.18.0.0/16,172.21.0.0/24
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=synapse
      - POSTGRES_USER=synapse
      - POSTGRES_PASSWORD=${SYNAPSE_DB_PASSWORD}
      - REDIS_HOST=redis-synapse
      - LDAP_ADMIN_PASSWORD=${LDAP_ADMIN_PASSWORD}
    volumes:
      - synapse_data:/data
      - ${HOME}/.datamancy/configs/applications/synapse/homeserver.yaml:/data/homeserver.yaml:ro
    networks:
      - backend
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8008/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  element:
    image: vectorim/element-web:v1.11.88
    container_name: element
    restart: unless-stopped
    networks:
      - backend
    volumes:
      - element_data:/app/config
      - ${HOME}/.datamancy/configs/applications/element/config.json:/app/config.json:ro
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:80/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  mastodon-web:
    image: ghcr.io/mastodon/mastodon:v4.3.3
    container_name: mastodon-web
    restart: unless-stopped
    networks:
      - backend
    depends_on:
      - postgres
      - valkey
    env_file:
      - ${HOME}/.datamancy/configs/applications/mastodon/mastodon.env
    volumes:
      - mastodon_data:/mastodon/public/system
      - ${HOME}/.datamancy/configs/applications/mastodon/entrypoint.sh:/entrypoint.sh:ro
    entrypoint: ["/bin/bash", "/entrypoint.sh"]
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '2.0'
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider --proxy=off localhost:3000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 180s

  mastodon-streaming:
    image: ghcr.io/mastodon/mastodon-streaming:v4.3.3
    container_name: mastodon-streaming
    restart: unless-stopped
    networks:
      - backend
    depends_on:
      - postgres
      - valkey
    env_file:
      - ${HOME}/.datamancy/configs/applications/mastodon/mastodon.env
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider --proxy=off localhost:4000/api/v1/streaming/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  mastodon-sidekiq:
    image: ghcr.io/mastodon/mastodon:v4.3.3
    container_name: mastodon-sidekiq
    restart: unless-stopped
    networks:
      - backend
    depends_on:
      - postgres
      - valkey
    env_file:
      - ${HOME}/.datamancy/configs/applications/mastodon/mastodon.env
    command: bundle exec sidekiq
    volumes:
      - mastodon_data:/mastodon/public/system
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '2.0'
    healthcheck:
      test: ["CMD-SHELL", "bundle exec rails runner 'exit(Sidekiq::ProcessSet.new.size > 0 ? 0 : 1)' || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s

  homepage:
    image: ghcr.io/gethomepage/homepage:v1.7.0
    container_name: homepage
    restart: unless-stopped
    networks:
      - backend
    depends_on:
      - docker-proxy
    environment:
      - PUID=1000
      - PGID=1000
      - HOMEPAGE_ALLOWED_HOSTS=homepage.${DOMAIN}
      - DOCKER_HOST=tcp://docker-proxy:2375
    volumes:
      - ${HOME}/.datamancy/configs/applications/homepage/services.yaml:/app/config/services.yaml:ro
      - ${HOME}/.datamancy/configs/applications/homepage/settings.yaml:/app/config/settings.yaml:ro
      - ${HOME}/.datamancy/configs/applications/homepage/widgets.yaml:/app/config/widgets.yaml:ro
      - ${HOME}/.datamancy/configs/applications/homepage/bookmarks.yaml:/app/config/bookmarks.yaml:ro
      - ${HOME}/.datamancy/configs/applications/homepage/docker.yaml:/app/config/docker.yaml:ro
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://127.0.0.1:3000/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  jupyter-notebook-builder:
    build:
      dockerfile: ./src/jupyter-notebook/Dockerfile.ai-ide
      context: .
      tags:
        - datamancy-jupyter-notebook:latest
    image: datamancy-jupyter-notebook:latest
    container_name: jupyter-notebook-builder
    command: ["echo", "AI-powered Jupyter IDE image built successfully"]
    restart: "no"

  jupyterhub:
    build:
      dockerfile: ./src/jupyterhub/Dockerfile
      context: .
    container_name: jupyterhub
    restart: unless-stopped
    networks:
      - backend
    depends_on:
      jupyter-notebook-builder:
        condition: service_completed_successfully
      docker-proxy:
        condition: service_started
      litellm:
        condition: service_started
      authelia:
        condition: service_started
    environment:
      - DOMAIN=${DOMAIN}
      - JUPYTERHUB_OAUTH_SECRET=${JUPYTERHUB_OAUTH_SECRET}
      - DOCKER_HOST=tcp://docker-proxy:2375
      - LITELLM_API_KEY=${LITELLM_MASTER_KEY}
    volumes:
      - jupyterhub_data:/srv/jupyterhub
      - ./src/jupyterhub/jupyterhub_config.py:/srv/jupyterhub/jupyterhub_config.py:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/hub/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  homeassistant:
    image: ghcr.io/home-assistant/home-assistant:stable
    container_name: homeassistant
    restart: unless-stopped
    networks:
      - backend
      - database
    depends_on:
      - postgres
      - ldap
    environment:
      - TZ=UTC
      - HOMEASSISTANT_DB_URL=postgresql://${STACK_ADMIN_USER}:${HOMEASSISTANT_DB_PASSWORD}@postgres:5432/homeassistant
      - STACK_ADMIN_USER=${STACK_ADMIN_USER}
      - STACK_ADMIN_PASSWORD=${STACK_ADMIN_PASSWORD}
      - STACK_ADMIN_EMAIL=${STACK_ADMIN_EMAIL}
      - LDAP_HOST=ldap
      - LDAP_PORT=389
      - LDAP_BASE_DN=dc=stack,dc=local
      - LDAP_BIND_DN=cn=admin,dc=stack,dc=local
      - LDAP_BIND_PASSWORD=${LDAP_ADMIN_PASSWORD}
      - LDAP_USER_FILTER=(uid={username})
    volumes:
      - homeassistant_config:/config
      - /etc/localtime:/etc/localtime:ro
      - ${HOME}/.datamancy/configs/applications/homeassistant/configuration.yaml:/config/configuration.yaml:ro
      - ${HOME}/.datamancy/configs/applications/homeassistant/automations.yaml:/config/automations.yaml:rw
      - ${HOME}/.datamancy/configs/applications/homeassistant/scripts.yaml:/config/scripts.yaml:rw
      - ${HOME}/.datamancy/configs/applications/homeassistant/scenes.yaml:/config/scenes.yaml:rw
      - ${HOME}/.datamancy/configs/applications/homeassistant/init-homeassistant.sh:/init-homeassistant.sh:ro
      - ${HOME}/.datamancy/configs/applications/homeassistant/entrypoint.sh:/entrypoint-wrapper.sh:ro
      - ${HOME}/.datamancy/configs/applications/homeassistant/auth_ldap.py:/usr/src/homeassistant/homeassistant/auth/providers/auth_ldap.py:ro
    entrypoint: ["/entrypoint-wrapper.sh"]
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8123/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  forgejo:
    image: codeberg.org/forgejo/forgejo:9
    container_name: forgejo
    restart: unless-stopped
    networks:
      - backend
      - database
    depends_on:
      - postgres
    environment:
      - USER_UID=1000
      - USER_GID=1000
      - FORGEJO__database__DB_TYPE=postgres
      - FORGEJO__database__HOST=postgres:5432
      - FORGEJO__database__NAME=forgejo
      - FORGEJO__database__USER=forgejo
      - FORGEJO__database__PASSWD=${FORGEJO_DB_PASSWORD}
      - FORGEJO__server__DOMAIN=forgejo.${DOMAIN}
      - FORGEJO__server__SSH_DOMAIN=forgejo.${DOMAIN}
      - FORGEJO__server__ROOT_URL=https://forgejo.${DOMAIN}/
      - FORGEJO__security__INSTALL_LOCK=true
      - FORGEJO__service__DISABLE_REGISTRATION=true
      - FORGEJO__service__ALLOW_ONLY_EXTERNAL_REGISTRATION=true
      - FORGEJO__service__REQUIRE_SIGNIN_VIEW=true
      - FORGEJO__oauth2_client__ENABLE_AUTO_REGISTRATION=true
      - FORGEJO__openid__ENABLE_OPENID_SIGNIN=true
      - FORGEJO__openid__ENABLE_OPENID_SIGNUP=true
      - FORGEJO_OAUTH_SECRET=${FORGEJO_OAUTH_SECRET}
      - DOMAIN=${DOMAIN}
    volumes:
      - forgejo_data:/data
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  forgejo-init:
    image: codeberg.org/forgejo/forgejo:9
    container_name: forgejo-init
    restart: "no"
    user: "1000:1000"
    networks:
      - backend
    depends_on:
      forgejo:
        condition: service_healthy
    environment:
      - FORGEJO_OAUTH_SECRET=${FORGEJO_OAUTH_SECRET}
      - DOMAIN=${DOMAIN}
    volumes:
      - forgejo_data:/data
      - ${HOME}/.datamancy/configs/applications/forgejo/init-forgejo.sh:/init-forgejo.sh:ro
    command: ["bash", "/init-forgejo.sh"]

  qbittorrent:
    image: lscr.io/linuxserver/qbittorrent:latest
    container_name: qbittorrent
    restart: unless-stopped
    networks:
      - backend
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=UTC
      - WEBUI_PORT=8080
    volumes:
      - qbittorrent_config:/config
      - qbittorrent_data:/downloads
      # Mount pre-configured qBittorrent.conf with subnet whitelist
      - ${VOLUMES_ROOT}/qbittorrent_init:/custom-cont-init.d:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm
    restart: unless-stopped
    networks:
      - backend
    environment:
      - HUGGINGFACEHUB_API_TOKEN=${HUGGINGFACEHUB_API_TOKEN}
      - HF_HUB_ENABLE_HF_TRANSFER=0
    command: [
      "--model", "Qwen/Qwen2.5-0.5B-Instruct",
      "--served-model-name", "qwen2.5-0.5b",
      "--max-model-len", "2048",
      "--max-num-seqs", "64",
      "--dtype", "auto",
      "--enable-auto-tool-choice",
      "--tool-call-parser", "hermes",
      "--gpu-memory-utilization", "0.3"
    ]
    volumes:
      - ${VOLUMES_ROOT}/vllm/hf-cache:/root/.cache/huggingface
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4.0'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 60s
      timeout: 10s
      retries: 5
      start_period: 2m

  embedding-service:
    image: ghcr.io/huggingface/text-embeddings-inference:cpu-1.2
    container_name: embedding-service
    restart: unless-stopped
    networks:
      - backend
    command: --model-id sentence-transformers/all-MiniLM-L6-v2 --port 8080
    volumes:
      - ${VOLUMES_ROOT}/embeddings/models:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  litellm:
    image: ghcr.io/berriai/litellm:v1.74.9-stable.patch.1
    container_name: litellm
    restart: unless-stopped
    networks:
      - backend
      - frontend
    depends_on:
      - vllm
    volumes:
      - litellm_config:/app/config
      - ${HOME}/.datamancy/configs/infrastructure/litellm/config.yaml:/app/config.yaml:ro
    environment:
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}
      # Consolidated admin credentials for UI login
      - UI_USERNAME=${STACK_ADMIN_USER}
      - UI_PASSWORD=${STACK_ADMIN_PASSWORD}
    command: --config /app/config.yaml --port 4000 --num_workers 4
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:4000/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 15s

  agent-tool-server:
    build:
      dockerfile: ./src/agent-tool-server/Dockerfile
      context: .
    container_name: agent-tool-server
    restart: unless-stopped
    networks:
      - backend
      - frontend
      - database
    depends_on:
      - litellm
      - postgres
      - mariadb
      - clickhouse
      - couchdb
      - qdrant
      - ldap
    user: "1000:1000"
    group_add:
      - "985"  # docker group on host for socket access
    environment:
      - TOOLSERVER_PORT=8081
      - TOOLSERVER_PLUGINS_DIR=/app/plugins
      - TOOLSERVER_ALLOW_CAPS=host.shell.read,host.docker.write,host.docker.inspect,host.network.http,host.network.ssh
      # SSH to host via forced-command wrapper (see scripts)
      - TOOLSERVER_SSH_HOST=${TOOLSERVER_SSH_HOST:-host.docker.internal}
      - TOOLSERVER_SSH_USER=${TOOLSERVER_SSH_USER:-stackops}
      - TOOLSERVER_SSH_KEY_PATH=/app/keys/stackops_ed25519
      - TOOLSERVER_SSH_KNOWN_HOSTS=/app/known_hosts
      - TOOLSERVER_SSH_TIMEOUT_MS=20000
      # Browser service timeouts (login operations need more time)
      - TOOLSERVER_BROWSER_HTTP_TIMEOUT_MS=90000
      - TOOLSERVER_BROWSER_URL=http://playwright:3000
      - TOOLSERVER_SCREENSHOTS_DIR=/app/proofs/screenshots
      - TOOLSERVER_DEBUG=true
      # LiteLLM connection for LLM completions
      - LITELLM_BASE_URL=http://litellm:4000
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}
      # Data source observation credentials
      - POSTGRES_OBSERVER_USER=agent_observer
      - POSTGRES_OBSERVER_PASSWORD=${AGENT_POSTGRES_OBSERVER_PASSWORD}
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - MARIADB_OBSERVER_USER=agent_observer
      - MARIADB_OBSERVER_PASSWORD=${AGENT_MARIADB_OBSERVER_PASSWORD}
      - MARIADB_HOST=mariadb
      - MARIADB_PORT=3306
      - CLICKHOUSE_OBSERVER_USER=${STACK_ADMIN_USER}
      - CLICKHOUSE_OBSERVER_PASSWORD=${AGENT_CLICKHOUSE_OBSERVER_PASSWORD}
      - CLICKHOUSE_HOST=clickhouse
      - CLICKHOUSE_PORT=8123
      - COUCHDB_OBSERVER_USER=agent_observer
      - COUCHDB_OBSERVER_PASSWORD=${AGENT_COUCHDB_OBSERVER_PASSWORD}
      - COUCHDB_HOST=couchdb
      - COUCHDB_PORT=5984
      - QDRANT_OBSERVER_API_KEY=${AGENT_QDRANT_API_KEY}
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - LDAP_OBSERVER_DN=cn=agent_observer,dc=stack,dc=local
      - LDAP_OBSERVER_PASSWORD=${AGENT_LDAP_OBSERVER_PASSWORD}
      - LDAP_HOST=ldap
      - LDAP_PORT=389
      - LDAP_BASE_DN=dc=stack,dc=local
      # Search service for semantic search
      - SEARCH_SERVICE_URL=http://search-service:8097
    volumes:
      - ${DOCKER_SOCKET:-/var/run/docker.sock}:/var/run/docker.sock:ro
      - ${VOLUMES_ROOT}/secrets/stackops_ed25519:/app/keys/stackops_ed25519:ro
      - ${VOLUMES_ROOT}/agent_tool_server/known_hosts:/app/known_hosts:ro
      - ${HOME}/.datamancy/configs/infrastructure/ssh/bootstrap_known_hosts.sh:/app/scripts/bootstrap_known_hosts.sh:ro
      - proofs:/app/proofs
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "wget", "-q", "-O", "/dev/null", "http://localhost:8081/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  ###############################################
  # Vector Stack (profile: bootstrap_vector_dbs)
  ###############################################
  qdrant:
    image: qdrant/qdrant:v1.12.5
    container_name: qdrant
    restart: unless-stopped
    networks:
      - database
      - backend
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
    # Do not expose ports publicly; access internally via service name
    volumes:
      - qdrant_data:/qdrant/storage
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '2.0'
    healthcheck:
      test: ["CMD-SHELL", "timeout 5 bash -c '</dev/tcp/127.0.0.1/6333' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  clickhouse:
    image: clickhouse/clickhouse-server:24.8
    container_name: clickhouse
    restart: unless-stopped
    networks:
      - database
    environment:
      - CLICKHOUSE_DB=default
      - CLICKHOUSE_USER=${STACK_ADMIN_USER}
      - CLICKHOUSE_PASSWORD=${CLICKHOUSE_ADMIN_PASSWORD}
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - ${HOME}/.datamancy/configs/databases/clickhouse/config.xml:/etc/clickhouse-server/config.d/config.xml:ro
      - ${HOME}/.datamancy/configs/databases/clickhouse/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    # Do not expose native TCP publicly; use internal networking
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8123/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  vector-bootstrap:
    image: zenika/kotlin:latest
    container_name: vector-bootstrap
    restart: "no"
    networks:
      - backend
      - database
    depends_on:
      - qdrant
    environment:
      - QDRANT_URL=http://qdrant:6333
      - VECTOR_SIZE=${VECTOR_EMBED_SIZE:-384}
      - QDRANT_API_KEY=${QDRANT_API_KEY:-}
    volumes:
      - ${HOME}/.datamancy/configs/databases/vectors/collections.yaml:/configs/collections.yaml:ro
      - ${HOME}/.datamancy/configs/databases/vectors/bootstrap_vectors.main.kts:/scripts/bootstrap_vectors.main.kts:ro
    command: ["kotlin", "/scripts/bootstrap_vectors.main.kts", "/configs/collections.yaml"]
    healthcheck:
      test: ["CMD", "kotlin", "-version"]
      interval: 1m
      timeout: 10s
      retries: 1
      start_period: 5s

  benthos:
    image: jeffail/benthos:4.27.0
    container_name: benthos
    restart: unless-stopped
    networks:
      - backend
      - database
    depends_on:
      - qdrant
      - clickhouse
      - litellm
    volumes:
      - ${HOME}/.datamancy/configs/infrastructure/benthos/benthos.yaml:/benthos.yaml:ro
      - benthos_data:/data
    command: -c /benthos.yaml
    environment:
      - QDRANT_URL=${QDRANT_URL:-http://qdrant:6333}
      - QDRANT_API_KEY=${QDRANT_API_KEY:-}
      - LITELLM_URL=${LITELLM_URL:-http://litellm:4000/v1}
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}
      - CLICKHOUSE_DSN=${CLICKHOUSE_DSN:-clickhouse://${STACK_ADMIN_USER}:${CLICKHOUSE_ADMIN_PASSWORD}@clickhouse:9000/default?dial_timeout=5s&read_timeout=10s&compress=true}
      - EMBED_MODEL=${EMBED_MODEL:-embed-small}
      - UNIFIED_INDEXER_URL=${UNIFIED_INDEXER_URL:-http://unified-indexer:8096}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:4195/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  data-fetcher:
    build:
      dockerfile: ./src/data-fetcher/Dockerfile
      context: .
    container_name: data-fetcher
    restart: unless-stopped
    networks:
      - backend
      - database
    depends_on:
      - postgres
      - clickhouse
      - bookstack
    environment:
      - DATAFETCHER_PORT=8095
      # Database connections
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=datamancy
      - POSTGRES_USER=${STACK_ADMIN_USER}
      - POSTGRES_PASSWORD=${POSTGRES_ROOT_PASSWORD}
      - CLICKHOUSE_HOST=clickhouse
      - CLICKHOUSE_PORT=8123
      - CLICKHOUSE_USER=${STACK_ADMIN_USER}
      - CLICKHOUSE_PASSWORD=${CLICKHOUSE_ADMIN_PASSWORD}
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      # API keys (optional, from .env)
      - OPENWEATHER_API_KEY=${OPENWEATHER_API_KEY:-}
      - COINGECKO_API_KEY=${COINGECKO_API_KEY:-}
      - SERP_API_KEY=${SERP_API_KEY:-}
      - FRED_API_KEY=${FRED_API_KEY:-}
      # BookStack integration
      - BOOKSTACK_URL=${BOOKSTACK_URL:-http://bookstack:80}
      - BOOKSTACK_API_TOKEN_ID=${BOOKSTACK_API_TOKEN_ID:-}
      - BOOKSTACK_API_TOKEN_SECRET=${BOOKSTACK_API_TOKEN_SECRET:-}
      # Config paths
      - FETCH_SCHEDULES_PATH=/app/config/schedules.yaml
      - FETCH_SOURCES_PATH=/app/config/sources.yaml
      - DATAFETCHER_DATA_PATH=/app/data
    volumes:
      - ${HOME}/.datamancy/configs/applications/data-fetcher/schedules.yaml:/app/config/schedules.yaml:ro
      - ${HOME}/.datamancy/configs/applications/data-fetcher/sources.yaml:/app/config/sources.yaml:ro
      - ${APPLICATION_DATA_PATH:-${VOLUMES_ROOT:-./volumes}/applications}/data_fetcher:/app/data
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "-O", "/dev/null", "http://localhost:8095/health"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s

  unified-indexer:
    build:
      dockerfile: ./src/unified-indexer/Dockerfile
      context: .
    container_name: unified-indexer
    restart: unless-stopped
    networks:
      - backend
      - database
    depends_on:
      - postgres
      - bookstack
      - qdrant
      - clickhouse
      - embedding-service
    environment:
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=datamancy
      - POSTGRES_USER=${STACK_ADMIN_USER}
      - POSTGRES_PASSWORD=${POSTGRES_ROOT_PASSWORD}
      - BOOKSTACK_URL=${BOOKSTACK_URL:-http://bookstack:80}
      - BOOKSTACK_API_TOKEN_ID=${BOOKSTACK_API_TOKEN_ID:-}
      - BOOKSTACK_API_TOKEN_SECRET=${BOOKSTACK_API_TOKEN_SECRET:-}
      - QDRANT_URL=http://qdrant:6334
      - CLICKHOUSE_URL=http://clickhouse:8123
      - CLICKHOUSE_USER=${STACK_ADMIN_USER}
      - CLICKHOUSE_PASSWORD=${CLICKHOUSE_ADMIN_PASSWORD}
      - EMBEDDING_SERVICE_URL=http://embedding-service:8080
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "-O", "/dev/null", "http://localhost:8096/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  search-service:
    build:
      dockerfile: ./src/search-service/Dockerfile
      context: .
    container_name: search-service
    restart: unless-stopped
    networks:
      - backend
      - database
    depends_on:
      - qdrant
      - clickhouse
      - embedding-service
    environment:
      - QDRANT_URL=http://qdrant:6334
      - CLICKHOUSE_URL=http://clickhouse:8123
      - CLICKHOUSE_USER=${STACK_ADMIN_USER}
      - CLICKHOUSE_PASSWORD=${CLICKHOUSE_ADMIN_PASSWORD}
      - EMBEDDING_SERVICE_URL=http://embedding-service:8080
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "-O", "/dev/null", "http://localhost:8097/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  control-panel:
    build:
      dockerfile: ./src/control-panel/Dockerfile
      context: .
    container_name: control-panel
    restart: unless-stopped
    networks:
      - frontend
      - backend
      - database
    depends_on:
      - postgres
      - data-fetcher
      - unified-indexer
    environment:
      - PANEL_PORT=8097
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=datamancy
      - POSTGRES_USER=${STACK_ADMIN_USER}
      - POSTGRES_PASSWORD=${POSTGRES_ROOT_PASSWORD}
      - DATA_FETCHER_URL=http://data-fetcher:8095
      - UNIFIED_INDEXER_URL=http://unified-indexer:8096
      - SEARCH_SERVICE_URL=http://search-service:8000
    ports:
      - "8097:8097"
    labels:
      - "caddy=control.${DOMAIN}"
      - "caddy.reverse_proxy={{upstreams 8097}}"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "-O", "/dev/null", "http://localhost:8097/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  integration-test-runner:
    build:
      context: .
      dockerfile: Dockerfile.test
    container_name: integration-test-runner
    profiles:
      - testing
    networks:
      - backend
      - database
    depends_on:
      - postgres
      - clickhouse
      - control-panel
      - data-fetcher
      - unified-indexer
    environment:
      # PostgreSQL connection
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=datamancy
      - POSTGRES_USER=${STACK_ADMIN_USER}
      - POSTGRES_PASSWORD=${POSTGRES_ROOT_PASSWORD}
      # ClickHouse connection
      - CLICKHOUSE_HOST=clickhouse
      - CLICKHOUSE_PORT=8123
      - CLICKHOUSE_USER=${STACK_ADMIN_USER}
      - CLICKHOUSE_PASSWORD=${CLICKHOUSE_ADMIN_PASSWORD}
      # Service URLs
      - CONTROL_PANEL_URL=http://control-panel:8097
      - DATA_FETCHER_URL=http://data-fetcher:8095
      - UNIFIED_INDEXER_URL=http://unified-indexer:8096
      - SEARCH_SERVICE_URL=http://search-service:8098
    # No volumes needed - source code is baked into the image at build time

  stack-test-runner:
    image: eclipse-temurin:21-jdk
    container_name: stack-test-runner
    profiles:
      - testing
    networks:
      - backend
      - database
    working_dir: /workspace
    # Run as host user to avoid permission issues
    user: "${DOCKER_USER_ID:-1000}:${DOCKER_GROUP_ID:-1000}"
    environment:
      # Use separate Gradle directories to avoid lock conflicts with host
      - GRADLE_USER_HOME=/tmp/gradle-home
    volumes:
      # Simple approach: mount everything read-write
      - .:/workspace:rw
    command:
      - ./gradlew
      - :stack-tests:test
      - --no-daemon
      - --console=plain
      - --project-cache-dir=/tmp/gradle-project-cache
    depends_on:
      - control-panel
      - data-fetcher
      - unified-indexer
      - search-service
