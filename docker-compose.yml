###############################################
# Datamancy â€” docker-compose.yml
#
# Documentation:
# - Overview and Quickstart: README.md
# - Bootstrap guide: docs/BOOTSTRAP.md
# - Architecture overview: docs/ARCHITECTURE.md
# - App catalog (all services): docs/APP_CATALOG.md
# - Data ingestion & RAG: docs/DATA_AND_RAG.md
# - Operations & Security: docs/OPERATIONS.md, docs/SECURITY.md
#
# Tip: Services are grouped by profiles (bootstrap, bootstrap_vector_dbs, full).
###############################################
networks:
  frontend:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/24
  backend:
    driver: bridge
    ipam:
      config:
        - subnet: 172.21.0.0/24
  database:
    driver: bridge
    ipam:
      config:
        - subnet: 172.22.0.0/24

volumes:
  caddy_data:
  caddy_config:
  grafana_data:
  open_webui_data:
  vaultwarden_data:
  planka_data:
  outline_data:
  seafile_data:
  onlyoffice_data:
  synapse_data:
  mastodon_data:
  jupyterhub_data:
  homeassistant_config:
  litellm_config:
  localai_models:
  ldap_data:
  ldap_config:
  lam_config:
  mariadb_data:
  postgres_data:
  couchdb_data:
  clickhouse_data:
  qdrant_data:
  benthos_data:
  dockge_data:
  kopia_data:
  kopia_cache:
  kopia_repository:
  redis_data:
  seafile_mysql_data:
  mailu_data:
  mailu_filter:
  mailu_cert:
  mailu_dkim:
  sogo_data:
  portainer_data:

services:
  caddy-docker-proxy:
    image: lucaslorentz/caddy-docker-proxy:2.8
    container_name: caddy
    restart: unless-stopped
    profiles:
      - bootstrap
      - infrastructure
    networks:
      frontend: {}
      backend:
        aliases:
          - grafana.${DOMAIN}
          - open-webui.${DOMAIN}
          - vaultwarden.${DOMAIN}
          - planka.${DOMAIN}
          - outline.${DOMAIN}
          - jupyterhub.${DOMAIN}
          - homepage.${DOMAIN}
          - seafile.${DOMAIN}
          - onlyoffice.${DOMAIN}
          - matrix.${DOMAIN}
          - sogo.${DOMAIN}
          - homeassistant.${DOMAIN}
          - litellm.${DOMAIN}
          - api.litellm.${DOMAIN}
          - auth.${DOMAIN}
          - kopia.${DOMAIN}
          - adminer.${DOMAIN}
          - pgadmin.${DOMAIN}
          - portainer.${DOMAIN}
          - mail.${DOMAIN}
          - localai.${DOMAIN}
          - browserless.${DOMAIN}
          - kfuncdb.${DOMAIN}
          - lam.${DOMAIN}
    ports:
      - "80:80"
      - "443:443"
      - "443:443/udp"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - caddy_data:/data
      - caddy_config:/config
    environment:
      - CADDY_INGRESS_NETWORKS=frontend backend
      - ACME_AGREE=true
    # Note: Healthcheck disabled; relying on container logs and reverse-proxy readiness


  test-runner:
    build:
      context: src/tests
      dockerfile: Dockerfile
    container_name: test-runner
    profiles:
      - bootstrap
      - infrastructure
    networks:
      - backend
      - frontend
    depends_on:
      caddy-docker-proxy:
        # caddy-docker-proxy has no healthcheck; wait until it's started
        condition: service_started
      authelia:
        condition: service_healthy
      localai:
        condition: service_healthy
      open-webui:
        condition: service_healthy
    volumes:
      - ./tests:/tests:ro
      - ./screenshots:/screenshots:rw
      - ./configs/grafana/caddy-ca.crt:/usr/local/share/ca-certificates/caddy-ca.crt:ro
    environment:
      - NODE_ENV=test
      - CI=true
      - LANG=en_US.UTF-8
      - LC_ALL=en_US.UTF-8
      - NODE_EXTRA_CA_CERTS=/usr/local/share/ca-certificates/caddy-ca.crt
      - TEST_USERNAME=admin
      - TEST_PASSWORD=DatamancyTest2025!
      - SCREEN_DIR=/screenshots
      - DOMAIN=${DOMAIN}
      - LOCALAI_URL=http://localai:8080
      - LITELLM_URL=http://litellm:4000
      - LITELLM_API_KEY=${LITELLM_MASTER_KEY}
    restart: "no"
    command: >
      sh -c 'set -eu &&
      update-ca-certificates 2>/dev/null || true &&
      printf "%s test-runner: starting screenshot analysis loop\n" "$(date -u +%Y-%m-%dT%H:%M:%SZ)" &&
      node /tests/screenshot-loop.js'

  ldap:
    image: osixia/openldap:1.5.0
    container_name: ldap
    restart: unless-stopped
    networks:
      - backend
    profiles:
      - bootstrap
      - infrastructure
    environment:
      - LDAP_ORGANISATION=Datamancy
      - LDAP_DOMAIN=stack.local
      - LDAP_BASE_DN=dc=stack,dc=local
      - LDAP_ADMIN_PASSWORD=${LDAP_ADMIN_PASSWORD}
      - LDAP_CONFIG_PASSWORD=${LDAP_CONFIG_PASSWORD}
      - LDAP_TLS=false
    volumes:
      - ldap_data:/var/lib/ldap
      - ldap_config:/etc/ldap/slapd.d
      - ./configs/ldap/bootstrap.ldif:/container/service/slapd/assets/config/bootstrap/ldif/custom/bootstrap.ldif:ro
    command: --copy-service --loglevel debug
    healthcheck:
      test: ["CMD", "ldapsearch", "-x", "-H", "ldap://localhost:389", "-b", "dc=stack,dc=local", "-D", "cn=admin,dc=stack,dc=local", "-w", "${LDAP_ADMIN_PASSWORD}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  ldap-account-manager:
    image: ghcr.io/ldapaccountmanager/lam:9.3
    container_name: ldap-account-manager
    restart: unless-stopped
    networks:
      - backend
      - frontend
    profiles:
      - bootstrap
      - applications
    labels:
      caddy: lam.${DOMAIN}
      caddy.forward_auth: "authelia:9091"
      caddy.forward_auth.uri: "/api/authz/forward-auth"
      caddy.forward_auth.copy_headers: "Remote-User Remote-Groups Remote-Name Remote-Email"
      caddy.reverse_proxy: "{{upstreams 80}}"
    depends_on:
      ldap:
        condition: service_healthy
      authelia:
        condition: service_healthy
    environment:
      - LDAP_DOMAIN=stack.local
      - LDAP_BASE_DN=dc=stack,dc=local
      - LDAP_USER=cn=admin,dc=stack,dc=local
      - LDAP_PASSWORD=${LDAP_ADMIN_PASSWORD}
      - LAM_PASSWORD=${LAM_PASSWORD:-lam}
      - LAM_LANG=en_US
    volumes:
      - lam_config:/var/lib/ldap-account-manager
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:80/lam/"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  redis:
    image: redis:7.4.7-alpine
    container_name: redis
    restart: unless-stopped
    networks:
      - backend
    profiles:
      - bootstrap
      - database
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  authelia:
    image: authelia/authelia:4.39.13
    container_name: authelia
    restart: unless-stopped
    networks:
      - backend
      - frontend
    profiles:
      - bootstrap
      - infrastructure
    labels:
      caddy: auth.${DOMAIN}
      caddy.reverse_proxy: "{{upstreams 9091}}"
    depends_on:
      ldap:
        condition: service_healthy
      redis:
        condition: service_healthy
    volumes:
      - ./volumes/authelia:/config
      - ${AUTHELIA_CONFIG_FILE:-./configs/authelia/configuration.bootstrap.yml}:/config/configuration.yml:ro
    command: ["--config", "/config/configuration.yml"]
    environment:
      - TZ=UTC
      - AUTHELIA_JWT_SECRET=${AUTHELIA_JWT_SECRET}
      - AUTHELIA_SESSION_SECRET=${AUTHELIA_SESSION_SECRET}
      - AUTHELIA_STORAGE_ENCRYPTION_KEY=${AUTHELIA_STORAGE_ENCRYPTION_KEY}
      - AUTHELIA_AUTHENTICATION_BACKEND_LDAP_PASSWORD=${LDAP_ADMIN_PASSWORD}
      - AUTHELIA_IDENTITY_PROVIDERS_OIDC_HMAC_SECRET=${AUTHELIA_IDENTITY_PROVIDERS_OIDC_HMAC_SECRET:-}
      - AUTHELIA_IDENTITY_PROVIDERS_OIDC_ISSUER_PRIVATE_KEY=${AUTHELIA_IDENTITY_PROVIDERS_OIDC_ISSUER_PRIVATE_KEY:-}
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9091/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  mariadb:
    image: mariadb:11.6.2
    container_name: mariadb
    restart: unless-stopped
    profiles:
      - database
    networks:
      - database
    environment:
      - MYSQL_ROOT_PASSWORD=${MARIADB_ROOT_PASSWORD}
      - MYSQL_DATABASE=datamancy
      - MYSQL_USER=datamancy
      - MYSQL_PASSWORD=${MARIADB_PASSWORD}
    volumes:
      - mariadb_data:/var/lib/mysql
      - ./configs/mariadb/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    command: --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci
    healthcheck:
      test: ["CMD", "mariadb-admin", "ping", "-h", "localhost", "-u", "root", "-p${MARIADB_ROOT_PASSWORD}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  postgres:
    image: postgres:16.11-alpine
    container_name: postgres
    restart: unless-stopped
    profiles:
      - database
    networks:
      database:
        aliases:
          - db
      backend: {}
    environment:
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_USER=postgres
      - POSTGRES_DB=postgres
      - PLANKA_DB_PASSWORD=${PLANKA_DB_PASSWORD}
      - OUTLINE_DB_PASSWORD=${OUTLINE_DB_PASSWORD}
      - SYNAPSE_DB_PASSWORD=${SYNAPSE_DB_PASSWORD}
      - MAILU_DB_PASSWORD=${MAILU_DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./configs/postgres/init-db.sh:/docker-entrypoint-initdb.d/init-db.sh:ro
      - ./configs/postgres/init-mailu-schema.sql:/docker-entrypoint-initdb.d/init-mailu-schema.sql:ro
    command: postgres -c 'max_connections=200'
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  couchdb:
    image: couchdb:3.5.1
    container_name: couchdb
    restart: unless-stopped
    profiles:
      - database
    networks:
      - database
    labels:
      caddy: couchdb.${DOMAIN}
      caddy.forward_auth: "authelia:9091"
      caddy.forward_auth.uri: "/api/authz/forward-auth"
      caddy.forward_auth.copy_headers: "Remote-User Remote-Groups Remote-Name Remote-Email"
      caddy.reverse_proxy: "{{upstreams 5984}}"
    environment:
      - COUCHDB_USER=admin
      - COUCHDB_PASSWORD=${COUCHDB_PASSWORD}
    volumes:
      - couchdb_data:/opt/couchdb/data
      - ./configs/couchdb/init-wrapper.sh:/usr/local/bin/init-wrapper.sh:ro
    entrypoint: ["/bin/bash", "/usr/local/bin/init-wrapper.sh"]
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://127.0.0.1:5984/_up | grep -q '\"status\":\"ok\"' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s


  memcached:
    image: memcached:1.6.39-alpine
    container_name: seafile-memcached
    restart: unless-stopped
    profiles:
      - database
    networks:
      - backend
    healthcheck:
      test: ["CMD-SHELL", "echo stats | nc localhost 11211"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  mariadb-seafile:
    image: mariadb:11.6.2
    container_name: mariadb-seafile
    restart: unless-stopped
    profiles:
      - database
    environment:
      - MYSQL_ROOT_PASSWORD=${MARIADB_SEAFILE_ROOT_PASSWORD}
      - MYSQL_DATABASE=seafile
      - MYSQL_USER=seafile
      - MYSQL_PASSWORD=${MARIADB_SEAFILE_PASSWORD}
    volumes:
      - seafile_mysql_data:/var/lib/mysql
    networks:
      - backend
    healthcheck:
      test: ["CMD", "mariadb-admin", "ping", "-h", "localhost", "-u", "root", "-p${MARIADB_SEAFILE_ROOT_PASSWORD}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  redis-synapse:
    image: redis:7.4.7-alpine
    container_name: redis-synapse
    restart: unless-stopped
    profiles:
      - database
    networks:
      - backend
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  mailu-front:
    image: ghcr.io/mailu/nginx:2024.06.45
    container_name: mailu-front
    restart: unless-stopped
    profiles:
      - applications
    labels:
      caddy: mail.${DOMAIN}
      caddy.reverse_proxy: "{{upstreams 80}}"
    env_file:
      - configs/mailu/mailu.env
    ports:
      - "25:25"
      - "465:465"
      - "587:587"
      - "110:110"
      - "995:995"
      - "143:143"
      - "993:993"
    volumes:
      - mailu_cert:/certs
      - mailu_dkim:/dkim
    networks:
      - backend
    depends_on:
      mailu-admin:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  mailu-redis:
    image: redis:7.4.7-alpine
    container_name: mailu-redis
    restart: unless-stopped
    profiles:
      - database
    networks:
      backend:
        aliases:
          - redis
          - mailu-redis
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  mailu-antispam:
    image: ghcr.io/mailu/rspamd:2024.06.45
    container_name: mailu-antispam
    restart: unless-stopped
    profiles:
      - applications
    env_file:
      - configs/mailu/mailu.env
    volumes:
      - mailu_filter:/var/lib/rspamd
    networks:
      - backend
    depends_on:
      mailu-admin:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "rspamc", "stat"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  mailu-antivirus:
    image: ghcr.io/mailu/clamav:2.0.43
    container_name: mailu-antivirus
    restart: unless-stopped
    profiles:
      - applications
    env_file:
      - configs/mailu/mailu.env
    networks:
      - backend
    healthcheck:
      test: ["CMD", "clamdscan", "--ping", "1"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 5m

  mailu-imap:
    image: ghcr.io/mailu/dovecot:2024.06.45
    container_name: mailu-imap
    restart: unless-stopped
    profiles:
      - applications
    env_file:
      - configs/mailu/mailu.env
    volumes:
      - mailu_data:/data
    networks:
      - backend
    depends_on:
      mailu-admin:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "doveadm", "director", "status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  mailu-smtp:
    image: ghcr.io/mailu/postfix:2024.06.45
    container_name: mailu-smtp
    restart: unless-stopped
    profiles:
      - applications
    env_file:
      - configs/mailu/mailu.env
    volumes:
      - mailu_data:/data
      - mailu_dkim:/dkim
    networks:
      - backend
    depends_on:
      mailu-admin:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "postfix", "status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  mailu-admin:
    image: ghcr.io/mailu/admin:2024.06.45
    container_name: mailu-admin
    restart: unless-stopped
    profiles:
      - applications
    env_file:
      - configs/mailu/mailu.env
    volumes:
      - mailu_data:/data
    networks:
      - backend
    depends_on:
      postgres:
        condition: service_healthy
      mailu-redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "-L", "http://localhost:8080/admin/ui"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s

  mailu-webmail:
    image: ghcr.io/mailu/webmail:2024.06.45
    container_name: mailu-webmail
    restart: unless-stopped
    profiles:
      - applications
    env_file:
      - configs/mailu/mailu.env
    volumes:
      - ./configs/mailu/nginx-override.conf:/overrides/nginx/roundcube.conf:ro
    networks:
      - backend
    depends_on:
      mailu-admin:
        condition: service_healthy
      mailu-imap:
        condition: service_healthy
      mailu-front:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  sogo:
    image: pmietlicki/sogo:v5
    container_name: sogo
    restart: unless-stopped
    profiles:
      - applications
    labels:
      caddy: sogo.${DOMAIN}
      caddy.forward_auth: "authelia:9091"
      caddy.forward_auth.uri: "/api/authz/forward-auth"
      caddy.forward_auth.copy_headers: "Remote-User Remote-Groups Remote-Name Remote-Email"
      caddy.reverse_proxy: "{{upstreams 20000}}"
    environment:
      - SOGO_WORKERS=1
      - SOGO_SIEVE_SCRIPTS_ENABLED=YES
      - SOGO_DOMAIN=${MAIL_DOMAIN}
      - SOGO_WEB_LOGIN_ENABLE_EMAIL_LOGIN=YES
      - SOGO_MAIL_SERVER=mailu-imap
      - SOGO_SOGOIMAPSERVERS=mailu-imap:143
      - SOGO_SOGOSMTPSERVERS=mailu-smtp:587
      - SOGO_SOGoSMTPAuthenticationType=PLAIN
      - SOGO_SOGoIMAPServer=imap
      - TZ=UTC
    volumes:
      - sogo_data:/var/lib/sogo
    depends_on:
      mailu-imap:
        condition: service_healthy
      mailu-smtp:
        condition: service_healthy
    networks:
      - backend
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:20000/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  dockge:
    image: louislam/dockge:1
    container_name: dockge
    restart: unless-stopped
    profiles:
      - infrastructure
    networks:
      - backend
    labels:
      caddy: dockge.${DOMAIN}
      caddy.forward_auth: "authelia:9091"
      caddy.forward_auth.uri: "/api/authz/forward-auth"
      caddy.forward_auth.copy_headers: "Remote-User Remote-Groups Remote-Name Remote-Email"
      caddy.reverse_proxy: "{{upstreams 5001}}"
    depends_on:
      - docker-proxy
    volumes:
      - dockge_data:/app/data
      - ./stacks:/opt/stacks
    environment:
      - DOCKGE_STACKS_DIR=/opt/stacks
      - DOCKER_HOST=tcp://docker-proxy:2375
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001"]
      interval: 5s
      timeout: 3s
      retries: 30
      start_period: 10s

  dockge-init:
    image: node:20.18.1-alpine
    container_name: dockge-init
    profiles:
      - infrastructure
    networks:
      - backend
    depends_on:
      dockge:
        condition: service_healthy
    volumes:
      - ./configs/dockge/init-dockge.js:/init-dockge.js:ro
    environment:
      - DOCKGE_HOST=dockge
      - DOCKGE_PORT=5001
      - DOCKGE_ADMIN_USER=${STACK_ADMIN_USER}
      - DOCKGE_ADMIN_PASSWORD=${STACK_ADMIN_PASSWORD}
    command: node /init-dockge.js
    restart: "no"

  kopia:
    image: kopia/kopia:0.18.2
    container_name: kopia
    restart: unless-stopped
    profiles:
      - infrastructure
    networks:
      - backend
    labels:
      caddy: kopia.${DOMAIN}
      caddy.forward_auth: "authelia:9091"
      caddy.forward_auth.uri: "/api/authz/forward-auth"
      caddy.forward_auth.copy_headers: "Remote-User Remote-Groups Remote-Name Remote-Email"
      caddy.reverse_proxy: "{{upstreams 51515}}"
    volumes:
      - kopia_data:/app/config
      - kopia_cache:/app/cache
      - kopia_data:/data:ro
      - kopia_repository:/repository
      - ./configs/kopia/init-kopia.sh:/init-kopia.sh:ro
    environment:
      - KOPIA_PASSWORD=${KOPIA_PASSWORD}
      - KOPIA_REPO_PATH=/repository
      - USER=kopia
    entrypoint: ["sh", "/init-kopia.sh"]
    healthcheck:
      test: ["CMD", "kopia", "repository", "status"]
      interval: 5m
      timeout: 30s
      retries: 3
      start_period: 1m

  docker-proxy:
    image: tecnativa/docker-socket-proxy:v0.4.1
    container_name: docker-proxy
    restart: unless-stopped
    profiles:
      - infrastructure
    networks:
      - backend
    environment:
      - CONTAINERS=1
      - IMAGES=1
      - NETWORKS=1
      - VOLUMES=1
      - POST=0
      - BUILD=0
      - COMMIT=0
      - CONFIGS=0
      - DISTRIBUTION=0
      - EXEC=0
      - GRPC=0
      - INFO=1
      - PLUGINS=0
      - SERVICES=0
      - SESSION=0
      - SWARM=0
      - SYSTEM=0
      - TASKS=0
      - VERSION=1
    volumes:
      - ${DOCKER_SOCKET:-/var/run/docker.sock}:/var/run/docker.sock:ro
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:2375/version"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  portainer-agent:
    image: portainer/agent:latest
    container_name: portainer-agent
    restart: unless-stopped
    profiles:
      - bootstrap
      - infrastructure
    networks:
      - backend
    volumes:
      - ${DOCKER_SOCKET:-/var/run/docker.sock}:/var/run/docker.sock
      - /var/lib/docker/volumes:/var/lib/docker/volumes
    healthcheck:
      test: ["CMD", "echo", "ok"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  portainer:
    image: portainer/portainer-ce:latest
    container_name: portainer
    restart: unless-stopped
    profiles:
      - bootstrap
      - infrastructure
    networks:
      - backend
      - frontend
    depends_on:
      portainer-agent:
        condition: service_healthy
      authelia:
        condition: service_healthy
    volumes:
      - portainer_data:/data
    labels:
      caddy: portainer.${DOMAIN}
      caddy.forward_auth: "authelia:9091"
      caddy.forward_auth.uri: "/api/authz/forward-auth"
      caddy.forward_auth.copy_headers: "Remote-User Remote-Groups Remote-Name Remote-Email"
      caddy.reverse_proxy: "{{upstreams 9000}}"
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9000/api/system/status"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 20s

  grafana:
    image: grafana/grafana:11.6.7
    container_name: grafana
    restart: unless-stopped
    profiles:
      - applications
    networks:
      - backend
      - database
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      caddy: grafana.${DOMAIN}
      caddy.reverse_proxy: "{{upstreams 3000}}"
    depends_on:
      authelia:
        condition: service_healthy
    environment:
      - GF_DEFAULT_LOCALE=en_US
      - GF_SERVER_ROOT_URL=https://grafana.${DOMAIN}
      - GF_SERVER_DOMAIN=grafana.${DOMAIN}
      - GF_AUTH_GENERIC_OAUTH_ENABLED=true
      - GF_AUTH_GENERIC_OAUTH_NAME=Authelia
      - GF_AUTH_GENERIC_OAUTH_CLIENT_ID=grafana
      - GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET=${GRAFANA_OAUTH_SECRET}
      - GF_AUTH_GENERIC_OAUTH_SCOPES=openid profile email groups
      - GF_AUTH_GENERIC_OAUTH_AUTH_URL=https://auth.${DOMAIN}/api/oidc/authorization
      - GF_AUTH_GENERIC_OAUTH_TOKEN_URL=http://authelia:9091/api/oidc/token
      - GF_AUTH_GENERIC_OAUTH_API_URL=http://authelia:9091/api/oidc/userinfo
      - GF_AUTH_GENERIC_OAUTH_LOGIN_ATTRIBUTE_PATH=email
      - GF_AUTH_GENERIC_OAUTH_EMAIL_ATTRIBUTE_PATH=email
      - GF_AUTH_GENERIC_OAUTH_GROUPS_ATTRIBUTE_PATH=groups
      - GF_AUTH_GENERIC_OAUTH_NAME_ATTRIBUTE_PATH=name
      - GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_PATH=contains(groups[*], 'admins') && 'Admin' || contains(groups[*], 'users') && 'Editor' || 'Viewer'
      - GF_AUTH_GENERIC_OAUTH_USE_PKCE=true
      - GF_AUTH_SIGNOUT_REDIRECT_URL=https://auth.${DOMAIN}/logout
      - GF_AUTH_GENERIC_OAUTH_SKIP_ORG_ROLE_SYNC=false
      - GF_AUTH_GENERIC_OAUTH_AUTO_LOGIN=false
      - GF_AUTH_GENERIC_OAUTH_ALLOW_SIGN_UP=true
      - GF_USERS_ALLOW_SIGN_UP=true
      - GF_USERS_AUTO_ASSIGN_ORG=true
      - GF_USERS_AUTO_ASSIGN_ORG_ROLE=Viewer
      - GF_AUTH_DISABLE_LOGIN_FORM=false
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./configs/grafana/grafana.ini:/etc/grafana/grafana.ini:ro
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  open-webui:
    image: ghcr.io/open-webui/open-webui:0.3.32
    container_name: open-webui
    restart: unless-stopped
    networks:
      - backend
      - frontend
    profiles:
      - bootstrap
      - applications
    dns:
      - 8.8.8.8
      - 1.1.1.1
    labels:
      caddy: open-webui.${DOMAIN}
      caddy.forward_auth: "authelia:9091"
      caddy.forward_auth.uri: "/api/authz/forward-auth"
      caddy.forward_auth.copy_headers: "Remote-User Remote-Groups Remote-Name Remote-Email"
      caddy.reverse_proxy: "{{upstreams 8080}}"
    depends_on:
      litellm:
        condition: service_healthy
      authelia:
        condition: service_healthy
    environment:
      - WEBUI_URL=${OPENWEBUI_WEBUI_URL:-https://open-webui.${DOMAIN}}
      - ENABLE_OAUTH_SIGNUP=${OPENWEBUI_ENABLE_OAUTH_SIGNUP:-false}
      - DEFAULT_USER_ROLE=${OPENWEBUI_DEFAULT_USER_ROLE:-admin}
      - ENABLE_OPENAI_API=true
      - OPENAI_API_BASE_URL=${OPENWEBUI_OPENAI_API_BASE_URL:-http://localai:8080/v1}
      - OPENAI_API_KEY=${LITELLM_MASTER_KEY}
    volumes:
      - open_webui_data:/app/backend/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  vaultwarden:
    image: vaultwarden/server:1.34.3
    container_name: vaultwarden
    restart: unless-stopped
    profiles:
      - applications
    networks:
      - backend
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      caddy: vaultwarden.${DOMAIN}
      caddy.reverse_proxy: "{{upstreams 80}}"
    depends_on:
      authelia:
        condition: service_healthy
      mailu-smtp:
        condition: service_healthy
    environment:
      - DOMAIN=https://vaultwarden.${DOMAIN}
      - SIGNUPS_ALLOWED=false
      - SIGNUPS_VERIFY=true
      - SIGNUPS_DOMAINS_WHITELIST=${MAIL_DOMAIN}
      - INVITATIONS_ALLOWED=true
      - SHOW_PASSWORD_HINT=false
      - SSO_ENABLED=true
      - SSO_ONLY=false
      - SSO_SIGNUPS_MATCH_EMAIL=true
      - SSO_AUTHORITY=https://auth.${DOMAIN}
      - SSO_ISSUER=https://auth.${DOMAIN}
      - SSO_CLIENT_ID=vaultwarden
      - SSO_CLIENT_SECRET=${VAULTWARDEN_OAUTH_SECRET}
      - SSO_SCOPES=openid email profile
      - SSO_CALLBACK_PATH=/identity/connect/oidc-signin
      - ADMIN_TOKEN=${VAULTWARDEN_ADMIN_TOKEN}
      - SMTP_HOST=mailu-smtp
      - SMTP_FROM=vaultwarden@${MAIL_DOMAIN}
      - SMTP_PORT=587
      - SMTP_SECURITY=starttls
      - SMTP_USERNAME=vaultwarden@${MAIL_DOMAIN}
      - SMTP_PASSWORD=${VAULTWARDEN_SMTP_PASSWORD}
    volumes:
      - vaultwarden_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/alive"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  planka:
    image: ghcr.io/plankanban/planka:1.26.3
    container_name: planka
    restart: unless-stopped
    profiles:
      - applications
    networks:
      - backend
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    labels:
      caddy: planka.${DOMAIN}
      caddy.reverse_proxy: "{{upstreams 1337}}"
    depends_on:
      postgres:
        condition: service_healthy
      authelia:
        condition: service_healthy
    environment:
      - BASE_URL=https://planka.${DOMAIN}
      - DATABASE_URL=postgresql://planka:${PLANKA_DB_PASSWORD}@postgres:5432/planka
      - SECRET_KEY=${PLANKA_SECRET_KEY}
      - OIDC_ISSUER=https://auth.${DOMAIN}
      - OIDC_CLIENT_ID=planka
      - OIDC_CLIENT_SECRET=${PLANKA_OAUTH_SECRET}
      - OIDC_SCOPES=openid profile email groups
      - OIDC_ADMIN_ROLES=planka-admin
      - OIDC_EMAIL_ATTRIBUTE=email
      - OIDC_NAME_ATTRIBUTE=name
      - OIDC_USERNAME_ATTRIBUTE=preferred_username
      - NODE_TLS_REJECT_UNAUTHORIZED=0
    volumes:
      - planka_data:/app/public/user-avatars
      - ./configs/grafana/caddy-ca.crt:/usr/local/share/ca-certificates/caddy-ca.crt:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:1337/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  outline:
    image: outlinewiki/outline:0.87.3
    container_name: outline
    restart: unless-stopped
    profiles:
      - applications
    networks:
      - backend
    labels:
      caddy: outline.${DOMAIN}
      caddy.reverse_proxy: "{{upstreams 3000}}"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      authelia:
        condition: service_healthy
    environment:
      - NODE_ENV=production
      - SECRET_KEY=${OUTLINE_SECRET_KEY}
      - UTILS_SECRET=${OUTLINE_UTILS_SECRET}
      - DATABASE_URL=postgres://outline:${OUTLINE_DB_PASSWORD}@postgres:5432/outline
      - PGSSLMODE=disable
      - REDIS_URL=redis://redis:6379
      - URL=https://outline.${DOMAIN}
      - PORT=3000
      - OIDC_CLIENT_ID=outline
      - OIDC_CLIENT_SECRET=${OUTLINE_OAUTH_SECRET}
      - OIDC_AUTH_URI=https://auth.${DOMAIN}/api/oidc/authorization
      - OIDC_TOKEN_URI=http://authelia:9091/api/oidc/token
      - OIDC_USERINFO_URI=http://authelia:9091/api/oidc/userinfo
      - OIDC_USERNAME_CLAIM=preferred_username
      - OIDC_DISPLAY_NAME=Authelia
      - OIDC_SCOPES=openid offline_access profile email
      - FILE_STORAGE=local
      - FILE_STORAGE_LOCAL_ROOT_DIR=/var/lib/outline/data
    volumes:
      - outline_data:/var/lib/outline/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  seafile:
    image: seafileltd/seafile-mc:13.0.12
    container_name: seafile
    restart: unless-stopped
    profiles:
      - applications
    labels:
      caddy: seafile.${DOMAIN}
      caddy.forward_auth: "authelia:9091"
      caddy.forward_auth.uri: "/api/authz/forward-auth"
      caddy.forward_auth.copy_headers: "Remote-User Remote-Groups Remote-Name Remote-Email"
      caddy.reverse_proxy: "{{upstreams 80}}"
    depends_on:
      memcached:
        condition: service_healthy
      mariadb-seafile:
        condition: service_healthy
      authelia:
        condition: service_healthy
    environment:
      - TZ=UTC
      - DB_HOST=mariadb-seafile
      - DB_ROOT_PASSWD=${MARIADB_SEAFILE_ROOT_PASSWORD}
      - SEAFILE_SERVER_HOSTNAME=seafile.${DOMAIN}
      - SEAFILE_ADMIN_EMAIL=${SEAFILE_ADMIN_EMAIL}
      - SEAFILE_ADMIN_PASSWORD=${SEAFILE_ADMIN_PASSWORD}
      - TIME_ZONE=UTC
    volumes:
      - seafile_data:/shared
      - ./configs/seafile/seahub_settings.py:/opt/seafile/conf/seahub_settings.py:ro
    networks:
      - backend
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  onlyoffice:
    image: onlyoffice/documentserver:8.2.2
    container_name: onlyoffice
    restart: unless-stopped
    profiles:
      - applications
    labels:
      caddy: onlyoffice.${DOMAIN}
      caddy.forward_auth: "authelia:9091"
      caddy.forward_auth.uri: "/api/authz/forward-auth"
      caddy.forward_auth.copy_headers: "Remote-User Remote-Groups Remote-Name Remote-Email"
      caddy.reverse_proxy: "{{upstreams 80}}"
    environment:
      - TZ=UTC
      - JWT_ENABLED=true
      - JWT_SECRET=${ONLYOFFICE_JWT_SECRET}
      - JWT_HEADER=Authorization
    volumes:
      - onlyoffice_data:/var/www/onlyoffice/Data
    networks:
      - backend
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/healthcheck"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 2m

  synapse:
    image: matrixdotorg/synapse:v1.141.0
    container_name: synapse
    restart: unless-stopped
    profiles:
      - applications
    labels:
      caddy: matrix.${DOMAIN}
      caddy.reverse_proxy: "{{upstreams 8008}}"
    depends_on:
      redis-synapse:
        condition: service_healthy
      postgres:
        condition: service_healthy
    environment:
      - SYNAPSE_SERVER_NAME=matrix.${DOMAIN}
      - SYNAPSE_REPORT_STATS=yes
      - SYNAPSE_ENABLE_REGISTRATION=${MATRIX_ENABLE_REGISTRATION:-false}
      - SYNAPSE_TRUSTED_PROXIES=172.18.0.0/16,172.21.0.0/24
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=synapse
      - POSTGRES_USER=synapse
      - POSTGRES_PASSWORD=${SYNAPSE_DB_PASSWORD}
      - REDIS_HOST=redis-synapse
    volumes:
      - synapse_data:/data
      - ./configs/synapse/homeserver.yaml:/data/homeserver.yaml:ro
      - ./configs/synapse/entrypoint.sh:/entrypoint.sh:ro
    entrypoint: ["/entrypoint.sh"]
    networks:
      - backend
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8008/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  mastodon-web:
    image: ghcr.io/mastodon/mastodon:v4.3.3
    container_name: mastodon-web
    restart: unless-stopped
    profiles:
      - applications
    networks:
      - backend
    labels:
      caddy: mastodon.${DOMAIN}
      caddy.reverse_proxy: "{{upstreams 3000}}"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      authelia:
        condition: service_healthy
      mailu-smtp:
        condition: service_healthy
    env_file:
      - configs/mastodon/mastodon.env
    command: bash -c "rm -f /mastodon/tmp/pids/server.pid; bundle exec rails s -p 3000 -b 0.0.0.0"
    volumes:
      - mastodon_data:/mastodon/public/system
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider --proxy=off localhost:3000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  mastodon-streaming:
    image: ghcr.io/mastodon/mastodon:v4.3.3
    container_name: mastodon-streaming
    restart: unless-stopped
    profiles:
      - applications
    networks:
      - backend
    depends_on:
      mastodon-web:
        condition: service_healthy
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
    env_file:
      - configs/mastodon/mastodon.env
    command: node ./streaming
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider --proxy=off localhost:4000/api/v1/streaming/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  mastodon-sidekiq:
    image: ghcr.io/mastodon/mastodon:v4.3.3
    container_name: mastodon-sidekiq
    restart: unless-stopped
    profiles:
      - applications
    networks:
      - backend
    depends_on:
      mastodon-web:
        condition: service_healthy
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
    env_file:
      - configs/mastodon/mastodon.env
    command: bundle exec sidekiq
    volumes:
      - mastodon_data:/mastodon/public/system
    healthcheck:
      test: ["CMD-SHELL", "ps aux | grep '[s]idekiq 6' || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s

  mastodon-init:
    image: docker:27-cli
    container_name: mastodon-init
    profiles:
      - applications
    networks:
      - backend
    depends_on:
      mastodon-web:
        condition: service_healthy
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./configs/mastodon/init-mastodon.sh:/init-mastodon.sh:ro
    command: sh /init-mastodon.sh
    restart: "no"

  homepage:
    image: ghcr.io/gethomepage/homepage:v1.7.0
    container_name: homepage
    restart: unless-stopped
    profiles:
      - applications
    networks:
      - backend
    labels:
      caddy: homepage.${DOMAIN}
      caddy.forward_auth: "authelia:9091"
      caddy.forward_auth.uri: "/api/authz/forward-auth"
      caddy.forward_auth.copy_headers: "Remote-User Remote-Groups Remote-Name Remote-Email"
      caddy.reverse_proxy: "{{upstreams 3000}}"
    environment:
      - PUID=1000
      - PGID=1000
      - HOMEPAGE_ALLOWED_HOSTS=homepage.${DOMAIN}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./configs/homepage/services.yaml:/app/config/services.yaml:ro
      - ./configs/homepage/settings.yaml:/app/config/settings.yaml:ro
      - ./configs/homepage/widgets.yaml:/app/config/widgets.yaml:ro
      - ./configs/homepage/bookmarks.yaml:/app/config/bookmarks.yaml:ro
      - ./configs/homepage/docker.yaml:/app/config/docker.yaml:ro
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3000/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  jupyterhub:
    build:
      context: configs/jupyterhub
      dockerfile: Dockerfile
    container_name: jupyterhub
    restart: unless-stopped
    profiles:
      - applications
    networks:
      - backend
    labels:
      caddy: jupyterhub.${DOMAIN}
      caddy.reverse_proxy: "{{upstreams 8000}}"
    depends_on:
      authelia:
        condition: service_healthy
      docker-proxy:
        condition: service_started
    environment:
      - DOMAIN=${DOMAIN}
      - JUPYTERHUB_OAUTH_SECRET=${JUPYTERHUB_OAUTH_SECRET}
      - DOCKER_HOST=tcp://docker-proxy:2375
    volumes:
      - jupyterhub_data:/srv/jupyterhub
      - ./configs/jupyterhub/jupyterhub_config.py:/srv/jupyterhub/jupyterhub_config.py:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/hub/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  homeassistant:
    image: ghcr.io/home-assistant/home-assistant:stable
    container_name: homeassistant
    restart: unless-stopped
    profiles:
      - applications
    networks:
      - backend
    labels:
      caddy: homeassistant.${DOMAIN}
      caddy.forward_auth: "authelia:9091"
      caddy.forward_auth.uri: "/api/authz/forward-auth"
      caddy.forward_auth.copy_headers: "Remote-User Remote-Groups Remote-Name Remote-Email"
      caddy.reverse_proxy: "{{upstreams 8123}}"
    depends_on:
      authelia:
        condition: service_healthy
      postgres:
        condition: service_healthy
    environment:
      - TZ=UTC
    volumes:
      - homeassistant_config:/config
      - /etc/localtime:/etc/localtime:ro
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8123/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # AI Services from docker-compose.llm.yml
  localai-init:
    image: alpine:latest
    container_name: localai-init
    volumes:
      - ./configs/localai/models:/source:ro
      - ./volumes/localai/models:/dest
    command: sh -c "cp -f /source/*.yaml /dest/ 2>/dev/null || true"
    restart: "no"
    profiles:
      - bootstrap
      - infrastructure

  localai:
    image: localai/localai:v3.7.0-aio-gpu-nvidia-cuda-12
    container_name: localai
    restart: unless-stopped
    networks:
      - backend
      - frontend
    profiles:
      - bootstrap
      - applications
    labels:
      caddy: localai.${DOMAIN}
      caddy.forward_auth: "authelia:9091"
      caddy.forward_auth.uri: "/api/authz/forward-auth"
      caddy.forward_auth.copy_headers: "Remote-User Remote-Groups Remote-Name Remote-Email"
      caddy.reverse_proxy: "{{upstreams 8080}}"
    depends_on:
      localai-init:
        condition: service_completed_successfully
      authelia:
        condition: service_healthy
    volumes:
      - localai_models:/build/models
      - ./volumes/localai/models:/models
    environment:
      HUGGINGFACEHUB_API_TOKEN: ${HUGGINGFACEHUB_API_TOKEN}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/readyz"]
      interval: 60s
      timeout: 30s
      retries: 5
      start_period: 5m

  litellm:
    image: ghcr.io/berriai/litellm:v1.74.9-stable.patch.1
    container_name: litellm
    restart: unless-stopped
    networks:
      - backend
      - frontend
    profiles:
      - bootstrap
      - applications
    labels:
      caddy: litellm.${DOMAIN}
      caddy.forward_auth: "authelia:9091"
      caddy.forward_auth.uri: "/api/authz/forward-auth"
      caddy.forward_auth.copy_headers: "Remote-User Remote-Groups Remote-Name Remote-Email"
      caddy.reverse_proxy: "{{upstreams 4000}}"
      # Public, non-interactive API endpoint without ForwardAuth.
      # Exposes api.litellm.${DOMAIN} guarded by an IP allowlist and LiteLLM's API key.
      # Allowlist is space-separated CIDRs from API_LITELLM_ALLOWLIST; default only permits private/local ranges.
      caddy_1: api.litellm.${DOMAIN}
      # Deny any request not coming from allowed IP CIDRs
      caddy_1.@notallowed.not.remote_ip: "${API_LITELLM_ALLOWLIST:-10.0.0.0/8 172.16.0.0/12 192.168.0.0/16 100.64.0.0/10 ::1 fc00::/7 fe80::/10}"
      caddy_1.respond: "@notallowed 403"
      # Allowed clients are proxied to LiteLLM upstream without SSO
      caddy_1.reverse_proxy: "{{upstreams 4000}}"
    depends_on:
      localai:
        condition: service_healthy
      authelia:
        condition: service_healthy
    volumes:
      - litellm_config:/app/config
      - ./configs/litellm/config.bootstrap.yaml:/app/config.yaml:ro
    environment:
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}
      - UI_USERNAME=admin
      - UI_PASSWORD=${LITELLM_MASTER_KEY}
    command: --config /app/config.yaml --port 4000 --num_workers 4
    healthcheck:
      test: ["CMD-SHELL", "python3 -c 'import socket; s=socket.socket(); s.settimeout(1); s.connect((\"127.0.0.1\", 4000)); s.close()' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 15s


  kfuncdb:
    build: ./src/kfuncdb
    container_name: kfuncdb
    restart: unless-stopped
    networks:
      - backend
      - frontend
    profiles:
      - bootstrap
      - applications
    user: "1000:1000"
    labels:
      caddy: kfuncdb.${DOMAIN}
      caddy.forward_auth: "authelia:9091"
      caddy.forward_auth.uri: "/api/authz/forward-auth"
      caddy.forward_auth.copy_headers: "Remote-User Remote-Groups Remote-Name Remote-Email"
      caddy.reverse_proxy: "{{upstreams 8081}}"
    depends_on:
      authelia:
        condition: service_healthy
    environment:
      - KFUNCDB_PORT=8081
      - KFUNCDB_PLUGINS_DIR=/app/plugins
      - KFUNCDB_ALLOW_CAPS=host.shell.read,host.docker.inspect
    volumes:
      - ${DOCKER_SOCKET:-/var/run/docker.sock}:/var/run/docker.sock:ro
    healthcheck:
      test: ["CMD", "wget", "-q", "-O", "/dev/null", "http://localhost:8081/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s


  ###############################################
  # Vector Stack (profile: bootstrap_vector_dbs)
  ###############################################
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    restart: unless-stopped
    networks:
      - database
      - backend
    profiles:
      - bootstrap_vector_dbs
      - database
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
    # Do not expose ports publicly; access internally via service name
    volumes:
      - qdrant_data:/qdrant/storage
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:6333/readyz"]
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 10s

  clickhouse:
    image: clickhouse/clickhouse-server:24.8
    container_name: clickhouse
    restart: unless-stopped
    networks:
      - database
    labels:
      caddy: clickhouse.${DOMAIN}
      caddy.forward_auth: "authelia:9091"
      caddy.forward_auth.uri: "/api/authz/forward-auth"
      caddy.forward_auth.copy_headers: "Remote-User Remote-Groups Remote-Name Remote-Email"
      caddy.reverse_proxy: "{{upstreams 8123}}"
    profiles:
      - bootstrap_vector_dbs
      - database
    environment:
      - CLICKHOUSE_DB=default
      - CLICKHOUSE_USER=default
      - CLICKHOUSE_PASSWORD=${CLICKHOUSE_PASSWORD}
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - ./configs/clickhouse/config.xml:/etc/clickhouse-server/config.d/config.xml:ro
      - ./configs/clickhouse/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    # Do not expose native TCP publicly; use internal networking
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8123/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  vector-bootstrap:
    image: python:3.12-alpine
    container_name: vector-bootstrap
    profiles:
      - bootstrap_vector_dbs
      - infrastructure
    restart: "no"
    networks:
      - backend
      - database
    depends_on:
      qdrant:
        condition: service_healthy
    environment:
      - QDRANT_URL=http://qdrant:6333
      - VECTOR_SIZE=${VECTOR_EMBED_SIZE:-384}
      - QDRANT_API_KEY=${QDRANT_API_KEY:-}
    volumes:
      - ./configs/vectors/collections.yaml:/configs/collections.yaml:ro
      - ./configs/vectors/bootstrap_vectors.py:/scripts/bootstrap_vectors.py:ro
    command: sh -c "python /scripts/bootstrap_vectors.py /configs/collections.yaml"

  benthos:
    image: jeffail/benthos:4.27.0
    container_name: benthos
    restart: unless-stopped
    networks:
      - backend
      - database
    profiles:
      - bootstrap_vector_dbs
      - infrastructure
    volumes:
      - ./configs/benthos/benthos.yaml:/benthos.yaml:ro
      - benthos_data:/data
    command: -c /benthos.yaml
    environment:
      - QDRANT_URL=${QDRANT_URL:-http://qdrant:6333}
      - QDRANT_API_KEY=${QDRANT_API_KEY:-}
      - LOCALAI_URL=${LOCALAI_URL:-http://localai:8080/v1}
      - CLICKHOUSE_DSN=${CLICKHOUSE_DSN:-clickhouse://default:${CLICKHOUSE_PASSWORD}@clickhouse:9000/default?dial_timeout=5s&read_timeout=10s&compress=true}
      - EMBED_MODEL=${EMBED_MODEL:-embed-small}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      qdrant:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:4195/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
