# Datamancy Stack - Rootless Docker Compose
# Provenance: Docker Compose v2 specification
# Architecture: Single front door (Traefik), path-based routing, no custom DNS

networks:
  datamancy:
    driver: bridge

services:
  # ============================================================================
  # Phase 0 - CA Generator (One-shot certificate authority)
  # Provenance: OpenSSL alpine image
  # ============================================================================
  ca-generator:
    image: alpine@sha256:6baf43584bcb78f2e5847d1de515f23499913ac9f12bdf834811a3145eb11ca1  # alpine:3.19
    container_name: ca-generator
    networks:
      - datamancy
    volumes:
      - ./certs:/certs
      - ./scripts/generate-ca.sh:/generate-ca.sh:ro
    command: /bin/sh /generate-ca.sh
    labels:
      - "datamancy.oneshot=true"
      - "datamancy.phase=0"
    profiles:
      - ca

  # ============================================================================
  # Infrastructure - Socket Proxy (Security boundary)
  # ============================================================================
  socket-proxy:
    image: tecnativa/docker-socket-proxy@sha256:dc8ec925b1360c54e6bf350602d6faac4e33c5d8d809118e4c000c0b14a4529a  # 0.1.2
    container_name: socket-proxy
    restart: unless-stopped
    networks:
      - datamancy
    volumes:
      - /run/user/1000/docker.sock:/var/run/docker.sock:ro
    environment:
      CONTAINERS: 1
      SERVICES: 1
      TASKS: 1
      NETWORKS: 1
      INFO: 1
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:2375/version"]
      interval: 10s
      timeout: 5s
      retries: 3
    profiles:
      - infra

  # ============================================================================
  # Infrastructure - Caddy (Single Front Door - Pure FOSS)
  # ============================================================================
  caddy:
    image: lucaslorentz/caddy-docker-proxy@sha256:a255f6712f0e576f90d2a1919fc930783121e0ac5c7767a562ad12c480c5ac20  # latest
    container_name: caddy
    restart: unless-stopped
    networks:
      - datamancy
    ports:
      - "80:80"
      - "443:443"
      - "2019:2019"  # Admin API
    volumes:
      - /run/user/1000/docker.sock:/var/run/docker.sock:ro
      - ./configs/caddy/Caddyfile:/etc/caddy/Caddyfile:ro
      - ./certs:/certs:ro
      - ./data/caddy:/data
    environment:
      TZ: UTC
      CADDY_DOCKER_CADDYFILE_PATH: /etc/caddy/Caddyfile
      CADDY_INGRESS_NETWORKS: datamancy
    labels:
      caddy: stack.local
      caddy.tls: internal
      # Metadata for landing page
      - "datamancy.service.name=Caddy"
      - "datamancy.service.path=/admin/"
      - "datamancy.service.description=Reverse proxy and load balancer (FOSS)"
      - "datamancy.service.category=infrastructure"
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:2019/config/"]
      interval: 10s
      timeout: 5s
      retries: 3
    depends_on:
      socket-proxy:
        condition: service_healthy
    profiles:
      - infra

  # ============================================================================
  # Infrastructure - Error Pages (Custom error handling)
  # ============================================================================
  error-pages:
    image: nginx:alpine@sha256:61e01287e546aac28a3f56839c136b31f590273f3b41187a36f46f6a03bbfe22  # nginx:alpine
    container_name: error-pages
    restart: unless-stopped
    networks:
      - datamancy
    volumes:
      - ./configs/traefik/errors:/usr/share/nginx/html:ro
      - ./configs/traefik/errors/nginx.conf:/etc/nginx/conf.d/default.conf:ro
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.error-pages.rule=Host(`stack.local`) && PathPrefix(`/_errors`)"
      - "traefik.http.routers.error-pages.entrypoints=websecure"
      - "traefik.http.routers.error-pages.tls=true"
      - "traefik.http.routers.error-pages.priority=1000"
      - "traefik.http.services.error-pages.loadbalancer.server.port=80"
      - "datamancy.service.name=Error Pages"
      - "datamancy.service.description=Custom error pages for SSO redirect (internal only)"
      - "datamancy.service.category=infrastructure"
      - "datamancy.service.internal=true"
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:80/401.html"]
      interval: 10s
      timeout: 5s
      retries: 3
    depends_on:
      traefik:
        condition: service_healthy
    profiles:
      - infra

  # ============================================================================
  # Phase 1 - Grafana (Observability UI)
  # ============================================================================
  grafana:
    image: grafana/grafana@sha256:0dc5a246ab16bb2c38a349fb588174e832b4c6c2db0981d0c3e6cd774ba66a54  # 11.0.0
    container_name: grafana
    restart: unless-stopped
    networks:
      - datamancy
    user: "${UID:-1000}:${GID:-1000}"
    volumes:
      - ./data/grafana:/var/lib/grafana
      - ./configs/grafana/provisioning:/etc/grafana/provisioning:ro
    environment:
      GF_SERVER_ROOT_URL: https://stack.local/grafana
      GF_SERVER_SERVE_FROM_SUB_PATH: "true"
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_AUTH_ANONYMOUS_ENABLED: "false"
      GF_USERS_ALLOW_SIGN_UP: "false"
      GF_LOG_MODE: console
      GF_LOG_LEVEL: info
      # OIDC Configuration via Dex
      GF_AUTH_GENERIC_OAUTH_ENABLED: "true"
      GF_AUTH_GENERIC_OAUTH_NAME: "Dex"
      GF_AUTH_GENERIC_OAUTH_CLIENT_ID: "grafana"
      GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET: "grafana-client-secret-change-me"
      GF_AUTH_GENERIC_OAUTH_SCOPES: "openid profile email groups"
      GF_AUTH_GENERIC_OAUTH_AUTH_URL: "https://stack.local/dex/auth"
      GF_AUTH_GENERIC_OAUTH_TOKEN_URL: "http://dex:5556/dex/token"
      GF_AUTH_GENERIC_OAUTH_API_URL: "http://dex:5556/dex/userinfo"
      GF_AUTH_GENERIC_OAUTH_ALLOW_SIGN_UP: "true"
      GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_PATH: "contains(groups[*], 'admins') && 'Admin' || 'Viewer'"
    labels:
      caddy: stack.local
      caddy.reverse_proxy: "{{upstreams 3000}}"
      caddy.handle_path: /grafana*
      caddy.handle_path.0_reverse_proxy: "{{upstreams 3000}}"
      caddy.tls: internal
      # Metadata for landing page
      - "datamancy.service.name=Grafana"
      - "datamancy.service.path=/grafana/"
      - "datamancy.service.description=Metrics and dashboards"
      - "datamancy.service.category=observability"
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3000/api/health"]
      interval: 10s
      timeout: 5s
      retries: 3
    depends_on:
      traefik:
        condition: service_healthy
    profiles:
      - infra

  # ============================================================================
  # Phase 1 - Browserless (Headless browser for testing)
  # ============================================================================
  browserless:
    image: browserless/chrome@sha256:efac47cfff3830d9a50b27d29f8bbb61949058ae336c823fbe9bd3c0d1debcc8  # 1.61.1-chrome-stable
    container_name: browserless
    restart: unless-stopped
    user: root
    networks:
      - datamancy
    extra_hosts:
      - "stack.local:host-gateway"
    volumes:
      - ./certs/ca.crt:/usr/local/share/ca-certificates/datamancy-ca.crt:ro
      - ./scripts/browserless-entrypoint.sh:/entrypoint.sh:ro
    entrypoint: ["/entrypoint.sh"]
    environment:
      NODE_EXTRA_CA_CERTS: /usr/local/share/ca-certificates/datamancy-ca.crt
      TOKEN: browserless-token-2024
      CONCURRENT: 3
      TIMEOUT: 30000
      PREBOOT_CHROME: "true"
    labels:
      - "traefik.enable=false"
      # Metadata for landing page (internal service, not publicly routed)
      - "datamancy.service.name=Browserless"
      - "datamancy.service.description=Headless Chrome for testing (internal only)"
      - "datamancy.service.category=testing"
      - "datamancy.service.internal=true"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/?token=browserless-token-2024"]
      interval: 10s
      timeout: 5s
      retries: 3
    depends_on:
      traefik:
        condition: service_healthy
    profiles:
      - infra

  # ============================================================================
  # Phase 1 - Homepage (Landing page)
  # ============================================================================
  homepage:
    image: ghcr.io/gethomepage/homepage@sha256:5356c97b51e3cc817bed93612b4e57b39d28048ab9e4e3b346e827160cf0923e  # v0.9.2
    container_name: homepage
    restart: unless-stopped
    networks:
      - datamancy
    extra_hosts:
      - "stack.local:host-gateway"
    volumes:
      - ./configs/homepage:/app/config:ro
      - ./data/tests:/app/tests:ro
    environment:
      TZ: UTC
      DOCKER_HOST: tcp://socket-proxy:2375
    labels:
      - "traefik.enable=true"
      # Landing at root path - catch all remaining requests (protected by forward auth)
      - "traefik.http.routers.homepage.rule=Host(`stack.local`)"
      - "traefik.http.routers.homepage.entrypoints=websecure"
      - "traefik.http.routers.homepage.tls=true"
      - "traefik.http.routers.homepage.priority=1"
      # Temporarily disable OAuth2 to allow agent testing
      # - "traefik.http.routers.homepage.middlewares=oauth2-proxy@docker,oauth2-errors@file"
      - "traefik.http.services.homepage.loadbalancer.server.port=3000"
      # Metadata
      - "datamancy.service.name=Landing"
      - "datamancy.service.path=/"
      - "datamancy.service.description=Service dashboard"
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3000/"]
      interval: 10s
      timeout: 5s
      retries: 3
    depends_on:
      traefik:
        condition: service_healthy
      socket-proxy:
        condition: service_healthy
    profiles:
      - infra

  # ============================================================================
  # Phase 2 - Prometheus (Metrics collection and alerting)
  # Provenance: https://prometheus.io/docs/prometheus/latest/installation/
  # ============================================================================
  prometheus:
    image: prom/prometheus@sha256:5ccad477d0057e62a7cd1981ffcc43785ac10c5a35522dc207466ff7e7ec845f  # v2.51.0
    container_name: prometheus
    restart: unless-stopped
    networks:
      - datamancy
    extra_hosts:
      - "stack.local:host-gateway"
    volumes:
      - ./configs/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./configs/prometheus/rules:/etc/prometheus/rules:ro
      - ./data/prometheus:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--web.route-prefix=/prometheus'
      - '--web.external-url=https://stack.local/prometheus'
    environment:
      TZ: UTC
    labels:
      - "traefik.enable=true"
      - "traefik.docker.network=datamancy_datamancy"
      # Metrics endpoint - NO AUTH (for Prometheus scraping)
      - "traefik.http.routers.prometheus-metrics.rule=Host(`stack.local`) && Path(`/prometheus/metrics`)"
      - "traefik.http.routers.prometheus-metrics.entrypoints=websecure"
      - "traefik.http.routers.prometheus-metrics.tls=true"
      - "traefik.http.routers.prometheus-metrics.priority=100"
      - "traefik.http.routers.prometheus-metrics.service=prometheus"
      # API endpoints - NO AUTH (for programmatic access)
      - "traefik.http.routers.prometheus-api.rule=Host(`stack.local`) && PathPrefix(`/prometheus/api/`)"
      - "traefik.http.routers.prometheus-api.entrypoints=websecure"
      - "traefik.http.routers.prometheus-api.tls=true"
      - "traefik.http.routers.prometheus-api.priority=90"
      - "traefik.http.routers.prometheus-api.service=prometheus"
      # UI endpoint - REQUIRES AUTH (metrics/api endpoints above are whitelisted)
      - "traefik.http.routers.prometheus.rule=Host(`stack.local`) && PathPrefix(`/prometheus`)"
      - "traefik.http.routers.prometheus.entrypoints=websecure"
      - "traefik.http.routers.prometheus.tls=true"
      - "traefik.http.routers.prometheus.priority=50"
      - "traefik.http.routers.prometheus.middlewares=authelia@docker"
      - "traefik.http.services.prometheus.loadbalancer.server.port=9090"
      # Metadata for landing page
      - "datamancy.service.name=Prometheus"
      - "datamancy.service.path=/prometheus/"
      - "datamancy.service.description=Metrics collection and time-series database"
      - "datamancy.service.category=observability"
      # Prometheus metrics exposure
      - "prometheus.io/scrape=true"
      - "prometheus.io/port=9090"
      - "prometheus.io/path=/prometheus/metrics"
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9090/prometheus/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 3
    depends_on:
      traefik:
        condition: service_healthy
    profiles:
      - infra
      - phase2

  # ============================================================================
  # Phase 2 - Alertmanager (Alert routing and notification)
  # Provenance: https://prometheus.io/docs/alerting/latest/alertmanager/
  # ============================================================================
  alertmanager:
    image: prom/alertmanager@sha256:e13b6ed5cb929eeaee733479dce55e10eb3bc2e9c4586c705a4e8da41e5eacf5  # v0.27.0
    container_name: alertmanager
    restart: unless-stopped
    networks:
      - datamancy
    extra_hosts:
      - "stack.local:host-gateway"
    volumes:
      - ./configs/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - ./data/alertmanager:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.route-prefix=/alertmanager'
      - '--web.external-url=https://stack.local/alertmanager'
    environment:
      TZ: UTC
    labels:
      - "traefik.enable=true"
      - "traefik.docker.network=datamancy_datamancy"
      # Metrics endpoint - NO AUTH
      - "traefik.http.routers.alertmanager-metrics.rule=Host(`stack.local`) && Path(`/alertmanager/metrics`)"
      - "traefik.http.routers.alertmanager-metrics.entrypoints=websecure"
      - "traefik.http.routers.alertmanager-metrics.tls=true"
      - "traefik.http.routers.alertmanager-metrics.priority=100"
      - "traefik.http.routers.alertmanager-metrics.service=alertmanager"
      # UI endpoint - REQUIRES AUTH (metrics endpoint above is whitelisted)
      - "traefik.http.routers.alertmanager.rule=Host(`stack.local`) && PathPrefix(`/alertmanager`)"
      - "traefik.http.routers.alertmanager.entrypoints=websecure"
      - "traefik.http.routers.alertmanager.tls=true"
      - "traefik.http.routers.alertmanager.priority=50"
      - "traefik.http.routers.alertmanager.middlewares=authelia@docker"
      - "traefik.http.services.alertmanager.loadbalancer.server.port=9093"
      # Metadata for landing page
      - "datamancy.service.name=Alertmanager"
      - "datamancy.service.path=/alertmanager/"
      - "datamancy.service.description=Alert routing and notification management"
      - "datamancy.service.category=observability"
      # Prometheus metrics exposure
      - "prometheus.io/scrape=true"
      - "prometheus.io/port=9093"
      - "prometheus.io/path=/alertmanager/metrics"
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9093/alertmanager/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 3
    depends_on:
      traefik:
        condition: service_healthy
    profiles:
      - infra
      - phase2

  # ============================================================================
  # Phase 2 - Loki (Log aggregation)
  # Provenance: https://grafana.com/docs/loki/latest/
  # ============================================================================
  loki:
    image: grafana/loki@sha256:6ca6e2cd3b6f45e0eb298da2920610fde63ecd8ab6c595d9c941c8559d1d9407  # 2.9.6
    container_name: loki
    restart: unless-stopped
    networks:
      - datamancy
    extra_hosts:
      - "stack.local:host-gateway"
    volumes:
      - ./configs/loki/loki.yml:/etc/loki/local-config.yaml:ro
      - ./data/loki:/loki
    command: -config.file=/etc/loki/local-config.yaml
    environment:
      TZ: UTC
    labels:
      - "traefik.enable=true"
      - "traefik.docker.network=datamancy_datamancy"
      - "traefik.http.routers.loki.rule=Host(`stack.local`) && PathPrefix(`/loki`)"
      - "traefik.http.routers.loki.entrypoints=websecure"
      - "traefik.http.routers.loki.tls=true"
      - "traefik.http.routers.loki.priority=50"
      # REQUIRES AUTH (metrics/ready endpoints whitelisted in oauth2-proxy config)
      - "traefik.http.routers.loki.middlewares=authelia@docker"
      - "traefik.http.services.loki.loadbalancer.server.port=3100"
      # Metadata for landing page
      - "datamancy.service.name=Loki"
      - "datamancy.service.path=/loki/"
      - "datamancy.service.description=Log aggregation system"
      - "datamancy.service.category=observability"
      # Prometheus metrics exposure
      - "prometheus.io/scrape=true"
      - "prometheus.io/port=3100"
      - "prometheus.io/path=/loki/metrics"
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3100/ready"]
      interval: 10s
      timeout: 5s
      retries: 3
    depends_on:
      traefik:
        condition: service_healthy
    profiles:
      - infra
      - phase2

  # ============================================================================
  # Phase 2 - Promtail (Log shipping agent)
  # Provenance: https://grafana.com/docs/loki/latest/send-data/promtail/
  # ============================================================================
  promtail:
    image: grafana/promtail@sha256:c0e57ee03512475e982893622544d76da4e3c3671a72425c670ccfc0024a4187  # 2.9.6
    container_name: promtail
    restart: unless-stopped
    networks:
      - datamancy
    volumes:
      - ./configs/promtail/promtail.yml:/etc/promtail/config.yml:ro
      - ${HOME}/.local/share/docker/containers:/var/lib/docker/containers:ro
    command: -config.file=/etc/promtail/config.yml
    environment:
      TZ: UTC
    labels:
      # Metadata for landing page (internal service, not publicly routed)
      - "datamancy.service.name=Promtail"
      - "datamancy.service.description=Log shipping agent for Loki (internal only)"
      - "datamancy.service.category=observability"
      - "datamancy.service.internal=true"
    healthcheck:
      test: ["CMD", "promtail", "-version"]
      interval: 10s
      timeout: 5s
      retries: 3
    depends_on:
      loki:
        condition: service_healthy
    profiles:
      - infra
      - phase2

  # ============================================================================
  # Phase 3 - OpenLDAP (User directory)
  # Provenance: https://github.com/osixia/docker-openldap
  # ============================================================================
  openldap:
    image: osixia/openldap@sha256:18742e9c449c9c1afe129d3f2f3ee15fb34cc43e5f940a20f3399728f41d7c28  # 1.5.0
    container_name: openldap
    restart: unless-stopped
    networks:
      - datamancy
    environment:
      LDAP_ORGANISATION: "Datamancy"
      LDAP_DOMAIN: "datamancy.local"
      LDAP_ADMIN_PASSWORD: "admin_password"
      LDAP_CONFIG_PASSWORD: "config_password"
      LDAP_READONLY_USER: "false"
      LDAP_RFC2307BIS_SCHEMA: "false"
      LDAP_BACKEND: "mdb"
      LDAP_TLS: "false"
      LDAP_REPLICATION: "false"
      KEEP_EXISTING_CONFIG: "false"
      LDAP_REMOVE_CONFIG_AFTER_SETUP: "true"
      LDAP_SSL_HELPER_PREFIX: "ldap"
    volumes:
      - ./data/ldap/database:/var/lib/ldap
      - ./data/ldap/config:/etc/ldap/slapd.d
      - ./configs/ldap:/container/service/slapd/assets/config/bootstrap/ldif/custom:ro
    command: --copy-service --loglevel debug
    labels:
      # Metadata for landing page
      - "datamancy.service.name=OpenLDAP"
      - "datamancy.service.description=LDAP user directory (internal only)"
      - "datamancy.service.category=auth"
      - "datamancy.service.internal=true"
    healthcheck:
      test: ["CMD", "ldapsearch", "-x", "-H", "ldap://localhost", "-b", "dc=datamancy,dc=local", "-D", "cn=admin,dc=datamancy,dc=local", "-w", "admin_password"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    profiles:
      - infra
      - phase3

  # ============================================================================
  # Phase 3 - Authelia (SSO and Forward Authentication)
  # Provenance: https://www.authelia.com/
  # ============================================================================
  authelia:
    image: authelia/authelia@sha256:7adc2a95b6a4be9332f6a420fdf59c7031bff203d1046ab80d8fbd66f5b1095f  # latest
    container_name: authelia
    restart: unless-stopped
    networks:
      - datamancy
    volumes:
      - ./configs/authelia/configuration.yml:/config/configuration.yml:ro
      - ./data/authelia:/config
    environment:
      TZ: UTC
      AUTHELIA_JWT_SECRET: insecure_jwt_secret_change_in_production
      AUTHELIA_SESSION_SECRET: insecure_session_secret_change_in_production
      AUTHELIA_STORAGE_ENCRYPTION_KEY: insecure_encryption_key_change_in_production
    labels:
      - "traefik.enable=true"
      - "traefik.docker.network=datamancy"
      # Authelia portal route
      - "traefik.http.routers.authelia.rule=Host(`stack.local`) && PathPrefix(`/authelia`)"
      - "traefik.http.routers.authelia.entrypoints=websecure"
      - "traefik.http.routers.authelia.tls=true"
      - "traefik.http.routers.authelia.priority=60"
      - "traefik.http.services.authelia.loadbalancer.server.port=9091"
      # Forward auth middleware
      - "traefik.http.middlewares.authelia.forwardauth.address=http://authelia:9091/api/verify?rd=https://stack.local/authelia/"
      - "traefik.http.middlewares.authelia.forwardauth.trustForwardHeader=true"
      - "traefik.http.middlewares.authelia.forwardauth.authResponseHeaders=Remote-User,Remote-Groups,Remote-Name,Remote-Email"
      # Metadata
      - "datamancy.service.name=Authelia"
      - "datamancy.service.path=/authelia/"
      - "datamancy.service.description=Single Sign-On and authentication portal"
      - "datamancy.service.category=auth"
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9091/api/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    depends_on:
      openldap:
        condition: service_healthy
      traefik:
        condition: service_healthy
    profiles:
      - infra
      - phase3

  # ============================================================================
  # Phase 3 - Dex (OIDC Provider) - DEPRECATED, replaced by Authelia
  # Provenance: https://dexidp.io/docs/
  # ============================================================================
  dex:
    image: ghcr.io/dexidp/dex@sha256:f579d00721b0d842328c43a562f50343c54b0048ef2d58d6b54e750c21fc7938  # v2.37.0
    container_name: dex
    restart: unless-stopped
    networks:
      - datamancy
    extra_hosts:
      - "stack.local:host-gateway"
    volumes:
      - ./configs/dex/config.yaml:/etc/dex/config.yaml:ro
    command: ["dex", "serve", "/etc/dex/config.yaml"]
    environment:
      TZ: UTC
    labels:
      - "traefik.enable=true"
      - "traefik.docker.network=datamancy_datamancy"
      - "traefik.http.routers.dex.rule=Host(`stack.local`) && PathPrefix(`/dex`)"
      - "traefik.http.routers.dex.entrypoints=websecure"
      - "traefik.http.routers.dex.tls=true"
      - "traefik.http.routers.dex.priority=50"
      - "traefik.http.services.dex.loadbalancer.server.port=5556"
      # Metadata for landing page
      - "datamancy.service.name=Dex"
      - "datamancy.service.path=/dex/"
      - "datamancy.service.description=OIDC provider for SSO"
      - "datamancy.service.category=auth"
      # Prometheus metrics exposure
      - "prometheus.io/scrape=true"
      - "prometheus.io/port=5558"
      - "prometheus.io/path=/metrics"
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:5556/dex/.well-known/openid-configuration"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    depends_on:
      - openldap
      - traefik
    profiles:
      - infra
      - phase3

  # ============================================================================
  # Phase 3 - Mailpit (Email testing)
  # Provenance: https://github.com/axllent/mailpit
  # ============================================================================
  mailpit:
    image: axllent/mailpit@sha256:5ce3cb2f8963bb5a48d0c6d87f8290cd9a531f9c02a63c84f8dc4de1ef541708  # v1.13
    container_name: mailpit
    restart: unless-stopped
    networks:
      - datamancy
    extra_hosts:
      - "stack.local:host-gateway"
    environment:
      TZ: UTC
      MP_WEBROOT: "/mailpit"
    volumes:
      - ./data/mailpit:/data
    labels:
      - "traefik.enable=true"
      - "traefik.docker.network=datamancy_datamancy"
      - "traefik.http.routers.mailpit.rule=Host(`stack.local`) && PathPrefix(`/mailpit`)"
      - "traefik.http.routers.mailpit.entrypoints=websecure"
      - "traefik.http.routers.mailpit.tls=true"
      - "traefik.http.routers.mailpit.priority=50"
      - "traefik.http.routers.mailpit.middlewares=oauth2-proxy@docker"
      - "traefik.http.services.mailpit.loadbalancer.server.port=8025"
      # Metadata for landing page
      - "datamancy.service.name=Mailpit"
      - "datamancy.service.path=/mailpit/"
      - "datamancy.service.description=Email testing interface"
      - "datamancy.service.category=auth"
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8025/mailpit/"]
      interval: 10s
      timeout: 5s
      retries: 3
    depends_on:
      - traefik
    profiles:
      - infra
      - phase3

  # ============================================================================
  # Phase 3 - OAuth2 Proxy (SSO for services without native OIDC)
  # Provenance: https://oauth2-proxy.github.io/oauth2-proxy/
  # ============================================================================
  oauth2-proxy:
    image: quay.io/oauth2-proxy/oauth2-proxy@sha256:dcb6ff8dd21bf3058f6a22c6fa385fa5b897a9cd3914c88a2cc2bb0a85f8065d  # v7.6.0
    container_name: oauth2-proxy
    restart: unless-stopped
    networks:
      - datamancy
    extra_hosts:
      - "stack.local:172.18.0.3"
    command:
      - --http-address=0.0.0.0:4180
      - --metrics-address=0.0.0.0:44180
      - --provider=oidc
      - --provider-display-name=Dex
      - --oidc-issuer-url=https://stack.local/dex
      - --client-id=traefik-forward-auth
      - --client-secret=traefik-forward-auth-secret-change-me
      - --cookie-secret=changethiscookiesecret32bytes000
      - --cookie-domain=stack.local
      - --cookie-secure=true
      - --cookie-name=_oauth2_proxy
      - --email-domain=*
      - --redirect-url=https://stack.local/oauth2/callback
      - --upstream=static://200
      - --skip-provider-button=true
      - --ssl-insecure-skip-verify=true
      - --oidc-email-claim=email
      - --oidc-groups-claim=groups
      - --scope=openid profile email groups
      - --reverse-proxy=true
      - --pass-authorization-header=true
      - --pass-access-token=true
      - --pass-user-headers=true
      - --set-authorization-header=true
      - --set-xauthrequest=true
      - --skip-auth-regex=^/prometheus/metrics
      - --skip-auth-regex=^/alertmanager/metrics
      - --skip-auth-regex=^/grafana/api/health
      - --skip-auth-regex=^/loki/ready
      - --skip-auth-regex=^/loki/metrics
    labels:
      - "traefik.enable=true"
      - "traefik.docker.network=datamancy_datamancy"
      # OAuth callback route
      - "traefik.http.routers.oauth2-proxy.rule=Host(`stack.local`) && PathPrefix(`/oauth2`)"
      - "traefik.http.routers.oauth2-proxy.entrypoints=websecure"
      - "traefik.http.routers.oauth2-proxy.tls=true"
      - "traefik.http.routers.oauth2-proxy.priority=60"
      - "traefik.http.routers.oauth2-proxy.service=oauth2-proxy"
      - "traefik.http.services.oauth2-proxy.loadbalancer.server.port=4180"
      # Forward auth middleware - will be applied to services
      - "traefik.http.middlewares.oauth2-proxy.forwardauth.address=http://oauth2-proxy:4180/oauth2/auth"
      - "traefik.http.middlewares.oauth2-proxy.forwardauth.trustForwardHeader=true"
      - "traefik.http.middlewares.oauth2-proxy.forwardauth.authResponseHeaders=X-Auth-Request-User,X-Auth-Request-Email"
      - "traefik.http.middlewares.oauth2-proxy.forwardauth.authResponseHeadersRegex=^X-"
      - "traefik.http.middlewares.oauth2-proxy.forwardauth.authRequestHeaders=X-Forwarded-Method,X-Forwarded-Proto,X-Forwarded-Host,X-Forwarded-Uri,X-Forwarded-For"
      # Metadata
      - "datamancy.service.name=OAuth2 Proxy"
      - "datamancy.service.description=Forward authentication for SSO"
      - "datamancy.service.category=auth"
    depends_on:
      - dex
      - traefik
    profiles:
      - infra
      - phase3

  # ============================================================================
  # Phase 1 - Change Tracker (Freshness Rule - Change detection)
  # Provenance: Alpine with git
  # ============================================================================
  change-tracker:
    image: alpine@sha256:6baf43584bcb78f2e5847d1de515f23499913ac9f12bdf834811a3145eb11ca1  # alpine:3.19
    container_name: change-tracker
    restart: "no"
    networks:
      - datamancy
    volumes:
      - .:/repo
      - ./.git:/repo/.git:ro
      - ./scripts/update-freshness-status.sh:/update-freshness-status.sh:ro
    environment:
      REPO_ROOT: /repo
      DOCKER_HOST: tcp://socket-proxy:2375
    working_dir: /repo
    command: >
      sh -c "
      apk add --no-cache git docker-cli jq bash coreutils &&
      echo 'âœ“ Change tracker initialized' &&
      /update-freshness-status.sh
      "
    labels:
      - "datamancy.service.name=Change Tracker"
      - "datamancy.service.description=Tracks file changes for Freshness Rule"
      - "datamancy.service.category=testing"
      - "datamancy.oneshot=true"
    depends_on:
      socket-proxy:
        condition: service_healthy
    profiles:
      - infra

  # ============================================================================
  # Phase 1 - Test Runner (Playwright agent for UI validation)
  # Provenance: mcr.microsoft.com/playwright
  # ============================================================================
  test-runner:
    image: mcr.microsoft.com/playwright@sha256:d518367161e599b64e4e8b83ff180be45bfe22efb78dde77fc4c2942340fe8ca  # v1.56.1-jammy
    container_name: test-runner
    restart: "no"
    networks:
      - datamancy
    volumes:
      - ./tests:/tests
      - ./data/tests:/results
      - ./data/firefox-profile:/firefox-profile
      - ./certs/ca.crt:/usr/local/share/ca-certificates/datamancy-ca.crt:ro
    environment:
      NODE_EXTRA_CA_CERTS: /usr/local/share/ca-certificates/datamancy-ca.crt
      PLAYWRIGHT_BROWSERS_PATH: /ms-playwright
      BASE_URL: https://stack.local
      CI: "true"
    working_dir: /tests
    user: root
    entrypoint: ["/tests/entrypoint.sh"]
    command: ["npx", "playwright", "test", "specs/phase1-smoke.spec.js", "--reporter=junit,list"]
    labels:
      - "datamancy.service.name=Test Runner"
      - "datamancy.service.description=Playwright UI test automation"
      - "datamancy.service.category=testing"
      - "datamancy.oneshot=true"
    depends_on:
      traefik:
        condition: service_healthy
      grafana:
        condition: service_healthy
      browserless:
        condition: service_healthy
      change-tracker:
        condition: service_completed_successfully
    profiles:
      - infra

  # ============================================================================
  # Phase 1 - Status Aggregator (Post-test freshness update)
  # ============================================================================
  status-aggregator:
    image: alpine@sha256:6baf43584bcb78f2e5847d1de515f23499913ac9f12bdf834811a3145eb11ca1  # alpine:3.19
    container_name: status-aggregator
    restart: "no"
    networks:
      - datamancy
    volumes:
      - .:/repo
      - ./.git:/repo/.git:ro
      - ./scripts/update-freshness-status.sh:/update-freshness-status.sh:ro
    environment:
      REPO_ROOT: /repo
      DOCKER_HOST: tcp://socket-proxy:2375
    working_dir: /repo
    command: >
      sh -c "
      apk add --no-cache git docker-cli jq bash coreutils &&
      echo '==> Aggregating final freshness status after tests' &&
      /update-freshness-status.sh &&
      echo '' &&
      echo '==> Final Status:' &&
      cat /repo/data/freshness-status.json | jq -r '.services | to_entries[] | \"\(.key): \(.value.status)\"'
      "
    labels:
      - "datamancy.service.name=Status Aggregator"
      - "datamancy.service.description=Aggregates freshness status post-test"
      - "datamancy.service.category=testing"
      - "datamancy.oneshot=true"
    depends_on:
      test-runner:
        condition: service_completed_successfully
      socket-proxy:
        condition: service_healthy
    profiles:
      - infra

  # ============================================================================
  # Phase 4 - MariaDB (Relational datastore)
  # Provenance: https://hub.docker.com/_/mariadb
  # ============================================================================
  mariadb:
    image: mariadb@sha256:e101f9db31916a5d4d7d594dd0dd092fb23ab4f499f1d7a7425d1afd4162c4bc  # 11.3
    container_name: mariadb
    restart: unless-stopped
    networks:
      - datamancy
    # Note: Running as root for DB initialization (standard for MariaDB)
    volumes:
      - ./data/mariadb:/var/lib/mysql
      - ./configs/mariadb/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    environment:
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD:-root_password_change_me}
      MYSQL_DATABASE: datamancy
      MYSQL_USER: datamancy
      MYSQL_PASSWORD: ${MYSQL_PASSWORD:-datamancy_password_change_me}
      TZ: UTC
    labels:
      - "traefik.enable=false"
      - "datamancy.service.name=MariaDB"
      - "datamancy.service.description=Relational database (internal only)"
      - "datamancy.service.category=database"
      - "datamancy.service.internal=true"
      # Prometheus mysqld-exporter will scrape metrics
      - "prometheus.io/scrape=true"
      - "prometheus.io/port=3306"
    healthcheck:
      test: ["CMD", "healthcheck.sh", "--connect", "--innodb_initialized"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    profiles:
      - db

  # ============================================================================
  # Phase 4 - ClickHouse (Analytics/OLAP datastore)
  # Provenance: https://hub.docker.com/r/clickhouse/clickhouse-server
  # ============================================================================
  clickhouse:
    image: clickhouse/clickhouse-server@sha256:44caeed7c81f7934c28a8ffa3a422a4ca96cf3bccc486d21311e5133ce88d49f  # 24.1
    container_name: clickhouse
    restart: unless-stopped
    networks:
      - datamancy
    user: "${UID:-1000}:${GID:-1000}"
    volumes:
      - ./data/clickhouse:/var/lib/clickhouse
      - ./configs/clickhouse/config.xml:/etc/clickhouse-server/config.d/custom.xml:ro
    environment:
      CLICKHOUSE_DB: datamancy
      CLICKHOUSE_USER: datamancy
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-clickhouse_password_change_me}
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
      TZ: UTC
    labels:
      - "traefik.enable=false"
      - "datamancy.service.name=ClickHouse"
      - "datamancy.service.description=Analytics database (internal only)"
      - "datamancy.service.category=database"
      - "datamancy.service.internal=true"
      - "prometheus.io/scrape=true"
      - "prometheus.io/port=9363"
      - "prometheus.io/path=/metrics"
    healthcheck:
      test: ["CMD", "clickhouse-client", "--query", "SELECT 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    profiles:
      - db

  # ============================================================================
  # Phase 4 - MongoDB (Document store for LibreChat)
  # Provenance: https://hub.docker.com/_/mongo
  # ============================================================================
  mongodb:
    image: mongo@sha256:a814f930db8c4514f5fe5dc3e489f58637fb7ee32a7b9bb0b7064d3274e90b8e  # 7.0
    container_name: mongodb
    restart: unless-stopped
    networks:
      - datamancy
    volumes:
      - ./data/mongodb:/data/db
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_ROOT_PASSWORD:-mongo_root_password_change_me}
      MONGO_INITDB_DATABASE: librechat
      TZ: UTC
    labels:
      - "traefik.enable=false"
      - "datamancy.service.name=MongoDB"
      - "datamancy.service.description=Document database for LibreChat (internal only)"
      - "datamancy.service.category=database"
      - "datamancy.service.internal=true"
      - "prometheus.io/scrape=true"
      - "prometheus.io/port=27017"
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    profiles:
      - db

  # ============================================================================
  # Phase 4 - Duplicati (Backup solution)
  # Provenance: https://hub.docker.com/r/duplicati/duplicati
  # ============================================================================
  duplicati:
    image: duplicati/duplicati@sha256:c928bba99739e9702d7590a9c869980d343feee537e6494b7c8323ce897f7270  # latest
    container_name: duplicati
    restart: unless-stopped
    networks:
      - datamancy
    volumes:
      - ./data/duplicati:/data
      - ./data:/backups/datamancy:ro  # Read-only access to all data dirs
      - ./backups:/backups/output  # Backup destination
    environment:
      PUID: "${UID:-1000}"
      PGID: "${GID:-1000}"
      TZ: UTC
    labels:
      - "traefik.enable=true"
      - "traefik.docker.network=datamancy_datamancy"
      - "traefik.http.routers.duplicati.rule=Host(`stack.local`) && PathPrefix(`/duplicati`)"
      - "traefik.http.routers.duplicati.entrypoints=websecure"
      - "traefik.http.routers.duplicati.tls=true"
      - "traefik.http.routers.duplicati.priority=50"
      # AUTH DISABLED FOR INITIAL SETUP - Re-enable after configuration
      # - "traefik.http.routers.duplicati.middlewares=oauth2-proxy@docker"
      - "traefik.http.services.duplicati.loadbalancer.server.port=8200"
      - "datamancy.service.name=Duplicati"
      - "datamancy.service.path=/duplicati/"
      - "datamancy.service.description=Backup management"
      - "datamancy.service.category=ops"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8200/"]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      traefik:
        condition: service_healthy
    profiles:
      - ops

