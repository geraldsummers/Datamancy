# LiteLLM bootstrap configuration
# Purpose: expose a local OpenAI-compatible endpoint at http://localhost:4000
# that proxies to the LocalAI container started by docker-compose.bootstrap.yml.

model_list:
  - model_name: hermes-2-pro-mistral-7b
    litellm_params:
      # Use OpenAI-compatible routes but point base to LocalAI
      model: openai/hermes-2-pro-mistral-7b
      api_base: http://localai:8080/v1
      api_key: unused

router_settings:
  num_retries: 1

general_settings:
  master_key: ${LITELLM_MASTER_KEY}
  # Disable database requirement for bootstrap
  database_url: null
  store_model_in_db: false

litellm_settings:
  drop_params: true
  # Disable features that require database
  success_callback: []
  failure_callback: []
