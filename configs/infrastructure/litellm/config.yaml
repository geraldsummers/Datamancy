model_list:
  # Primary chat model served by vLLM
  - model_name: hermes-2-pro-mistral-7b
    litellm_params:
      model: openai/hermes-2-pro-mistral-7b
      api_base: http://vllm:8000/v1
      api_key: unused
      max_tokens: 4096

  # Alias for coding tasks - routed to same vLLM model
  - model_name: qwen-code
    litellm_params:
      model: openai/hermes-2-pro-mistral-7b
      api_base: http://vllm:8000/v1
      api_key: unused
      max_tokens: 4096

  # Simple router alias - also routed to main model
  - model_name: router
    litellm_params:
      model: openai/hermes-2-pro-mistral-7b
      api_base: http://vllm:8000/v1
      api_key: unused
      max_tokens: 2048

litellm_settings:
  drop_params: true
  success_callback: []
  failure_callback: []

general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: os.environ/DATABASE_URL
  # Internal proxy - no SSO needed, auth via master key only
