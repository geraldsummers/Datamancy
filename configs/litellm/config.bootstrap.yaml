# LiteLLM bootstrap configuration
# Purpose: expose a local OpenAI-compatible endpoint at http://localhost:4000
# that proxies to the LocalAI container started by docker-compose.bootstrap.yml.

model_list:
  - model_name: llama3
    litellm_params:
      # Use OpenAI-compatible routes but point base to LocalAI
      model: openai/llama3
      api_base: http://localai:8080/v1
      api_key: unused

router_settings:
  num_retries: 1

general_settings:
  master_key: ${LITELLM_MASTER_KEY}

litellm_settings:
  drop_params: true
