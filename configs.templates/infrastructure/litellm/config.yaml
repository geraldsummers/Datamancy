model_list:
  # Qwen 2.5 7B Instruct (AWQ quantized) - main reasoning model
  - model_name: qwen2.5-7b-instruct
    litellm_params:
      model: openai/qwen2.5-7b-instruct
      api_base: http://vllm-7b:8000/v1
      api_key: unused
      # max_tokens removed - let client specify, default to reasonable value
      # Model supports 32K context total (prompt + completion)
    supports_function_calling: true

  # Qwen 2.5 0.5B Instruct - fast model for simple operations
  # DISABLED: RTX 3060 12GB doesn't have enough VRAM for concurrent models
  #- model_name: qwen2.5-0.5b-instruct
  #  litellm_params:
  #    model: openai/qwen2.5-0.5b-instruct
  #    api_base: http://vllm-0.5b:8000/v1
  #    api_key: unused
  #    max_tokens: 8192
  #  supports_function_calling: true

  # BGE Base embeddings for RAG
  - model_name: bge-base-en-v1.5
    litellm_params:
      model: huggingface/BAAI/bge-base-en-v1.5
      api_base: http://embedding-service:8080
      api_key: unused

litellm_settings:
  drop_params: true
  success_callback: []
  failure_callback: []

general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: os.environ/DATABASE_URL
  # Internal proxy - no SSO needed, auth via master key only
