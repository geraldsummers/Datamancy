# Auto-generated merged docker-compose.yml
# Generated by build-datamancy.main.kts at 2026-01-15T02:52:23.699660464Z

services:
  caddy:
    image: caddy:2.10.2
    container_name: caddy
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
      - "443:443/udp"
    networks:
      caddy:
        aliases:
          - www.${DOMAIN}
          - grafana.${DOMAIN}
          - open-webui.${DOMAIN}
          - vaultwarden.${DOMAIN}
          - app.vaultwarden.${DOMAIN}
          - planka.${DOMAIN}
          - bookstack.${DOMAIN}
          - jupyterhub.${DOMAIN}
          - homepage.${DOMAIN}
          - seafile.${DOMAIN}
          - api.seafile.${DOMAIN}
          - onlyoffice.${DOMAIN}
          - matrix.${DOMAIN}
          - api.matrix.${DOMAIN}
          - element.${DOMAIN}
          - homeassistant.${DOMAIN}
          - api.homeassistant.${DOMAIN}
          - litellm.${DOMAIN}
          - api.litellm.${DOMAIN}
          - auth.${DOMAIN}
          - kopia.${DOMAIN}
          - mail.${DOMAIN}
          - agent-tool-server.${DOMAIN}
          - lam.${DOMAIN}
          - forgejo.${DOMAIN}
          - qbittorrent.${DOMAIN}
      authelia:
        aliases:
          - www.${DOMAIN}
          - grafana.${DOMAIN}
          - open-webui.${DOMAIN}
          - vaultwarden.${DOMAIN}
          - app.vaultwarden.${DOMAIN}
          - planka.${DOMAIN}
          - bookstack.${DOMAIN}
          - jupyterhub.${DOMAIN}
          - homepage.${DOMAIN}
          - seafile.${DOMAIN}
          - api.seafile.${DOMAIN}
          - onlyoffice.${DOMAIN}
          - matrix.${DOMAIN}
          - api.matrix.${DOMAIN}
          - element.${DOMAIN}
          - homeassistant.${DOMAIN}
          - api.homeassistant.${DOMAIN}
          - litellm.${DOMAIN}
          - api.litellm.${DOMAIN}
          - auth.${DOMAIN}
          - kopia.${DOMAIN}
          - mail.${DOMAIN}
          - agent-tool-server.${DOMAIN}
          - lam.${DOMAIN}
          - forgejo.${DOMAIN}
          - qbittorrent.${DOMAIN}
      postgres:
        aliases:
          - www.${DOMAIN}
          - grafana.${DOMAIN}
          - open-webui.${DOMAIN}
          - vaultwarden.${DOMAIN}
          - app.vaultwarden.${DOMAIN}
          - planka.${DOMAIN}
          - bookstack.${DOMAIN}
          - jupyterhub.${DOMAIN}
          - homepage.${DOMAIN}
          - seafile.${DOMAIN}
          - api.seafile.${DOMAIN}
          - onlyoffice.${DOMAIN}
          - matrix.${DOMAIN}
          - api.matrix.${DOMAIN}
          - element.${DOMAIN}
          - homeassistant.${DOMAIN}
          - api.homeassistant.${DOMAIN}
          - litellm.${DOMAIN}
          - api.litellm.${DOMAIN}
          - auth.${DOMAIN}
          - kopia.${DOMAIN}
          - mail.${DOMAIN}
          - agent-tool-server.${DOMAIN}
          - lam.${DOMAIN}
          - forgejo.${DOMAIN}
          - qbittorrent.${DOMAIN}
      mariadb:
        aliases:
          - www.${DOMAIN}
          - grafana.${DOMAIN}
          - open-webui.${DOMAIN}
          - vaultwarden.${DOMAIN}
          - app.vaultwarden.${DOMAIN}
          - planka.${DOMAIN}
          - bookstack.${DOMAIN}
          - jupyterhub.${DOMAIN}
          - homepage.${DOMAIN}
          - seafile.${DOMAIN}
          - api.seafile.${DOMAIN}
          - onlyoffice.${DOMAIN}
          - matrix.${DOMAIN}
          - api.matrix.${DOMAIN}
          - element.${DOMAIN}
          - homeassistant.${DOMAIN}
          - api.homeassistant.${DOMAIN}
          - litellm.${DOMAIN}
          - api.litellm.${DOMAIN}
          - auth.${DOMAIN}
          - kopia.${DOMAIN}
          - mail.${DOMAIN}
          - agent-tool-server.${DOMAIN}
          - lam.${DOMAIN}
          - forgejo.${DOMAIN}
          - qbittorrent.${DOMAIN}
      clickhouse:
        aliases:
          - www.${DOMAIN}
          - grafana.${DOMAIN}
          - open-webui.${DOMAIN}
          - vaultwarden.${DOMAIN}
          - app.vaultwarden.${DOMAIN}
          - planka.${DOMAIN}
          - bookstack.${DOMAIN}
          - jupyterhub.${DOMAIN}
          - homepage.${DOMAIN}
          - seafile.${DOMAIN}
          - api.seafile.${DOMAIN}
          - onlyoffice.${DOMAIN}
          - matrix.${DOMAIN}
          - api.matrix.${DOMAIN}
          - element.${DOMAIN}
          - homeassistant.${DOMAIN}
          - api.homeassistant.${DOMAIN}
          - litellm.${DOMAIN}
          - api.litellm.${DOMAIN}
          - auth.${DOMAIN}
          - kopia.${DOMAIN}
          - mail.${DOMAIN}
          - agent-tool-server.${DOMAIN}
          - lam.${DOMAIN}
          - forgejo.${DOMAIN}
          - qbittorrent.${DOMAIN}
      qdrant:
        aliases:
          - www.${DOMAIN}
          - grafana.${DOMAIN}
          - open-webui.${DOMAIN}
          - vaultwarden.${DOMAIN}
          - app.vaultwarden.${DOMAIN}
          - planka.${DOMAIN}
          - bookstack.${DOMAIN}
          - jupyterhub.${DOMAIN}
          - homepage.${DOMAIN}
          - seafile.${DOMAIN}
          - api.seafile.${DOMAIN}
          - onlyoffice.${DOMAIN}
          - matrix.${DOMAIN}
          - api.matrix.${DOMAIN}
          - element.${DOMAIN}
          - homeassistant.${DOMAIN}
          - api.homeassistant.${DOMAIN}
          - litellm.${DOMAIN}
          - api.litellm.${DOMAIN}
          - auth.${DOMAIN}
          - kopia.${DOMAIN}
          - mail.${DOMAIN}
          - agent-tool-server.${DOMAIN}
          - lam.${DOMAIN}
          - forgejo.${DOMAIN}
          - qbittorrent.${DOMAIN}
      ldap:
        aliases:
          - www.${DOMAIN}
          - grafana.${DOMAIN}
          - open-webui.${DOMAIN}
          - vaultwarden.${DOMAIN}
          - app.vaultwarden.${DOMAIN}
          - planka.${DOMAIN}
          - bookstack.${DOMAIN}
          - jupyterhub.${DOMAIN}
          - homepage.${DOMAIN}
          - seafile.${DOMAIN}
          - api.seafile.${DOMAIN}
          - onlyoffice.${DOMAIN}
          - matrix.${DOMAIN}
          - api.matrix.${DOMAIN}
          - element.${DOMAIN}
          - homeassistant.${DOMAIN}
          - api.homeassistant.${DOMAIN}
          - litellm.${DOMAIN}
          - api.litellm.${DOMAIN}
          - auth.${DOMAIN}
          - kopia.${DOMAIN}
          - mail.${DOMAIN}
          - agent-tool-server.${DOMAIN}
          - lam.${DOMAIN}
          - forgejo.${DOMAIN}
          - qbittorrent.${DOMAIN}
      litellm:
        aliases:
          - www.${DOMAIN}
          - grafana.${DOMAIN}
          - open-webui.${DOMAIN}
          - vaultwarden.${DOMAIN}
          - app.vaultwarden.${DOMAIN}
          - planka.${DOMAIN}
          - bookstack.${DOMAIN}
          - jupyterhub.${DOMAIN}
          - homepage.${DOMAIN}
          - seafile.${DOMAIN}
          - api.seafile.${DOMAIN}
          - onlyoffice.${DOMAIN}
          - matrix.${DOMAIN}
          - api.matrix.${DOMAIN}
          - element.${DOMAIN}
          - homeassistant.${DOMAIN}
          - api.homeassistant.${DOMAIN}
          - litellm.${DOMAIN}
          - api.litellm.${DOMAIN}
          - auth.${DOMAIN}
          - kopia.${DOMAIN}
          - mail.${DOMAIN}
          - agent-tool-server.${DOMAIN}
          - lam.${DOMAIN}
          - forgejo.${DOMAIN}
          - qbittorrent.${DOMAIN}
      ai:
        aliases:
          - www.${DOMAIN}
          - grafana.${DOMAIN}
          - open-webui.${DOMAIN}
          - vaultwarden.${DOMAIN}
          - app.vaultwarden.${DOMAIN}
          - planka.${DOMAIN}
          - bookstack.${DOMAIN}
          - jupyterhub.${DOMAIN}
          - homepage.${DOMAIN}
          - seafile.${DOMAIN}
          - api.seafile.${DOMAIN}
          - onlyoffice.${DOMAIN}
          - matrix.${DOMAIN}
          - api.matrix.${DOMAIN}
          - element.${DOMAIN}
          - homeassistant.${DOMAIN}
          - api.homeassistant.${DOMAIN}
          - litellm.${DOMAIN}
          - api.litellm.${DOMAIN}
          - auth.${DOMAIN}
          - kopia.${DOMAIN}
          - mail.${DOMAIN}
          - agent-tool-server.${DOMAIN}
          - lam.${DOMAIN}
          - forgejo.${DOMAIN}
          - qbittorrent.${DOMAIN}
      ai-gateway:
        aliases:
          - www.${DOMAIN}
          - grafana.${DOMAIN}
          - open-webui.${DOMAIN}
          - vaultwarden.${DOMAIN}
          - app.vaultwarden.${DOMAIN}
          - planka.${DOMAIN}
          - bookstack.${DOMAIN}
          - jupyterhub.${DOMAIN}
          - homepage.${DOMAIN}
          - seafile.${DOMAIN}
          - api.seafile.${DOMAIN}
          - onlyoffice.${DOMAIN}
          - matrix.${DOMAIN}
          - api.matrix.${DOMAIN}
          - element.${DOMAIN}
          - homeassistant.${DOMAIN}
          - api.homeassistant.${DOMAIN}
          - litellm.${DOMAIN}
          - api.litellm.${DOMAIN}
          - auth.${DOMAIN}
          - kopia.${DOMAIN}
          - mail.${DOMAIN}
          - agent-tool-server.${DOMAIN}
          - lam.${DOMAIN}
          - forgejo.${DOMAIN}
          - qbittorrent.${DOMAIN}
      mailserver:
        aliases:
          - www.${DOMAIN}
          - grafana.${DOMAIN}
          - open-webui.${DOMAIN}
          - vaultwarden.${DOMAIN}
          - app.vaultwarden.${DOMAIN}
          - planka.${DOMAIN}
          - bookstack.${DOMAIN}
          - jupyterhub.${DOMAIN}
          - homepage.${DOMAIN}
          - seafile.${DOMAIN}
          - api.seafile.${DOMAIN}
          - onlyoffice.${DOMAIN}
          - matrix.${DOMAIN}
          - api.matrix.${DOMAIN}
          - element.${DOMAIN}
          - homeassistant.${DOMAIN}
          - api.homeassistant.${DOMAIN}
          - litellm.${DOMAIN}
          - api.litellm.${DOMAIN}
          - auth.${DOMAIN}
          - kopia.${DOMAIN}
          - mail.${DOMAIN}
          - agent-tool-server.${DOMAIN}
          - lam.${DOMAIN}
          - forgejo.${DOMAIN}
          - qbittorrent.${DOMAIN}
      memcached:
        aliases:
          - www.${DOMAIN}
          - grafana.${DOMAIN}
          - open-webui.${DOMAIN}
          - vaultwarden.${DOMAIN}
          - app.vaultwarden.${DOMAIN}
          - planka.${DOMAIN}
          - bookstack.${DOMAIN}
          - jupyterhub.${DOMAIN}
          - homepage.${DOMAIN}
          - seafile.${DOMAIN}
          - api.seafile.${DOMAIN}
          - onlyoffice.${DOMAIN}
          - matrix.${DOMAIN}
          - api.matrix.${DOMAIN}
          - element.${DOMAIN}
          - homeassistant.${DOMAIN}
          - api.homeassistant.${DOMAIN}
          - litellm.${DOMAIN}
          - api.litellm.${DOMAIN}
          - auth.${DOMAIN}
          - kopia.${DOMAIN}
          - mail.${DOMAIN}
          - agent-tool-server.${DOMAIN}
          - lam.${DOMAIN}
          - forgejo.${DOMAIN}
          - qbittorrent.${DOMAIN}
      matrix-internal:
        aliases:
          - www.${DOMAIN}
          - grafana.${DOMAIN}
          - open-webui.${DOMAIN}
          - vaultwarden.${DOMAIN}
          - app.vaultwarden.${DOMAIN}
          - planka.${DOMAIN}
          - bookstack.${DOMAIN}
          - jupyterhub.${DOMAIN}
          - homepage.${DOMAIN}
          - seafile.${DOMAIN}
          - api.seafile.${DOMAIN}
          - onlyoffice.${DOMAIN}
          - matrix.${DOMAIN}
          - api.matrix.${DOMAIN}
          - element.${DOMAIN}
          - homeassistant.${DOMAIN}
          - api.homeassistant.${DOMAIN}
          - litellm.${DOMAIN}
          - api.litellm.${DOMAIN}
          - auth.${DOMAIN}
          - kopia.${DOMAIN}
          - mail.${DOMAIN}
          - agent-tool-server.${DOMAIN}
          - lam.${DOMAIN}
          - forgejo.${DOMAIN}
          - qbittorrent.${DOMAIN}
      mastodon-internal:
        aliases:
          - www.${DOMAIN}
          - grafana.${DOMAIN}
          - open-webui.${DOMAIN}
          - vaultwarden.${DOMAIN}
          - app.vaultwarden.${DOMAIN}
          - planka.${DOMAIN}
          - bookstack.${DOMAIN}
          - jupyterhub.${DOMAIN}
          - homepage.${DOMAIN}
          - seafile.${DOMAIN}
          - api.seafile.${DOMAIN}
          - onlyoffice.${DOMAIN}
          - matrix.${DOMAIN}
          - api.matrix.${DOMAIN}
          - element.${DOMAIN}
          - homeassistant.${DOMAIN}
          - api.homeassistant.${DOMAIN}
          - litellm.${DOMAIN}
          - api.litellm.${DOMAIN}
          - auth.${DOMAIN}
          - kopia.${DOMAIN}
          - mail.${DOMAIN}
          - agent-tool-server.${DOMAIN}
          - lam.${DOMAIN}
          - forgejo.${DOMAIN}
          - qbittorrent.${DOMAIN}
      monitoring:
        aliases:
          - www.${DOMAIN}
          - grafana.${DOMAIN}
          - open-webui.${DOMAIN}
          - vaultwarden.${DOMAIN}
          - app.vaultwarden.${DOMAIN}
          - planka.${DOMAIN}
          - bookstack.${DOMAIN}
          - jupyterhub.${DOMAIN}
          - homepage.${DOMAIN}
          - seafile.${DOMAIN}
          - api.seafile.${DOMAIN}
          - onlyoffice.${DOMAIN}
          - matrix.${DOMAIN}
          - api.matrix.${DOMAIN}
          - element.${DOMAIN}
          - homeassistant.${DOMAIN}
          - api.homeassistant.${DOMAIN}
          - litellm.${DOMAIN}
          - api.litellm.${DOMAIN}
          - auth.${DOMAIN}
          - kopia.${DOMAIN}
          - mail.${DOMAIN}
          - agent-tool-server.${DOMAIN}
          - lam.${DOMAIN}
          - forgejo.${DOMAIN}
          - qbittorrent.${DOMAIN}
    environment:
      DOMAIN: ${DOMAIN}
      STACK_ADMIN_EMAIL: ${STACK_ADMIN_EMAIL}
      API_LITELLM_ALLOWLIST: ${API_LITELLM_ALLOWLIST}
      MAIL_DOMAIN: ${MAIL_DOMAIN}
    volumes:
      - caddy_data:/data
      - caddy_config:/config
      - ./configs/infrastructure/caddy/Caddyfile:/etc/caddy/Caddyfile:ro
      - ./configs/applications/vaultwarden/index.html:/srv/vaultwarden/index.html:ro
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://127.0.0.1:2019/config/"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
  ldap:
    image: osixia/openldap:1.5.0
    container_name: ldap
    restart: unless-stopped
    networks:
      - ldap
    environment:
      LDAP_ORGANISATION: Datamancy
      LDAP_DOMAIN: stack.local
      LDAP_BASE_DN: dc=stack,dc=local
      LDAP_ADMIN_PASSWORD: ${LDAP_ADMIN_PASSWORD}
      LDAP_CONFIG_PASSWORD: ${LDAP_ADMIN_PASSWORD}
      LDAP_TLS: false
      LDAP_MEMBEROF_OVERLAY: true
      LDAP_MEMBEROF_GROUP_OC: groupOfNames
      LDAP_MEMBEROF_MEMBER_AD: member
    volumes:
      - ldap_data:/var/lib/ldap
      - ldap_config:/etc/ldap/slapd.d
      - ./configs/infrastructure/ldap/bootstrap_ldap.ldif:/container/service/slapd/assets/config/bootstrap/ldif/custom/bootstrap_ldap.ldif:ro
    command: --copy-service --loglevel info
    healthcheck:
      test: ["CMD-SHELL", "ldapsearch -x -H ldap://localhost:389 -b '' -s base '(objectclass=*)' namingContexts >/dev/null 2>&1"]
      interval: 15s
      timeout: 10s
      retries: 8
      start_period: 180s
  valkey:
    image: valkey/valkey:8.1.5
    container_name: valkey
    restart: unless-stopped
    networks:
      - valkey
    volumes:
      - redis_data:/data
    command: valkey-server --appendonly yes --dir /data --databases 16
    healthcheck:
      test: ["CMD", "valkey-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 5s
  authelia:
    image: authelia/authelia:4.39.15
    container_name: authelia
    restart: unless-stopped
    networks:
      authelia:
        aliases:
          - auth.${DOMAIN}
      caddy:
        aliases:
          - auth.${DOMAIN}
      ldap:
        aliases:
          - auth.${DOMAIN}
      postgres:
        aliases:
          - auth.${DOMAIN}
      valkey:
        aliases:
          - auth.${DOMAIN}
    depends_on:
      ldap:
        condition: service_started
      postgres:
        condition: service_started
      valkey:
        condition: service_started
    environment:
      TZ: UTC
      DOMAIN: ${DOMAIN}
      AUTHELIA_JWT_SECRET: ${AUTHELIA_JWT_SECRET}
      AUTHELIA_SESSION_SECRET: ${AUTHELIA_SESSION_SECRET}
      AUTHELIA_STORAGE_ENCRYPTION_KEY: ${AUTHELIA_STORAGE_ENCRYPTION_KEY}
      AUTHELIA_AUTHENTICATION_BACKEND_LDAP_PASSWORD: ${LDAP_ADMIN_PASSWORD}
      AUTHELIA_IDENTITY_PROVIDERS_OIDC_HMAC_SECRET: ${AUTHELIA_OIDC_HMAC_SECRET}
      AUTHELIA_IDENTITY_PROVIDERS_OIDC_ISSUER_PRIVATE_KEY: ${AUTHELIA_IDENTITY_PROVIDERS_OIDC_ISSUER_PRIVATE_KEY}
      AUTHELIA_STORAGE_POSTGRES_PASSWORD: ${AUTHELIA_DB_PASSWORD}
      # OIDC client secrets (Grafana, HomeAssistant, JupyterHub use forward_auth - no OIDC)
      PGADMIN_OAUTH_SECRET: ${PGADMIN_OAUTH_SECRET}
      OPENWEBUI_OAUTH_SECRET: ${OPENWEBUI_OAUTH_SECRET}
      NEXTCLOUD_OAUTH_SECRET: ${NEXTCLOUD_OAUTH_SECRET}
      DIM_OAUTH_SECRET: ${DIM_OAUTH_SECRET}
      PLANKA_OAUTH_SECRET: ${PLANKA_OAUTH_SECRET}
      VAULTWARDEN_OAUTH_SECRET: ${VAULTWARDEN_OAUTH_SECRET}
      MASTODON_OIDC_SECRET: ${MASTODON_OIDC_SECRET}
      BOOKSTACK_OAUTH_SECRET: ${BOOKSTACK_OAUTH_SECRET}
      FORGEJO_OAUTH_SECRET: ${FORGEJO_OAUTH_SECRET}
      MATRIX_OAUTH_SECRET: ${MATRIX_OAUTH_SECRET}
    volumes:
      - ./authelia:/config
      - ./configs/applications/authelia/configuration.yml:/config/configuration.yml:ro
      - ./configs/applications/authelia/entrypoint.sh:/entrypoint.sh:ro
    command:
      - --config
      - /config/configuration.yml
    entrypoint:
      - /entrypoint.sh
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9091/api/health"]
      interval: 15s
      timeout: 10s
      retries: 3
      start_period: 30s
  mailserver:
    image: ghcr.io/docker-mailserver/docker-mailserver:14.0.0
    container_name: mailserver
    restart: unless-stopped
    ports:
      - "25:25"
      - "465:465"
      - "587:587"
      - "143:143"
      - "993:993"
    networks:
      mailserver:
        aliases:
          - mail.${DOMAIN}
      caddy:
        aliases:
          - mail.${DOMAIN}
      ldap:
        aliases:
          - mail.${DOMAIN}
    depends_on:
      ldap:
        condition: service_healthy
      caddy:
        condition: service_healthy
    environment:
      ENABLE_RSPAMD: 1
      ENABLE_CLAMAV: 1
      ENABLE_FAIL2BAN: 1
      ENABLE_POSTGREY: 0
      ENABLE_SPAMASSASSIN: 0
      SPOOF_PROTECTION: 0
      ENABLE_SRS: 0
      PERMIT_DOCKER: network
      SSL_TYPE: manual
      OVERRIDE_HOSTNAME: mail.${DOMAIN}
      DOMAIN: ${DOMAIN}
      POSTMASTER_ADDRESS: postmaster@${MAIL_DOMAIN}
      ACCOUNT_PROVISIONER: LDAP
      LDAP_SERVER_HOST: ldap://ldap:389
      LDAP_BIND_DN: cn=admin,dc=stack,dc=local
      LDAP_BIND_PW: ${LDAP_ADMIN_PASSWORD}
      LDAP_SEARCH_BASE: ou=users,dc=stack,dc=local
      LDAP_QUERY_FILTER_USER: (&(objectClass=inetOrgPerson)(|(mail=%s)(uid=%s)))
      LDAP_QUERY_FILTER_GROUP: (&(objectClass=groupOfNames)(member=%s))
      LDAP_QUERY_FILTER_ALIAS: (&(objectClass=inetOrgPerson)(|(mail=%s)(uid=%s)))
      DOVECOT_MAILBOX_FORMAT: maildir
      DOVECOT_USER_FILTER: (&(objectClass=inetOrgPerson)(mail=%u))
      DOVECOT_PASS_FILTER: (&(objectClass=inetOrgPerson)(mail=%u))
      DOVECOT_PASS_ATTRS: mail=user,userPassword=password
      DOVECOT_USER_ATTRS: =home=/var/mail/%Ln,=mail=maildir:~/Maildir
      ENABLE_QUOTAS: 1
      POSTFIX_MESSAGE_SIZE_LIMIT: 50000000
      # SSL paths will be auto-detected by find-certs.sh entrypoint
      # Supports both local_certs (dev) and Let's Encrypt (prod)
    dns:
      - 8.8.8.8
      - 8.8.4.4
    entrypoint: ["/bin/bash", "/tmp/docker-mailserver/find-certs.sh"]
    volumes:
      - mailserver_data:/var/mail/
      - mailserver_state:/var/mail-state/
      - mailserver_logs:/var/log/mail/
      - mailserver_config:/tmp/docker-mailserver/
      - /etc/localtime:/etc/localtime:ro
      - caddy_data:/caddy-certs:ro
      - ./configs/applications/mailserver/find-certs.sh:/tmp/docker-mailserver/find-certs.sh:ro
      - ./configs/applications/mailserver/dovecot-master-users.conf:/tmp/docker-mailserver/dovecot-master-users.conf:ro
      - ./configs/applications/mailserver/local.conf:/etc/dovecot/local.conf:ro
    healthcheck:
      test: ["CMD-SHELL", "ss -ltn | grep -E ':25|:587|:993|:143' && supervisorctl status | grep -E 'postfix.*RUNNING' && supervisorctl status | grep -E 'dovecot.*RUNNING'"]
      interval: 60s
      timeout: 10s
      retries: 5
      start_period: 180s
  ldap-account-manager:
    image: ghcr.io/ldapaccountmanager/lam:9.4
    container_name: ldap-account-manager
    restart: unless-stopped
    networks:
      caddy:
        aliases:
          - lam.${DOMAIN}
      ldap:
        aliases:
          - lam.${DOMAIN}
    depends_on:
      ldap:
        condition: service_started
    environment:
      LDAP_DOMAIN: stack.local
      LDAP_BASE_DN: dc=stack,dc=local
      LDAP_USER: cn=admin,dc=stack,dc=local
      LDAP_PASSWORD: ${LDAP_ADMIN_PASSWORD}
      LAM_PASSWORD: ${LDAP_ADMIN_PASSWORD}
      LAM_LANG: en_US
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:80/lam/templates/login.php"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
  seafile-memcached:
    image: memcached:1.6.40
    container_name: seafile-memcached
    restart: unless-stopped
    networks:
      - memcached
    healthcheck:
      test: ["CMD-SHELL", "timeout 5 bash -c '</dev/tcp/127.0.0.1/11211' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
  docker-proxy:
    image: tecnativa/docker-socket-proxy:v0.4.2
    container_name: docker-proxy
    restart: unless-stopped
    networks:
      - docker-proxy
    environment:
      CONTAINERS: 1
      IMAGES: 1
      NETWORKS: 1
      VOLUMES: 1
      POST: 1
      DELETE: 1
      INFO: 1
      VERSION: 1
      BUILD: 0
      COMMIT: 0
      EXEC: 0
    volumes:
      - ${DOCKER_SOCKET:-/var/run/docker.sock}:/var/run/docker.sock:ro
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:2375/version"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
  kopia:
    image: kopia/kopia:0.22.3
    container_name: kopia
    restart: unless-stopped
    networks:
      caddy:
        aliases:
          - kopia.${DOMAIN}
    environment:
      KOPIA_PASSWORD: ${STACK_ADMIN_PASSWORD}
      KOPIA_REPO_PATH: /repository
      USER: kopia
    volumes:
      - kopia_data:/app/config
      - kopia_cache:/app/cache
      - postgres_data:/backup/postgres:ro
      - mariadb_data:/backup/mariadb:ro
      - kopia_repository:/repository
      - ./configs/applications/kopia/init-kopia.sh:/init-kopia.sh:ro
    entrypoint:
      - sh
      - /init-kopia.sh
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:51515/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
  postgres:
    image: postgres:16.11
    container_name: postgres
    restart: unless-stopped
    networks:
      - postgres
    environment:
      POSTGRES_PASSWORD: ${POSTGRES_ROOT_PASSWORD}
      POSTGRES_USER: ${STACK_ADMIN_USER}
      POSTGRES_DB: postgres
      POSTGRES_ROOT_PASSWORD: ${POSTGRES_ROOT_PASSWORD}
      PLANKA_DB_PASSWORD: ${PLANKA_DB_PASSWORD}
      SYNAPSE_DB_PASSWORD: ${SYNAPSE_DB_PASSWORD}
      AUTHELIA_DB_PASSWORD: ${AUTHELIA_DB_PASSWORD}
      GRAFANA_DB_PASSWORD: ${GRAFANA_DB_PASSWORD}
      VAULTWARDEN_DB_PASSWORD: ${VAULTWARDEN_DB_PASSWORD}
      OPENWEBUI_DB_PASSWORD: ${OPENWEBUI_DB_PASSWORD}
      MASTODON_DB_PASSWORD: ${MASTODON_DB_PASSWORD}
      FORGEJO_DB_PASSWORD: ${FORGEJO_DB_PASSWORD}
      HOMEASSISTANT_DB_PASSWORD: ${HOMEASSISTANT_DB_PASSWORD}
      ROUNDCUBE_DB_PASSWORD: ${ROUNDCUBE_DB_PASSWORD}
      AGENT_POSTGRES_OBSERVER_PASSWORD: ${AGENT_POSTGRES_OBSERVER_PASSWORD}
      DATAMANCY_SERVICE_PASSWORD: ${DATAMANCY_SERVICE_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./configs/databases/postgres/init-db.sh:/docker-entrypoint-initdb.d/init-db.sh:ro
    command: postgres -c 'max_connections=300' -c 'shared_buffers=1GB'
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${STACK_ADMIN_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
  postgres-init:
    image: postgres:16.11
    container_name: postgres-init
    restart: no
    networks:
      - postgres
    depends_on:
      postgres:
        condition: service_started
    environment:
      POSTGRES_PASSWORD: ${POSTGRES_ROOT_PASSWORD}
      POSTGRES_USER: ${STACK_ADMIN_USER}
      POSTGRES_DB: postgres
      PGHOST: postgres
      PGPORT: 5432
      POSTGRES_ROOT_PASSWORD: ${POSTGRES_ROOT_PASSWORD}
      PLANKA_DB_PASSWORD: ${PLANKA_DB_PASSWORD}
      SYNAPSE_DB_PASSWORD: ${SYNAPSE_DB_PASSWORD}
      AUTHELIA_DB_PASSWORD: ${AUTHELIA_DB_PASSWORD}
      GRAFANA_DB_PASSWORD: ${GRAFANA_DB_PASSWORD}
      VAULTWARDEN_DB_PASSWORD: ${VAULTWARDEN_DB_PASSWORD}
      OPENWEBUI_DB_PASSWORD: ${OPENWEBUI_DB_PASSWORD}
      MASTODON_DB_PASSWORD: ${MASTODON_DB_PASSWORD}
      FORGEJO_DB_PASSWORD: ${FORGEJO_DB_PASSWORD}
      HOMEASSISTANT_DB_PASSWORD: ${HOMEASSISTANT_DB_PASSWORD}
      ROUNDCUBE_DB_PASSWORD: ${ROUNDCUBE_DB_PASSWORD}
      AGENT_POSTGRES_OBSERVER_PASSWORD: ${AGENT_POSTGRES_OBSERVER_PASSWORD}
      DATAMANCY_SERVICE_PASSWORD: ${DATAMANCY_SERVICE_PASSWORD}
    volumes:
      - ./configs/databases/postgres/init-db.sh:/init-db.sh:ro
    entrypoint:
      - /bin/bash
      - -c
      - |
        echo "Waiting for Postgres to be ready..."
        until pg_isready -h postgres -U ${STACK_ADMIN_USER}; do
          sleep 2
        done
        echo "Running idempotent database initialization..."
        export POSTGRES_USER=${STACK_ADMIN_USER}
        /init-db.sh
        touch /tmp/init_complete
        echo "Database initialization complete"
    healthcheck:
      test: ["CMD-SHELL", "test -f /tmp/init_complete"]
      interval: 5s
      timeout: 3s
      retries: 3
      start_period: 10s
        
  mariadb:
    image: mariadb:11.8.5
    container_name: mariadb
    restart: unless-stopped
    networks:
      - mariadb
    environment:
      MYSQL_ROOT_PASSWORD: ${MARIADB_ROOT_PASSWORD}
      MYSQL_DATABASE: datamancy
      MYSQL_USER: ${STACK_ADMIN_USER}
      MYSQL_PASSWORD: ${MARIADB_ROOT_PASSWORD}
      BOOKSTACK_DB_PASSWORD: ${BOOKSTACK_DB_PASSWORD}
      MARIADB_SEAFILE_PASSWORD: ${MARIADB_SEAFILE_PASSWORD}
    volumes:
      - mariadb_data:/var/lib/mysql
      - ./configs/databases/mariadb/init-template.sql:/docker-entrypoint-initdb.d/init-template.sql:ro
      - ./configs/databases/mariadb/init-wrapper.sh:/docker-entrypoint-initdb.d/01-init-wrapper.sh:ro
    command: --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci
    healthcheck:
      test: ["CMD-SHELL", "healthcheck.sh --connect --innodb_initialized && test -f /var/lib/mysql/.init_complete"]
      interval: 60s
      timeout: 5s
      retries: 10
  mariadb-init:
    image: mariadb:11.8.5
    container_name: mariadb-init
    restart: no
    networks:
      - mariadb
    depends_on:
      mariadb:
        condition: service_started
    environment:
      MYSQL_ROOT_PASSWORD: ${MARIADB_ROOT_PASSWORD}
      BOOKSTACK_DB_PASSWORD: ${BOOKSTACK_DB_PASSWORD}
      MARIADB_SEAFILE_PASSWORD: ${MARIADB_SEAFILE_PASSWORD}
      STACK_ADMIN_PASSWORD: ${STACK_ADMIN_PASSWORD}
    volumes:
      - ./configs/databases/mariadb/init-db.sh:/init-db.sh:ro
    entrypoint:
      - /bin/bash
      - -c
      - |
        echo "Waiting for MariaDB to be ready..."
        until mariadb -h mariadb -u root -p$$MYSQL_ROOT_PASSWORD --ssl=0 -e "SELECT 1" >/dev/null 2>&1; do
          sleep 2
        done
        echo "Running idempotent database initialization..."
        /init-db.sh
        touch /tmp/init_complete
        echo "Database initialization complete"
    healthcheck:
      test: ["CMD-SHELL", "test -f /tmp/init_complete"]
      interval: 5s
      timeout: 3s
      retries: 3
      start_period: 10s
        
  clickhouse:
    image: clickhouse/clickhouse-server:24.12
    container_name: clickhouse
    restart: unless-stopped
    networks:
      - clickhouse
    environment:
      CLICKHOUSE_DB: default
      CLICKHOUSE_USER: ${STACK_ADMIN_USER}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_ADMIN_PASSWORD}
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - ./configs/databases/clickhouse/config.xml:/etc/clickhouse-server/config.d/config.xml:ro
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8123/ping"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
  qdrant:
    image: qdrant/qdrant:v1.16.3
    container_name: qdrant
    restart: unless-stopped
    networks:
      - qdrant
    environment:
      QDRANT__SERVICE__GRPC_PORT: 6334
    volumes:
      - qdrant_data:/qdrant/storage
    healthcheck:
      test: ["CMD-SHELL", "test -d /qdrant/storage || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '2.0'
  vector-bootstrap:
    image: zenika/kotlin:latest
    container_name: vector-bootstrap
    restart: no
    networks:
      - qdrant
    depends_on:
      qdrant:
        condition: service_started
    environment:
      QDRANT_URL: http://qdrant:6333
      VECTOR_SIZE: ${VECTOR_EMBED_SIZE:-384}
      QDRANT_API_KEY: ${QDRANT_API_KEY:-}
    volumes:
      - ./configs/databases/vectors/collections.yaml:/configs/collections.yaml:ro
      - ./configs/databases/vectors/bootstrap_vectors.main.kts:/scripts/bootstrap_vectors.main.kts:ro
    command:
      - kotlin
      - /scripts/bootstrap_vectors.main.kts
      - /configs/collections.yaml
  grafana:
    image: grafana/grafana:11.6.9
    container_name: grafana
    restart: unless-stopped
    networks:
      caddy:
        aliases:
          - grafana.${DOMAIN}
      postgres:
        aliases:
          - grafana.${DOMAIN}
      authelia:
        aliases:
          - grafana.${DOMAIN}
      monitoring:
        aliases:
          - grafana.${DOMAIN}
    depends_on:
      postgres:
        condition: service_started
      authelia:
        condition: service_started
    environment:
      GF_DEFAULT_LOCALE: en_US
      GF_SERVER_ROOT_URL: https://grafana.${DOMAIN}
      GF_SERVER_DOMAIN: grafana.${DOMAIN}
      GF_DATABASE_TYPE: postgres
      GF_DATABASE_HOST: postgres:5432
      GF_DATABASE_NAME: grafana
      GF_DATABASE_USER: grafana
      GF_DATABASE_PASSWORD: ${GRAFANA_DB_PASSWORD}
      GF_DATABASE_SSL_MODE: disable
      # Using Caddy forward_auth with Authelia (not OIDC)
      GF_AUTH_PROXY_ENABLED: true
      GF_AUTH_PROXY_HEADER_NAME: Remote-User
      GF_AUTH_PROXY_HEADER_PROPERTY: username
      GF_AUTH_PROXY_AUTO_SIGN_UP: true
      GF_AUTH_PROXY_HEADERS: "Email:Remote-Email Name:Remote-Name"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./configs/monitoring/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources:ro
      - ./configs/monitoring/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards:ro
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s
  prompt-server:
    image: nginx:alpine
    container_name: prompt-server
    restart: unless-stopped
    networks:
      caddy:
        aliases:
          - prompts.${DOMAIN}
    volumes:
      - ./configs/prompts:/usr/share/nginx/html:ro
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:80/README.md || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
  open-webui:
    image: ghcr.io/open-webui/open-webui:0.4.6
    container_name: open-webui
    restart: unless-stopped
    networks:
      caddy:
        aliases:
          - open-webui.${DOMAIN}
      postgres:
        aliases:
          - open-webui.${DOMAIN}
      litellm:
        aliases:
          - open-webui.${DOMAIN}
      authelia:
        aliases:
          - open-webui.${DOMAIN}
    depends_on:
      postgres:
        condition: service_started
      litellm:
        condition: service_started
      authelia:
        condition: service_started
    environment:
      WEBUI_URL: ${OPENWEBUI_WEBUI_URL:-https://open-webui.${DOMAIN}}
      DATABASE_URL: postgresql://openwebui:${OPENWEBUI_DB_PASSWORD}@postgres:5432/openwebui
      ENABLE_OAUTH_SIGNUP: ${OPENWEBUI_ENABLE_OAUTH_SIGNUP:-true}
      DEFAULT_USER_ROLE: ${OPENWEBUI_DEFAULT_USER_ROLE:-admin}
      ENABLE_OPENAI_API: true
      OPENAI_API_BASE_URL: ${OPENWEBUI_OPENAI_API_BASE_URL:-http://litellm:4000/v1}
      OPENAI_API_KEY: ${LITELLM_MASTER_KEY}
      DEFAULT_MODELS: qwen2.5-7b-instruct
      # Smooth streaming for better UX
      STREAMING_CHUNK_SIZE: 1
      # Disable auto-downloading models from HuggingFace
      ENABLE_RAG_WEB_LOADER_SSL_VERIFICATION: false
      RAG_EMBEDDING_ENGINE: ollama
      RAG_RERANKING_MODEL: ""
      OPENID_PROVIDER_URL: https://auth.${DOMAIN}
      OAUTH_CLIENT_ID: open-webui
      OAUTH_CLIENT_SECRET: ${OPENWEBUI_OAUTH_SECRET}
      OAUTH_PROVIDER_NAME: Authelia
      OAUTH_SCOPES: openid profile email groups
      ENABLE_LOGIN_FORM: false
      WEBUI_AUTH: false
    volumes:
      - open_webui_data:/app/backend/data
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 90s
  vaultwarden:
    image: vaultwarden/server:testing
    container_name: vaultwarden
    restart: unless-stopped
    networks:
      caddy:
        aliases:
          - vaultwarden.${DOMAIN}
          - app.vaultwarden.${DOMAIN}
      postgres:
        aliases:
          - vaultwarden.${DOMAIN}
          - app.vaultwarden.${DOMAIN}
      mailserver:
        aliases:
          - vaultwarden.${DOMAIN}
          - app.vaultwarden.${DOMAIN}
      authelia:
        aliases:
          - vaultwarden.${DOMAIN}
          - app.vaultwarden.${DOMAIN}
    depends_on:
      postgres:
        condition: service_started
      mailserver:
        condition: service_started
      authelia:
        condition: service_started
    environment:
      DOMAIN: https://app.vaultwarden.${DOMAIN}
      DATABASE_URL: postgresql://vaultwarden:${VAULTWARDEN_DB_PASSWORD}@postgres:5432/vaultwarden
      SIGNUPS_ALLOWED: false
      SIGNUPS_VERIFY: true
      SIGNUPS_DOMAINS_WHITELIST: ${MAIL_DOMAIN}
      INVITATIONS_ALLOWED: false
      SHOW_PASSWORD_HINT: false
      SSO_ENABLED: true
      SSO_ONLY: true
      SSO_AUTHORITY: https://auth.${DOMAIN}
      SSO_CLIENT_ID: vaultwarden
      SSO_CLIENT_SECRET: ${VAULTWARDEN_OAUTH_SECRET}
      SSO_SCOPES: openid email profile
      SSO_CALLBACK_PATH: /identity/connect/oidc-signin
      SSO_SIGNUPS_MATCH_EMAIL: true
      ADMIN_TOKEN: ${VAULTWARDEN_ADMIN_TOKEN}
      SMTP_HOST: mailserver
      SMTP_FROM: vaultwarden@${MAIL_DOMAIN}
      SMTP_PORT: 587
      SMTP_SECURITY: starttls
      SMTP_USERNAME: vaultwarden@${MAIL_DOMAIN}
      SMTP_PASSWORD: ${VAULTWARDEN_SMTP_PASSWORD}
    volumes:
      - vaultwarden_data:/data
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:80/alive || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s
  bookstack:
    image: lscr.io/linuxserver/bookstack:version-v24.12.1
    container_name: bookstack
    restart: unless-stopped
    networks:
      caddy:
        aliases:
          - bookstack.${DOMAIN}
      mariadb:
        aliases:
          - bookstack.${DOMAIN}
    depends_on:
      mariadb:
        condition: service_healthy
      mariadb-init:
        condition: service_completed_successfully
    environment:
      PUID: ${DOCKER_USER_ID:-1000}
      PGID: ${DOCKER_GROUP_ID:-1000}
      TZ: UTC
      APP_URL: https://bookstack.${DOMAIN}
      APP_KEY: ${BOOKSTACK_APP_KEY}
      DB_HOST: mariadb
      DB_PORT: 3306
      DB_USER: bookstack
      DB_PASS: ${BOOKSTACK_DB_PASSWORD}
      DB_DATABASE: bookstack
      # Using Caddy forward_auth with Authelia (not OIDC)
    volumes:
      - bookstack_data:/config
      - ./configs/applications/bookstack/init:/custom-cont-init.d:ro
    healthcheck:
      test: ["CMD", "wget", "--no-check-certificate", "--quiet", "--tries=1", "--spider", "http://localhost:80/"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 120s
  planka:
    image: ghcr.io/plankanban/planka:1.26.3
    container_name: planka
    restart: unless-stopped
    networks:
      caddy:
        aliases:
          - planka.${DOMAIN}
      postgres:
        aliases:
          - planka.${DOMAIN}
      authelia:
        aliases:
          - planka.${DOMAIN}
    depends_on:
      postgres-init:
        condition: service_completed_successfully
      authelia:
        condition: service_started
    environment:
      BASE_URL: https://planka.${DOMAIN}
      DATABASE_URL: postgresql://planka:${PLANKA_DB_PASSWORD_ENCODED:-${PLANKA_DB_PASSWORD}}@postgres:5432/planka
      SECRET_KEY: ${PLANKA_SECRET_KEY}
      OIDC_ISSUER: https://auth.${DOMAIN}
      OIDC_CLIENT_ID: planka
      OIDC_CLIENT_SECRET: ${PLANKA_OAUTH_SECRET}
      OIDC_SCOPES: openid profile email groups
      OIDC_ADMIN_ROLES: admins
      OIDC_EMAIL_ATTRIBUTE: email
      OIDC_NAME_ATTRIBUTE: name
      OIDC_USERNAME_ATTRIBUTE: preferred_username
      OIDC_REDIRECT_URI: https://planka.${DOMAIN}/oidc-callback
      OIDC_AUTHORIZATION_ENDPOINT: https://auth.${DOMAIN}/api/oidc/authorization
      OIDC_TOKEN_ENDPOINT: http://authelia:9091/api/oidc/token
      OIDC_USERINFO_ENDPOINT: http://authelia:9091/api/oidc/userinfo
      OIDC_TOKEN_ENDPOINT_AUTH_METHOD: client_secret_post
      OIDC_ENFORCED: false
      NODE_EXTRA_CA_CERTS: /usr/local/share/ca-certificates/caddy-ca.crt
    volumes:
      - planka_data:/app/public/user-avatars
      - ./configs/applications/planka/caddy-ca.crt:/usr/local/share/ca-certificates/caddy-ca.crt:ro
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:1337/"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s
  forgejo:
    image: codeberg.org/forgejo/forgejo:9.0
    container_name: forgejo
    restart: unless-stopped
    networks:
      caddy:
        aliases:
          - forgejo.${DOMAIN}
      postgres:
        aliases:
          - forgejo.${DOMAIN}
    depends_on:
      postgres:
        condition: service_started
    environment:
      USER_UID: ${DOCKER_USER_ID:-1000}
      USER_GID: ${DOCKER_GROUP_ID:-1000}
      FORGEJO__database__DB_TYPE: postgres
      FORGEJO__database__HOST: postgres:5432
      FORGEJO__database__NAME: forgejo
      FORGEJO__database__USER: forgejo
      FORGEJO__database__PASSWD: ${FORGEJO_DB_PASSWORD}
      FORGEJO__server__DOMAIN: forgejo.${DOMAIN}
      FORGEJO__server__SSH_DOMAIN: forgejo.${DOMAIN}
      FORGEJO__server__ROOT_URL: https://forgejo.${DOMAIN}/
      FORGEJO__security__INSTALL_LOCK: true
      FORGEJO__service__DISABLE_REGISTRATION: true
      FORGEJO__service__ALLOW_ONLY_EXTERNAL_REGISTRATION: true
      FORGEJO__service__REQUIRE_SIGNIN_VIEW: true
      FORGEJO__oauth2_client__ENABLE_AUTO_REGISTRATION: true
      FORGEJO__openid__ENABLE_OPENID_SIGNIN: true
      FORGEJO__openid__ENABLE_OPENID_SIGNUP: true
      FORGEJO_OAUTH_SECRET: ${FORGEJO_OAUTH_SECRET}
      DOMAIN: ${DOMAIN}
    volumes:
      - forgejo_data:/data
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/api/healthz"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 90s
  homepage:
    image: ghcr.io/gethomepage/homepage:v1.8.0
    container_name: homepage
    restart: unless-stopped
    networks:
      caddy:
        aliases:
          - homepage.${DOMAIN}
      docker-proxy:
        aliases:
          - homepage.${DOMAIN}
    depends_on:
      docker-proxy:
        condition: service_started
    environment:
      PUID: ${DOCKER_USER_ID:-1000}
      PGID: ${DOCKER_GROUP_ID:-1000}
      HOMEPAGE_ALLOWED_HOSTS: homepage.${DOMAIN}
      DOCKER_HOST: tcp://docker-proxy:2375
    volumes:
      - ./configs/applications/homepage/services.yaml:/app/config/services.yaml:ro
      - ./configs/applications/homepage/settings.yaml:/app/config/settings.yaml:ro
      - ./configs/applications/homepage/widgets.yaml:/app/config/widgets.yaml:ro
      - ./configs/applications/homepage/bookmarks.yaml:/app/config/bookmarks.yaml:ro
      - ./configs/applications/homepage/docker.yaml:/app/config/docker.yaml:ro
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
  jupyterhub:
    image: datamancy-jupyterhub:5.4.3
    container_name: jupyterhub
    restart: unless-stopped
    networks:
      caddy:
        aliases:
          - jupyterhub.${DOMAIN}
      litellm:
        aliases:
          - jupyterhub.${DOMAIN}
      docker-proxy:
        aliases:
          - jupyterhub.${DOMAIN}
    depends_on:
      docker-proxy:
        condition: service_started
    environment:
      DOCKER_HOST: tcp://docker-proxy:2375
      DOCKER_NETWORK_NAME: datamancy-stack_litellm
      LITELLM_API_KEY: ${LITELLM_MASTER_KEY}
      JUPYTERHUB_CRYPT_KEY: ${JUPYTERHUB_CRYPT_KEY}
    volumes:
      - ./volumes/data/jupyterhub:/srv/jupyterhub
      - ./configs/applications/jupyterhub/jupyterhub_config.py:/srv/jupyterhub/jupyterhub_config.py:ro
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/hub/health || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 90s
  homeassistant:
    image: ghcr.io/home-assistant/home-assistant:2026.1.0
    container_name: homeassistant
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      postgres-init:
        condition: service_completed_successfully
      ldap:
        condition: service_healthy
    networks:
      caddy:
        aliases:
          - homeassistant.${DOMAIN}
          - api.homeassistant.${DOMAIN}
      postgres:
        aliases:
          - homeassistant.${DOMAIN}
          - api.homeassistant.${DOMAIN}
      ldap:
        aliases:
          - homeassistant.${DOMAIN}
          - api.homeassistant.${DOMAIN}
    environment:
      TZ: UTC
      PUID: ${DOCKER_USER_ID:-1000}
      PGID: ${DOCKER_GROUP_ID:-1000}
      HOMEASSISTANT_DB_URL: postgresql://homeassistant:${HOMEASSISTANT_DB_PASSWORD}@postgres:5432/homeassistant
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_DB: homeassistant
      POSTGRES_USER: homeassistant
      POSTGRES_PASSWORD: ${HOMEASSISTANT_DB_PASSWORD}
      LDAP_URL: ldap://ldap:389
      STACK_ADMIN_USER: ${STACK_ADMIN_USER}
      STACK_ADMIN_PASSWORD: ${STACK_ADMIN_PASSWORD}
      STACK_ADMIN_EMAIL: ${STACK_ADMIN_EMAIL}
      LDAP_BASE_DN: dc=stack,dc=local
      LDAP_USER_DN: ou=users,dc=stack,dc=local
      LDAP_BIND_DN: cn=admin,dc=stack,dc=local
      LDAP_BIND_PASSWORD: ${LDAP_ADMIN_PASSWORD}
      LDAP_USER_FILTER: (uid={username})
    volumes:
      - ./volumes/data/homeassistant:/config
      - /etc/localtime:/etc/localtime:ro
      - ./configs/applications/homeassistant/configuration.yaml:/config/configuration.yaml:ro
      - ./configs/applications/homeassistant/automations.yaml:/config/automations.yaml:rw
      - ./configs/applications/homeassistant/scripts.yaml:/config/scripts.yaml:rw
      - ./configs/applications/homeassistant/scenes.yaml:/config/scenes.yaml:rw
      - ./configs/applications/homeassistant/init-homeassistant.sh:/init-homeassistant.sh:ro
      - ./configs/applications/homeassistant/entrypoint.sh:/entrypoint-wrapper.sh:ro
      - ./configs/applications/homeassistant/auth_ldap.py:/usr/src/homeassistant/homeassistant/auth/providers/auth_ldap.py:ro
    entrypoint:
      - /entrypoint-wrapper.sh
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8123/manifest.json || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 120s
  qbittorrent:
    image: lscr.io/linuxserver/qbittorrent:latest
    container_name: qbittorrent
    restart: unless-stopped
    networks:
      caddy:
        aliases:
          - qbittorrent.${DOMAIN}
    environment:
      PUID: ${DOCKER_USER_ID:-1000}
      PGID: ${DOCKER_GROUP_ID:-1000}
      TZ: UTC
      WEBUI_PORT: 8080
    volumes:
      - qbittorrent_config:/config
      - qbittorrent_data:/downloads
      - ./qbittorrent_init:/custom-cont-init.d:ro
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8080/"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
  synapse-init:
    image: alpine:latest
    container_name: synapse-init
    restart: no
    networks:
      - postgres
    volumes:
      - ./volumes/data/synapse:/data
      - ./configs/applications/synapse/entrypoint.sh:/tmp/entrypoint.sh:ro
    command:
      - sh
      - -c
      - |
        echo "Copying entrypoint and fixing Synapse volume ownership..."
        cp /tmp/entrypoint.sh /data/entrypoint.sh
        chmod +x /data/entrypoint.sh
        chown -R 991:991 /data
        echo "Synapse volume permissions fixed (UID:GID = 991:991)"
  synapse:
    image: matrixdotorg/synapse:v1.144.0
    container_name: synapse
    restart: unless-stopped
    networks:
      caddy:
        aliases:
          - matrix.${DOMAIN}
          - api.matrix.${DOMAIN}
      postgres:
        aliases:
          - matrix.${DOMAIN}
          - api.matrix.${DOMAIN}
      valkey:
        aliases:
          - matrix.${DOMAIN}
          - api.matrix.${DOMAIN}
      ldap:
        aliases:
          - matrix.${DOMAIN}
          - api.matrix.${DOMAIN}
      matrix-internal:
        aliases:
          - matrix.${DOMAIN}
          - api.matrix.${DOMAIN}
      authelia:
        aliases:
          - matrix.${DOMAIN}
          - api.matrix.${DOMAIN}
    depends_on:
      postgres:
        condition: service_started
      synapse-init:
        condition: service_completed_successfully
      valkey:
        condition: service_started
      ldap:
        condition: service_started
    environment:
      DOMAIN: ${DOMAIN}
      SYNAPSE_SERVER_NAME: matrix.${DOMAIN}
      SYNAPSE_REPORT_STATS: yes
      SYNAPSE_ENABLE_REGISTRATION: ${MATRIX_ENABLE_REGISTRATION:-false}
      SYNAPSE_TRUSTED_PROXIES: 172.18.0.0/16,172.21.0.0/24
      SYNAPSE_LOG_LEVEL: WARNING
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_DB: synapse
      POSTGRES_USER: synapse
      POSTGRES_PASSWORD: ${SYNAPSE_DB_PASSWORD}
      SYNAPSE_DB_PASSWORD: ${SYNAPSE_DB_PASSWORD}
      SYNAPSE_REGISTRATION_SECRET: ${SYNAPSE_REGISTRATION_SECRET}
      SYNAPSE_MACAROON_SECRET: ${SYNAPSE_MACAROON_SECRET}
      SYNAPSE_FORM_SECRET: ${SYNAPSE_FORM_SECRET}
      MATRIX_OAUTH_SECRET: ${MATRIX_OAUTH_SECRET}
      REDIS_HOST: redis-synapse
      LDAP_ADMIN_PASSWORD: ${LDAP_ADMIN_PASSWORD}
      UID: 991
      GID: 991
    entrypoint: ["/data/entrypoint.sh"]
    volumes:
      - ./volumes/data/synapse:/data
      - ./configs/applications/synapse/homeserver.yaml:/data/homeserver.yaml:ro
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8008/health || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 120s
  element:
    image: vectorim/element-web:v1.11.91
    container_name: element
    restart: unless-stopped
    networks:
      caddy:
        aliases:
          - element.${DOMAIN}
      matrix-internal:
        aliases:
          - element.${DOMAIN}
    depends_on:
      synapse:
        condition: service_started
    volumes:
      - element_data:/app/config
      - ./configs/applications/element/config.json:/app/config.json:ro
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:80/"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
  mastodon-init:
    image: ghcr.io/mastodon/mastodon:v4.5.4
    container_name: mastodon-init
    restart: no
    networks:
      - postgres
      - valkey
    depends_on:
      postgres-init:
        condition: service_completed_successfully
      valkey:
        condition: service_started
    environment:
      LOCAL_DOMAIN: ${DOMAIN}
      RAILS_ENV: production
      DB_HOST: postgres
      DB_PORT: 5432
      DB_NAME: mastodon
      DB_USER: mastodon
      DB_PASS: ${MASTODON_DB_PASSWORD}
      REDIS_HOST: valkey
      REDIS_PORT: 6379
      SECRET_KEY_BASE: ${MASTODON_SECRET_KEY_BASE}
      OTP_SECRET: ${MASTODON_OTP_SECRET}
      VAPID_PRIVATE_KEY: ${MASTODON_VAPID_PRIVATE_KEY}
      VAPID_PUBLIC_KEY: ${MASTODON_VAPID_PUBLIC_KEY}
      ACTIVE_RECORD_ENCRYPTION_DETERMINISTIC_KEY: ${MASTODON_ACTIVE_RECORD_ENCRYPTION_DETERMINISTIC_KEY}
      ACTIVE_RECORD_ENCRYPTION_KEY_DERIVATION_SALT: ${MASTODON_ACTIVE_RECORD_ENCRYPTION_KEY_DERIVATION_SALT}
      ACTIVE_RECORD_ENCRYPTION_PRIMARY_KEY: ${MASTODON_ACTIVE_RECORD_ENCRYPTION_PRIMARY_KEY}
    command:
      - bash
      - -c
      - |
        echo "Running Mastodon database migrations..."
        bundle exec rails db:migrate
        echo "Mastodon database migrations complete"
        
  mastodon-web:
    image: ghcr.io/mastodon/mastodon:v4.5.4
    container_name: mastodon-web
    restart: unless-stopped
    networks:
      - caddy
      - postgres
      - valkey
      - mastodon-internal
    depends_on:
      mastodon-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      valkey:
        condition: service_healthy
    environment:
      LOCAL_DOMAIN: ${DOMAIN}
      WEB_DOMAIN: mastodon.${DOMAIN}
      RAILS_ENV: production
      DB_HOST: postgres
      DB_PORT: 5432
      DB_NAME: mastodon
      DB_USER: mastodon
      DB_PASS: ${MASTODON_DB_PASSWORD}
      REDIS_HOST: valkey
      REDIS_PORT: 6379
      SECRET_KEY_BASE: ${MASTODON_SECRET_KEY_BASE}
      OTP_SECRET: ${MASTODON_OTP_SECRET}
      VAPID_PRIVATE_KEY: ${MASTODON_VAPID_PRIVATE_KEY}
      VAPID_PUBLIC_KEY: ${MASTODON_VAPID_PUBLIC_KEY}
      ACTIVE_RECORD_ENCRYPTION_DETERMINISTIC_KEY: ${MASTODON_ACTIVE_RECORD_ENCRYPTION_DETERMINISTIC_KEY}
      ACTIVE_RECORD_ENCRYPTION_KEY_DERIVATION_SALT: ${MASTODON_ACTIVE_RECORD_ENCRYPTION_KEY_DERIVATION_SALT}
      ACTIVE_RECORD_ENCRYPTION_PRIMARY_KEY: ${MASTODON_ACTIVE_RECORD_ENCRYPTION_PRIMARY_KEY}
      SMTP_SERVER: mailserver
      SMTP_PORT: 587
      SMTP_FROM_ADDRESS: mastodon@${DOMAIN}
      SMTP_AUTH_METHOD: plain
      SMTP_OPENSSL_VERIFY_MODE: none
      STREAMING_API_BASE_URL: wss://mastodon.${DOMAIN}
      WEB_CONCURRENCY: 2
      MAX_THREADS: 5
      # Rails configuration
      RAILS_MAX_THREADS: 5
      DB_POOL: 10  # Should be >= MAX_THREADS for connection pooling
      # Trust all Docker bridge networks for X-Forwarded-* headers
      TRUSTED_PROXY_IP: 172.16.0.0/12  # Docker default bridge range (172.16-31.x.x)
    command: bash -c "bundle exec rails server -b 0.0.0.0"
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/health"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 120s
  mastodon-streaming:
    image: ghcr.io/mastodon/mastodon-streaming:v4.5.4
    container_name: mastodon-streaming
    restart: unless-stopped
    networks:
      - postgres
      - valkey
      - mastodon-internal
    depends_on:
      postgres:
        condition: service_started
      valkey:
        condition: service_started
    environment:
      DB_HOST: postgres
      DB_PORT: 5432
      DB_NAME: mastodon
      DB_USER: mastodon
      DB_PASS: ${MASTODON_DB_PASSWORD}
      REDIS_HOST: valkey
      REDIS_PORT: 6379
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:4000/api/v1/streaming/health"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s
  mastodon-sidekiq:
    image: ghcr.io/mastodon/mastodon:v4.5.4
    container_name: mastodon-sidekiq
    restart: unless-stopped
    networks:
      - postgres
      - valkey
      - mastodon-internal
    depends_on:
      mastodon-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      valkey:
        condition: service_healthy
    environment:
      LOCAL_DOMAIN: ${DOMAIN}
      WEB_DOMAIN: mastodon.${DOMAIN}
      RAILS_ENV: production
      DB_HOST: postgres
      DB_PORT: 5432
      DB_NAME: mastodon
      DB_USER: mastodon
      DB_PASS: ${MASTODON_DB_PASSWORD}
      REDIS_HOST: valkey
      REDIS_PORT: 6379
      SECRET_KEY_BASE: ${MASTODON_SECRET_KEY_BASE}
      OTP_SECRET: ${MASTODON_OTP_SECRET}
      VAPID_PRIVATE_KEY: ${MASTODON_VAPID_PRIVATE_KEY}
      VAPID_PUBLIC_KEY: ${MASTODON_VAPID_PUBLIC_KEY}
      ACTIVE_RECORD_ENCRYPTION_DETERMINISTIC_KEY: ${MASTODON_ACTIVE_RECORD_ENCRYPTION_DETERMINISTIC_KEY}
      ACTIVE_RECORD_ENCRYPTION_KEY_DERIVATION_SALT: ${MASTODON_ACTIVE_RECORD_ENCRYPTION_KEY_DERIVATION_SALT}
      ACTIVE_RECORD_ENCRYPTION_PRIMARY_KEY: ${MASTODON_ACTIVE_RECORD_ENCRYPTION_PRIMARY_KEY}
      SMTP_SERVER: mailserver
      SMTP_PORT: 587
      SMTP_FROM_ADDRESS: mastodon@${DOMAIN}
      SMTP_AUTH_METHOD: plain
      SMTP_OPENSSL_VERIFY_MODE: none
    command: bundle exec sidekiq
    healthcheck:
      test: ["CMD-SHELL", "ps aux | grep -v grep | grep 'sidekiq'"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s
  roundcube-init:
    image: postgres:16.11
    container_name: roundcube-init
    restart: no
    networks:
      - postgres
    depends_on:
      postgres-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    environment:
      PGPASSWORD: ${POSTGRES_ROOT_PASSWORD}
      POSTGRES_USER: ${STACK_ADMIN_USER}
      ROUNDCUBE_DB_PASSWORD: ${ROUNDCUBE_DB_PASSWORD}
    volumes:
      - ./configs/databases/postgres/roundcube-schema.sql:/tmp/roundcube-schema.sql:ro
    command:
      - bash
      - -c
      - |
        echo "Initializing Roundcube database schema..."
        # Wait for postgres to be ready
        until pg_isready -h postgres -U ${STACK_ADMIN_USER}; do
          echo "Waiting for postgres..."
          sleep 2
        done
        # Check if session table already exists
        TABLE_EXISTS=$$(psql -h postgres -U $${STACK_ADMIN_USER} -d roundcube -tAc "SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_schema = 'public' AND table_name = 'session');")
        if [ "$$TABLE_EXISTS" = "t" ]; then
          echo "Roundcube schema already initialized, skipping..."
          exit 0
        fi
        echo "Applying Roundcube schema from bundled file..."
        psql -h postgres -U ${STACK_ADMIN_USER} -d roundcube -f /tmp/roundcube-schema.sql
        echo "Granting permissions to roundcube user..."
        psql -h postgres -U ${STACK_ADMIN_USER} -d roundcube <<-EOSQL
          GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO roundcube;
          GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public TO roundcube;
        EOSQL
        echo "Roundcube database schema initialized successfully!"
  roundcube:
    image: roundcube/roundcubemail:latest
    container_name: roundcube
    restart: unless-stopped
    networks:
      - caddy
      - mailserver
      - postgres
    depends_on:
      mailserver:
        condition: service_started
      postgres:
        condition: service_started
      roundcube-init:
        condition: service_completed_successfully
    environment:
      ROUNDCUBEMAIL_DB_TYPE: pgsql
      ROUNDCUBEMAIL_DB_HOST: postgres
      ROUNDCUBEMAIL_DB_PORT: 5432
      ROUNDCUBEMAIL_DB_USER: roundcube
      ROUNDCUBEMAIL_DB_PASSWORD: ${ROUNDCUBE_DB_PASSWORD}
      ROUNDCUBEMAIL_DB_NAME: roundcube
      ROUNDCUBEMAIL_DEFAULT_HOST: mailserver
      ROUNDCUBEMAIL_DEFAULT_PORT: 143
      ROUNDCUBEMAIL_SMTP_SERVER: mailserver
      ROUNDCUBEMAIL_SMTP_PORT: 587
      ROUNDCUBEMAIL_UPLOAD_MAX_FILESIZE: 50M
    volumes:
      - roundcube_data:/var/www/html
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:80/ || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s
  radicale:
    image: tomsquest/docker-radicale:latest
    container_name: radicale
    restart: unless-stopped
    networks:
      - caddy
    environment:
      TZ: UTC
    volumes:
      - radicale_data:/data
      - ./configs/applications/radicale/config:/etc/radicale/config:ro
      - ./configs/applications/radicale/users:/data/users
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:5232/.web/"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
  seafile:
    image: seafileltd/seafile-mc:13.0.15
    container_name: seafile
    restart: unless-stopped
    networks:
      caddy:
        aliases:
          - seafile.${DOMAIN}
          - api.seafile.${DOMAIN}
      mariadb:
        aliases:
          - seafile.${DOMAIN}
          - api.seafile.${DOMAIN}
      memcached:
        aliases:
          - seafile.${DOMAIN}
          - api.seafile.${DOMAIN}
      valkey:
        aliases:
          - seafile.${DOMAIN}
    depends_on:
      mariadb:
        condition: service_healthy
      mariadb-init:
        condition: service_completed_successfully
      seafile-memcached:
        condition: service_started
      valkey:
        condition: service_healthy
    environment:
      TZ: UTC
      DB_HOST: mariadb
      DB_ROOT_PASSWD: ${MARIADB_ROOT_PASSWORD}
      SEAFILE_MYSQL_DB_HOST: mariadb
      SEAFILE_MYSQL_DB_PORT: 3306
      SEAFILE_MYSQL_DB_USER: seafile
      SEAFILE_MYSQL_DB_PASSWORD: ${MARIADB_SEAFILE_PASSWORD}
      INIT_SEAFILE_MYSQL_ROOT_PASSWORD: ${MARIADB_ROOT_PASSWORD}
      SEAFILE_SERVER_HOSTNAME: seafile.${DOMAIN}
      SEAFILE_ADMIN_EMAIL: ${STACK_ADMIN_EMAIL}
      SEAFILE_ADMIN_PASSWORD: ${STACK_ADMIN_PASSWORD}
      JWT_PRIVATE_KEY: ${SEAFILE_JWT_KEY}
      TIME_ZONE: UTC
      CACHE_PROVIDER: redis
      REDIS_HOST: valkey
      REDIS_PORT: 6379
    volumes:
      - seafile_files:/shared
      - seafile_media:/shared/seafile/library-template
    healthcheck:
      test: ["CMD", "curl", "-f", "-s", "http://localhost:80/api2/ping/"]
      interval: 60s
      timeout: 20s
      retries: 3
      start_period: 180s
  onlyoffice:
    image: onlyoffice/documentserver:8.3.3
    container_name: onlyoffice
    restart: unless-stopped
    networks:
      caddy:
        aliases:
          - onlyoffice.${DOMAIN}
    environment:
      JWT_ENABLED: true
      JWT_SECRET: ${ONLYOFFICE_JWT_SECRET}
    volumes:
      - onlyoffice_data:/var/www/onlyoffice/Data
      - onlyoffice_log:/var/log/onlyoffice
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:80/healthcheck"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 120s
  control-panel:
    image: datamancy/control-panel:local-build
    container_name: control-panel
    restart: unless-stopped
    ports:
      - "8097:8097"
    networks:
      caddy:
        aliases:
          - control.${DOMAIN}
      postgres:
        aliases:
          - control.${DOMAIN}
    depends_on:
      postgres:
        condition: service_started
      data-fetcher:
        condition: service_started
      unified-indexer:
        condition: service_started
    environment:
      CONTROL_PANEL_PORT: 8097
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_DB: datamancy
      POSTGRES_USER: datamancer
      POSTGRES_PASSWORD: ${DATAMANCY_SERVICE_PASSWORD}
      DATA_FETCHER_URL: http://data-fetcher:8095
      UNIFIED_INDEXER_URL: http://unified-indexer:8096
      SEARCH_SERVICE_URL: http://search-service:8098
    healthcheck:
      test: ["CMD", "wget", "-O", "/dev/null", "--quiet", "--tries=1", "--timeout=15", "http://localhost:8097/"]
      interval: 60s
      timeout: 20s
      retries: 3
      start_period: 180s
  data-fetcher:
    image: datamancy/data-fetcher:local-build
    container_name: data-fetcher
    restart: unless-stopped
    networks:
      - postgres
      - clickhouse
      - mariadb
      - qdrant
    depends_on:
      postgres:
        condition: service_started
      clickhouse:
        condition: service_started
      bookstack:
        condition: service_started
    environment:
      DATAFETCHER_PORT: 8095
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_DB: datamancy
      POSTGRES_USER: datamancer
      POSTGRES_PASSWORD: ${DATAMANCY_SERVICE_PASSWORD}
      CLICKHOUSE_HOST: clickhouse
      CLICKHOUSE_PORT: 8123
      CLICKHOUSE_USER: ${STACK_ADMIN_USER}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_ADMIN_PASSWORD}
      QDRANT_HOST: qdrant
      QDRANT_PORT: 6333
      OPENWEATHER_API_KEY: ${OPENWEATHER_API_KEY:-}
      COINGECKO_API_KEY: ${COINGECKO_API_KEY:-}
      SERP_API_KEY: ${SERP_API_KEY:-}
      FRED_API_KEY: ${FRED_API_KEY:-}
      BOOKSTACK_URL: ${BOOKSTACK_URL:-http://bookstack:80}
      BOOKSTACK_API_TOKEN_ID: ${BOOKSTACK_API_TOKEN_ID:-}
      BOOKSTACK_API_TOKEN_SECRET: ${BOOKSTACK_API_TOKEN_SECRET:-}
      FETCH_SCHEDULES_PATH: /app/config/schedules.yaml
      FETCH_SOURCES_PATH: /app/config/sources.yaml
      DATAFETCHER_DATA_PATH: /app/data
    volumes:
      - ./configs/applications/data-fetcher/schedules.yaml:/app/config/schedules.yaml:ro
      - ./configs/applications/data-fetcher/sources.yaml:/app/config/sources.yaml:ro
      - ./volumes/data/data-fetcher:/app/data
    healthcheck:
      test: ["CMD", "wget", "-O", "/dev/null", "--quiet", "--tries=1", "--timeout=15", "http://localhost:8095/"]
      interval: 60s
      timeout: 20s
      retries: 3
      start_period: 180s
  unified-indexer:
    image: datamancy/unified-indexer:local-build
    container_name: unified-indexer
    restart: unless-stopped
    networks:
      - postgres
      - mariadb
      - qdrant
      - clickhouse
      - ai-gateway
    depends_on:
      postgres:
        condition: service_started
      bookstack:
        condition: service_started
      qdrant:
        condition: service_started
      clickhouse:
        condition: service_started
      embedding-service:
        condition: service_started
    environment:
      UNIFIED_INDEXER_PORT: 8096
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_DB: datamancy
      POSTGRES_USER: datamancer
      POSTGRES_PASSWORD: ${DATAMANCY_SERVICE_PASSWORD}
      BOOKSTACK_URL: ${BOOKSTACK_URL:-http://bookstack:80}
      BOOKSTACK_API_TOKEN_ID: ${BOOKSTACK_API_TOKEN_ID:-}
      BOOKSTACK_API_TOKEN_SECRET: ${BOOKSTACK_API_TOKEN_SECRET:-}
      QDRANT_URL: http://qdrant:6334
      CLICKHOUSE_URL: http://clickhouse:8123
      CLICKHOUSE_USER: ${STACK_ADMIN_USER}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_ADMIN_PASSWORD}
      EMBEDDING_SERVICE_URL: http://embedding-service:8080
    healthcheck:
      test: ["CMD", "wget", "-O", "/dev/null", "--quiet", "--tries=1", "--timeout=15", "http://localhost:8096/"]
      interval: 60s
      timeout: 20s
      retries: 3
      start_period: 180s
  search-service:
    image: datamancy/search-service:local-build
    container_name: search-service
    restart: unless-stopped
    networks:
      - qdrant
      - clickhouse
      - ai-gateway
    depends_on:
      qdrant:
        condition: service_started
      clickhouse:
        condition: service_started
      embedding-service:
        condition: service_started
    environment:
      SEARCH_SERVICE_PORT: 8098
      QDRANT_HOST: qdrant
      QDRANT_PORT: 6333
      QDRANT_API_KEY: ${QDRANT_API_KEY}
      CLICKHOUSE_HOST: clickhouse
      CLICKHOUSE_PORT: 8123
      CLICKHOUSE_USER: ${STACK_ADMIN_USER}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_ADMIN_PASSWORD}
      EMBEDDING_SERVICE_URL: http://embedding-service:8080
    healthcheck:
      test: ["CMD-SHELL", "timeout 5 wget --quiet --tries=1 -O /dev/null http://127.0.0.1:8098/ || exit 1"]
      interval: 60s
      timeout: 20s
      retries: 3
      start_period: 180s
  agent-tool-server:
    image: datamancy/agent-tool-server:local-build
    container_name: agent-tool-server
    restart: unless-stopped
    networks:
      ai:
        aliases:
          - agent-tool-server.${DOMAIN}
      ai-gateway:
        aliases:
          - agent-tool-server.${DOMAIN}
      caddy:
        aliases:
          - agent-tool-server.${DOMAIN}
      postgres:
        aliases:
          - agent-tool-server.${DOMAIN}
      mariadb:
        aliases:
          - agent-tool-server.${DOMAIN}
      clickhouse:
        aliases:
          - agent-tool-server.${DOMAIN}
      qdrant:
        aliases:
          - agent-tool-server.${DOMAIN}
      ldap:
        aliases:
          - agent-tool-server.${DOMAIN}
    depends_on:
      litellm:
        condition: service_started
    environment:
      LITELLM_API_KEY: ${LITELLM_MASTER_KEY}
      LITELLM_BASE_URL: http://litellm:4000/v1
      POSTGRES_OBSERVER_USER: agent_observer
      POSTGRES_OBSERVER_PASSWORD: ${AGENT_POSTGRES_OBSERVER_PASSWORD}
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      MARIADB_OBSERVER_USER: agent_observer
      MARIADB_OBSERVER_PASSWORD: ${AGENT_MARIADB_OBSERVER_PASSWORD}
      MARIADB_HOST: mariadb
      MARIADB_PORT: 3306
      CLICKHOUSE_OBSERVER_USER: ${STACK_ADMIN_USER}
      CLICKHOUSE_OBSERVER_PASSWORD: ${AGENT_CLICKHOUSE_OBSERVER_PASSWORD}
      CLICKHOUSE_HOST: clickhouse
      CLICKHOUSE_PORT: 8123
      QDRANT_OBSERVER_API_KEY: ${AGENT_QDRANT_API_KEY}
      QDRANT_HOST: qdrant
      QDRANT_PORT: 6333
      LDAP_OBSERVER_DN: cn=agent_observer,dc=stack,dc=local
      LDAP_OBSERVER_PASSWORD: ${AGENT_LDAP_OBSERVER_PASSWORD}
      LDAP_HOST: ldap
      LDAP_PORT: 389
      LDAP_BASE_DN: dc=stack,dc=local
      SEARCH_SERVICE_URL: http://search-service:8098
      MCP_GRANT_HOST_NETWORK_HTTP: "true"
      MCP_GRANT_HOST_NETWORK_SSH: "true"
    volumes:
      - ${DOCKER_SOCKET:-/var/run/docker.sock}:/var/run/docker.sock:ro
      - ./secrets/stackops_ed25519:/app/keys/stackops_ed25519:ro
      - ./agent_tool_server/known_hosts:/app/known_hosts:ro
      - ./configs/infrastructure/ssh/bootstrap_known_hosts.sh:/app/scripts/bootstrap_known_hosts.sh:ro
    healthcheck:
      test: ["CMD-SHELL", "wget --quiet --tries=1 --timeout=15 --spider http://localhost:8081/tools && test -S /var/run/docker.sock"]
      interval: 60s
      timeout: 20s
      retries: 3
      start_period: 180s
  prometheus:
    image: prom/prometheus:v3.9.1
    container_name: prometheus
    restart: unless-stopped
    networks:
      - monitoring
    volumes:
      - prometheus_data:/prometheus
      - ./configs/monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    command:
      - --config.file=/etc/prometheus/prometheus.yml
      - --storage.tsdb.path=/prometheus
      - --storage.tsdb.retention.time=30d
      - --web.console.libraries=/usr/share/prometheus/console_libraries
      - --web.console.templates=/usr/share/prometheus/consoles
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/ready"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
  node-exporter:
    image: prom/node-exporter:v1.10.2
    container_name: node-exporter
    restart: unless-stopped
    networks:
      - monitoring
    volumes:
      - /:/host:ro,rslave
      - /sys:/host/sys:ro
      - /proc:/host/proc:ro
    command:
      - --path.rootfs=/host
      - --collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9100/metrics"]
      interval: 60s
      timeout: 5s
      retries: 3
      start_period: 10s
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.55.1
    container_name: cadvisor
    restart: unless-stopped
    networks:
      - monitoring
      - docker-proxy
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8080/healthz"]
      interval: 60s
      timeout: 5s
      retries: 3
      start_period: 20s
  watchtower:
    image: containrrr/watchtower:1.7.1
    container_name: watchtower
    restart: unless-stopped
    networks:
      - docker-proxy
    environment:
      WATCHTOWER_CLEANUP: true
      WATCHTOWER_INCLUDE_RESTARTING: true
      WATCHTOWER_SCHEDULE: 0 0 4 * * *
      DOCKER_API_VERSION: 1.44
      WATCHTOWER_NOTIFICATIONS: shoutrrr
      WATCHTOWER_NOTIFICATION_URL: 
      TZ: Australia/Sydney
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    # Healthcheck disabled - distroless image without /bin/sh
    # Service runs as scheduled cron, doesn't need healthcheck
  vllm-7b:
    image: vllm/vllm-openai:v0.13.0
    container_name: vllm-7b
    restart: unless-stopped
    networks:
      - ai
    environment:
      HUGGINGFACEHUB_API_TOKEN: ${HUGGINGFACEHUB_API_TOKEN}
      HF_HUB_ENABLE_HF_TRANSFER: 0
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
    volumes:
      - ./vllm/models:/root/.cache/huggingface
    command:
      - Qwen/Qwen2.5-7B-Instruct-AWQ
      - --quantization
      - awq
      - --served-model-name
      - qwen2.5-7b-instruct
      - --max-model-len
      - "32768"
      - --max-num-seqs
      - "16"
      - --dtype
      - auto
      - --enable-auto-tool-choice
      - --tool-call-parser
      - hermes
      - --gpu-memory-utilization
      - "0.9"
      - --tensor-parallel-size
      - "1"
      - --trust-remote-code
    deploy:
      resources:
        limits:
          memory: 12G
          cpus: '8.0'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 300s
  embedding-service:
    image: ghcr.io/huggingface/text-embeddings-inference:1.8.3
    container_name: embedding-service
    restart: unless-stopped
    networks:
      - ai
      - ai-gateway
    environment:
      HF_HUB_ENABLE_HF_TRANSFER: 0
      MODEL_ID: BAAI/bge-base-en-v1.5
    volumes:
      - ./embeddings/models:/data
    command: --port 8080
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 120s
  litellm:
    image: ghcr.io/berriai/litellm:v1.80.8-stable.1
    container_name: litellm
    restart: unless-stopped
    networks:
      ai:
        aliases:
          - litellm.${DOMAIN}
          - api.litellm.${DOMAIN}
      litellm:
        aliases:
          - litellm.${DOMAIN}
          - api.litellm.${DOMAIN}
      caddy:
        aliases:
          - litellm.${DOMAIN}
          - api.litellm.${DOMAIN}
    depends_on:
      vllm-7b:
        condition: service_started
      embedding-service:
        condition: service_started
    environment:
      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY}
      UI_USERNAME: ${STACK_ADMIN_USER}
      UI_PASSWORD: ${STACK_ADMIN_PASSWORD}
    volumes:
      - litellm_config:/app/config
      - ./configs/infrastructure/litellm/config.yaml:/app/config.yaml:ro
    command: --config /app/config.yaml --port 4000 --num_workers 4
    healthcheck:
      test: ["CMD-SHELL", "python3 -c 'import socket; s=socket.socket(); s.settimeout(5); s.connect((\"127.0.0.1\", 4000)); s.close()' || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s

networks:
  postgres:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/24
    # PostgreSQL database access

  mariadb:
    driver: bridge
    ipam:
      config:
        - subnet: 172.21.0.0/24
    # MariaDB database access

  clickhouse:
    driver: bridge
    ipam:
      config:
        - subnet: 172.22.0.0/24
    # ClickHouse database access

  qdrant:
    driver: bridge
    ipam:
      config:
        - subnet: 172.23.0.0/24
    # Qdrant vector database access

  valkey:
    driver: bridge
    ipam:
      config:
        - subnet: 172.24.0.0/24
    # Valkey (Redis) access

  ldap:
    driver: bridge
    ipam:
      config:
        - subnet: 172.25.0.0/24
    # LDAP directory access

  authelia:
    driver: bridge
    ipam:
      config:
        - subnet: 172.26.0.0/24
    # Authelia SSO access

  mailserver:
    driver: bridge
    ipam:
      config:
        - subnet: 172.27.0.0/24
    # Mail server access

  memcached:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/24
    # Memcached access (Seafile)

  docker-proxy:
    driver: bridge
    ipam:
      config:
        - subnet: 172.29.0.0/24
    # Docker socket proxy access

  caddy:
    driver: bridge
    ipam:
      config:
        - subnet: 172.30.0.0/24
    # Caddy reverse proxy (web-exposed services)

  ai:
    driver: bridge
    ipam:
      config:
        - subnet: 172.40.0.0/24
    # ISOLATED AI/LLM services (external actor zone)

  ai-gateway:
    driver: bridge
    ipam:
      config:
        - subnet: 172.41.0.0/24
    # Controlled gateway between AI and backend

  litellm:
    driver: bridge
    ipam:
      config:
        - subnet: 172.42.0.0/24
    # LiteLLM AI gateway access

  monitoring:
    driver: bridge
    ipam:
      config:
        - subnet: 172.50.0.0/24
    # Prometheus, Node Exporter, cAdvisor metrics collection

  mastodon-internal:
    driver: bridge
    ipam:
      config:
        - subnet: 172.52.0.0/24
    # Mastodon multi-container internal communication

  matrix-internal:
    driver: bridge
    ipam:
      config:
        - subnet: 172.53.0.0/24
    # Matrix Synapse and Element communication

volumes:
  # Bookstack
  bookstack_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/data/bookstack

  # Caddy
  caddy_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/config/caddy
  caddy_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/credentials/caddy

  # ClickHouse
  clickhouse_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/data/clickhouse

  # Element
  element_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/data/element

  # Forgejo
  forgejo_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/data/forgejo

  # Grafana
  grafana_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/data/grafana

  # Kopia
  kopia_cache:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/data/kopia/cache
  kopia_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/data/kopia/data
  kopia_repository:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/data/kopia/repository

  # LDAP
  ldap_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/config/ldap
  ldap_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/data/ldap

  # LiteLLM
  litellm_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/config/litellm

  # Mailserver
  mailserver_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/config/mailserver
  mailserver_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/data/mailserver/data
  mailserver_logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/data/mailserver/logs
  mailserver_state:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/data/mailserver/state

  # MariaDB
  mariadb_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/data/mariadb

  # OnlyOffice
  onlyoffice_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/data/onlyoffice/data
  onlyoffice_log:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/data/onlyoffice/logs

  # Open WebUI
  open_webui_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/data/open-webui

  # Planka
  planka_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/data/planka

  # PostgreSQL
  postgres_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/data/postgres

  # Prometheus
  prometheus_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/data/prometheus

  # qBittorrent
  qbittorrent_config:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/config/qbittorrent
  qbittorrent_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${QBITTORRENT_DATA_ROOT}

  # Qdrant (vector DB)
  qdrant_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${VECTOR_DB_ROOT}/qdrant

  # Radicale
  radicale_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/data/radicale

  # Redis
  redis_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/data/redis

  # Roundcube
  roundcube_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/data/roundcube

  # Seafile
  seafile_files:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${SEAFILE_FILES_ROOT}
  seafile_media:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${SEAFILE_MEDIA_ROOT}

  # Vaultwarden
  vaultwarden_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/data/vaultwarden

